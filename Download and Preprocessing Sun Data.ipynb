{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sunpy.net import Fido, attrs as a\n",
    "import astropy.units as u\n",
    "\n",
    "import sunpy.map\n",
    "from astropy.io import fits\n",
    "\n",
    "from aiapy.calibrate import register, update_pointing, correct_degradation\n",
    "from aiapy.calibrate.util import get_pointing_table, get_correction_table\n",
    "import aiapy.psf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import shutil\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2013/01/01 00:00:00'\n",
    "end_date = '2014/01/01 00:00:00'\n",
    "\n",
    "# Set Time Range\n",
    "time_range = a.Time(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to download the files\n",
    "download_directory = 'C:/Sun Data/'\n",
    "os.makedirs(download_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for fits on VSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for:  <sunpy.net.attrs.Time(2024-01-01 00:00:00.000, 2024-01-02 00:00:00.000)>\n",
      "Found 24 results\n",
      "[<sunpy.net.vso.table_response.VSOQueryResponseTable object at 0x00000292229CA5F0>\n",
      "       Start Time               End Time        Source ... Extent Type   Size  \n",
      "                                                       ...              Mibyte \n",
      "----------------------- ----------------------- ------ ... ----------- --------\n",
      "2024-01-01 00:00:07.000 2024-01-01 00:00:08.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 01:00:04.000 2024-01-01 01:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 02:00:04.000 2024-01-01 02:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 03:00:04.000 2024-01-01 03:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 04:00:04.000 2024-01-01 04:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 05:00:04.000 2024-01-01 05:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 06:00:04.000 2024-01-01 06:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 07:00:04.000 2024-01-01 07:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 08:00:04.000 2024-01-01 08:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "                    ...                     ...    ... ...         ...      ...\n",
      "2024-01-01 14:00:04.000 2024-01-01 14:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 15:00:04.000 2024-01-01 15:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 16:00:04.000 2024-01-01 16:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 17:00:04.000 2024-01-01 17:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 18:00:04.000 2024-01-01 18:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 19:00:04.000 2024-01-01 19:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 20:00:04.000 2024-01-01 20:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 21:00:02.000 2024-01-01 21:00:03.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 22:00:04.000 2024-01-01 22:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "2024-01-01 23:00:04.000 2024-01-01 23:00:05.000    SDO ...    FULLDISK 64.64844\n",
      "Length = 24 rows]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "start_date_dt = datetime.datetime.strptime(start_date, '%Y/%m/%d %H:%M:%S')\n",
    "end_date_dt = datetime.datetime.strptime(end_date, '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "current_date = start_date_dt\n",
    "\n",
    "while current_date < end_date_dt:\n",
    "    next_date = current_date + relativedelta(months=1)\n",
    "    if next_date > end_date_dt:\n",
    "        next_date = end_date_dt\n",
    "\n",
    "    time_range = a.Time(current_date.strftime('%Y/%m/%d %H:%M:%S'), next_date.strftime('%Y/%m/%d %H:%M:%S'))\n",
    "    print(\"Searching for: \", time_range)\n",
    "    results = Fido.search(\n",
    "        time_range,\n",
    "        a.Instrument.aia,\n",
    "        a.Wavelength(193 * u.angstrom),\n",
    "        a.Sample(60 * u.minute)\n",
    "    )\n",
    "    print(\"Found\", results.file_num, \"results\")\n",
    "    all_results.extend(results)\n",
    "    current_date = next_date\n",
    "\n",
    "\n",
    "print(all_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download files with error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f410e4941b448482718f0dcb78bafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files Downloaded:   0%|          | 0/24 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1878b388cc7441d0a602ff5c7c959af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T01_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571f07d502e043b188750b413cde226f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T02_00_04.85Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc161b004154783af277220c68874a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T03_00_04.85Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726104f4d59549cebd84ed565274430b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T00_00_07.05Z.image_lev1.fits:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f15203eef214231a49d98cf320965c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T04_00_04.85Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b36f1820fe488dac176d59158d070f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T05_00_04.85Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61302ff52d1140c1ae58cae86d6fe5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T06_00_04.83Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc23a8e39d954e9ea7dae34fa64823da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T07_00_04.85Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb831b6a8114c0bae1ab2e78f64b6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T08_00_04.85Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a673379c1154194a0d9f4185fcf7732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T09_00_07.25Z.image_lev1.fits:   0%|          | 0.00/9.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543b1e60e3eb4430813b16f2d7ab76ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T10_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b060392fdb444f9075cfb6d60eca0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T11_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553cbb236bc64d578483fb267af78e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T12_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7512f754bb1f4f2d92481fc4bef46551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T13_00_06.91Z.image_lev1.fits:   0%|          | 0.00/10.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1b6f31e475463c8296aa6c047ee92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T14_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fba62821d94554a8fa175ac127a812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T15_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837df096131740249d3f3cf42e67ee05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T16_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1893cfd698480ba3ad3897e7c401bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T17_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3728ba7c88474fa6d62ce859a44fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T18_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00c4164b59247c5a138acf19468067f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T19_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749185d6327d4b7eafd810f37d4e4ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T20_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234ea1c1ef2444069d0fa7c3c27551d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T21_00_02.01Z.image_lev1.fits:   0%|          | 0.00/6.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8399acd943584a928d5914c0e4377cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T22_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aa0ff2be3e4888aedab24c462bb66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aia.lev1.193A_2024_01_01T23_00_04.84Z.image_lev1.fits:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m failed \u001b[38;5;129;01mand\u001b[39;00m counter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m         downloaded_files \u001b[38;5;241m=\u001b[39m \u001b[43mFido\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\site-packages\\sunpy\\net\\fido_factory.py:435\u001b[0m, in \u001b[0;36mUnifiedDownloaderFactory.fetch\u001b[1;34m(self, path, max_conn, progress, overwrite, downloader, *query_results, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mNotImplemented\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    433\u001b[0m             reslist\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m--> 435\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdownloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Combine the results objects from all the clients into one Results object.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m reslist:\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\site-packages\\parfive\\downloader.py:341\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    Download all files in the queue.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    thread if a loop is already running).\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_in_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\site-packages\\parfive\\downloader.py:264\u001b[0m, in \u001b[0;36mDownloader._run_in_loop\u001b[1;34m(self, coro)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;66;03m# Execute the task\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_run_in_thread:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_task_in_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\site-packages\\parfive\\utils.py:84\u001b[0m, in \u001b[0;36mrun_task_in_thread\u001b[1;34m(loop, coro)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_task_in_thread\u001b[39m(loop: asyncio\u001b[38;5;241m.\u001b[39mBaseEventLoop, coro: asyncio\u001b[38;5;241m.\u001b[39mTask) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    This function returns the asyncio Future after running the loop in a\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    thread.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    of ``loop.run_until_complete``.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m aio_pool:\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             future \u001b[38;5;241m=\u001b[39m aio_pool\u001b[38;5;241m.\u001b[39msubmit(loop\u001b[38;5;241m.\u001b[39mrun_until_complete, coro)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\concurrent\\futures\\_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\concurrent\\futures\\thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[1;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[1;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for results in all_results:\n",
    "    downloaded_files = []\n",
    "    while results.file_num != len(downloaded_files):\n",
    "        try:\n",
    "            downloaded_files = Fido.fetch(\n",
    "                results, path=download_directory)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Failed to download files. Retrying...\")\n",
    "            continue\n",
    "\n",
    "# Output the list of downloaded files\n",
    "print(f\"Downloaded {len(downloaded_files)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality checking and Re-downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we check quality of all files, if the quality is less than 0 we delete the file and save the date and time in an array.\n",
    "#### Then we redownload files 5 minutes after those files \n",
    "#### Then we re-check the quality of those newly downloaded files\n",
    "#### Then we delete all the bad quality files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'downloaded_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 68\u001b[0m\n\u001b[0;32m     63\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# check and delete bad quality files\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m bad_quality_files \u001b[38;5;241m=\u001b[39m check_file_quality(\u001b[43mdownloaded_files\u001b[49m, download_directory)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# download new files for the bad quality files\u001b[39;00m\n\u001b[0;32m     70\u001b[0m download_list_of_files(bad_quality_files, download_directory)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'downloaded_files' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_file_quality(file_list, download_directory):\n",
    "    bad_quality_files = []\n",
    "    # open all files and check the quality\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            with fits.open(file) as hdul:\n",
    "                header = hdul[1].header\n",
    "                # Check if the quality is bad add to the list\n",
    "                if header['QUALITY'] != 0:\n",
    "                    bad_quality_files.append(file)\n",
    "                    print(f\"Bad quality file: {file}\")\n",
    "                hdul.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to open {file}: {e}\")\n",
    "            continue\n",
    "    # delete all bad quality files\n",
    "    for bad_file in bad_quality_files:\n",
    "        os.remove(bad_file)\n",
    "        \n",
    "    # Save the list of bad quality files to a text file\n",
    "    bad_quality_files_file = os.path.join(download_directory, 'bad_quality_files.txt')\n",
    "    with open(bad_quality_files_file, 'w') as f:\n",
    "        for file in bad_quality_files:\n",
    "            f.write(f\"{file}\\n\")\n",
    "            \n",
    "    # return the list of bad quality files\n",
    "    return bad_quality_files\n",
    "\n",
    "def download_list_of_files(file_list, download_directory):\n",
    "    for bad_file in file_list:\n",
    "        # To remove path\n",
    "        bad_file_list = bad_file.split('\\\\')\n",
    "        # Get just the file name\n",
    "        bad_file = bad_file_list[-1]\n",
    "        # Get the date and time of the bad quality file\n",
    "        year = bad_file[14:18]\n",
    "        month = bad_file[19:21]\n",
    "        day = bad_file[22:24]\n",
    "        hour = bad_file[25:27]\n",
    "        \n",
    "        # Create a new date with the same date but 5 minutes later\n",
    "        new_date = f'{year}/{month}/{day} {hour}:05:00'\n",
    "        new_date_10 = f'{year}/{month}/{day} {hour}:10:00'\n",
    "\n",
    "        # Download the new file for the bad quality file\n",
    "        new_time_range = a.Time(new_date, new_date_10)\n",
    "        new_results = Fido.search(\n",
    "            new_time_range,\n",
    "            a.Instrument.aia,\n",
    "            a.Wavelength(193 * u.angstrom),\n",
    "            a.Sample(60 * u.minute)\n",
    "        )\n",
    "\n",
    "        # print(new_results)\n",
    "\n",
    "        # Download the new data\n",
    "        new_downloaded_files = Fido.fetch(new_results, path=download_directory)\n",
    "\n",
    "        # Save the list of newly downloaded files to a text file\n",
    "        new_downloaded_files_file = os.path.join(download_directory, 'new_downloaded_files.txt')\n",
    "        with open(new_downloaded_files_file, 'a') as f:\n",
    "            for file in new_downloaded_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "# check and delete bad quality files\n",
    "bad_quality_files = check_file_quality(downloaded_files, download_directory)\n",
    "# download new files for the bad quality files\n",
    "download_list_of_files(bad_quality_files, download_directory)\n",
    "\n",
    "# Open the new_downloaded_files.txt and read the contents\n",
    "new_downloaded_files_file = os.path.join(download_directory, 'new_downloaded_files.txt')\n",
    "with open(new_downloaded_files_file, 'r') as f:\n",
    "    new_downloaded_files = f.readlines()\n",
    "\n",
    "# Remove the newline character from the file names\n",
    "new_downloaded_files = [file.strip() for file in new_downloaded_files]\n",
    "\n",
    "# Print the list of newly downloaded files\n",
    "print(\"Newly downloaded files:\")\n",
    "for file in new_downloaded_files:\n",
    "    print(file)\n",
    "\n",
    "# check and delete bad quality files\n",
    "bad_quality_files = check_file_quality(new_downloaded_files, download_directory)\n",
    "\n",
    "print(\"New files with bad quality:\")\n",
    "for file in bad_quality_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pointing correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pointing correction can be done together with image registration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # List all files in the download directory\n",
    "# aia_files = [f for f in os.listdir(download_directory) if f.endswith('.fits')]\n",
    "\n",
    "# # update pointing table for all fits files in the download directory\n",
    "# for aia_file in tqdm(aia_files, desc=\"Updating pointing\"):\n",
    "#     aia_file_path = os.path.join(download_directory, aia_file)\n",
    "#     # Open the FITS file\n",
    "#     try:\n",
    "#         hdul = fits.open(aia_file_path, mode=\"update\")\n",
    "#         header = hdul[1].header\n",
    "#         data = hdul[1].data\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"FILE CORRUPTED: {aia_file_path}, Error: {e}\")\n",
    "#         continue\n",
    "#     # remove these two header keywords because they are nan and are causing errors with sunpy map\n",
    "#     header.remove('OSCNMEAN', ignore_missing=True)\n",
    "#     header.remove('OSCNRMS', ignore_missing=True)\n",
    "#     # convert to sunpy map\n",
    "#     aia_map = sunpy.map.Map((data, header))\n",
    "#     # download pointing table\n",
    "#     pointing_table = get_pointing_table(\"JSOC\", time_range=(aia_map.date - 12 * u.h, aia_map.date + 12 * u.h))\n",
    "#     # update pointing\n",
    "#     aia_map_updated_pointing = update_pointing(aia_map, pointing_table=pointing_table)\n",
    "#     hdul[1].header.update(aia_map_updated_pointing.meta)\n",
    "#     hdul.close()\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the list of all fits files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the download directory\n",
    "aia_files = [f for f in os.listdir(download_directory) if f.endswith('.fits')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PSF Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7edbc10f864c4e9dca15fc9524383a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSF Deconvolution:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def deconvolve_file(aia_file):\n",
    "    aia_file_path = os.path.join(download_directory, aia_file)\n",
    "    try:\n",
    "        hdul = fits.open(aia_file_path, mode=\"update\")\n",
    "        header = hdul[1].header\n",
    "        data = hdul[1].data\n",
    "    except Exception as e:\n",
    "        return f\"FILE CORRUPTED: {aia_file_path}, Error: {e}\"\n",
    "\n",
    "    header.remove('OSCNMEAN', ignore_missing=True)\n",
    "    header.remove('OSCNRMS', ignore_missing=True)\n",
    "\n",
    "    aia_map = sunpy.map.Map((data, header))\n",
    "    psf = aiapy.psf.psf(aia_map.wavelength)\n",
    "    aia_map_deconvolved = aiapy.psf.deconvolve(aia_map, psf=psf)\n",
    "\n",
    "    hdul[1].data = aia_map_deconvolved.data\n",
    "    hdul.close()\n",
    "    return None\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    list(tqdm(executor.map(deconvolve_file, aia_files), total=len(aia_files), desc=\"PSF Deconvolution\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointing Correction and Image Registration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b441a41f6eae4d7d9ddf336a23babcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pointing Correction and Image Registration:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_file(aia_file):\n",
    "    aia_file_path = os.path.join(download_directory, aia_file)\n",
    "    try:\n",
    "        hdul = fits.open(aia_file_path, mode=\"update\")\n",
    "        header = hdul[1].header\n",
    "        data = hdul[1].data\n",
    "    except Exception as e:\n",
    "        return f\"FILE CORRUPTED: {aia_file_path}, Error: {e}\"\n",
    "    header.remove('OSCNMEAN', ignore_missing=True)\n",
    "    header.remove('OSCNRMS', ignore_missing=True)\n",
    "    aia_map = sunpy.map.Map((data, header))\n",
    "    pointing_table = get_pointing_table(\"JSOC\", time_range=(aia_map.date - 12 * u.h, aia_map.date + 12 * u.h))\n",
    "    aia_map_updated_pointing = update_pointing(aia_map, pointing_table=pointing_table)\n",
    "    aia_map_registered = register(aia_map_updated_pointing)\n",
    "\n",
    "    hdul[1].data = aia_map_registered.data\n",
    "    hdul.close()\n",
    "    return None\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    list(tqdm(executor.map(process_file, aia_files), total=len(aia_files), desc=\"Pointing Correction and Image Registration\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degradation Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got correction table\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16397db731348748d2dc5e52170aa22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Degradation Correction:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correction_table = get_correction_table(\"JSOC\")\n",
    "print(\"Got correction table\")\n",
    "\n",
    "def correct_degradation_file(aia_file):\n",
    "    aia_file_path = os.path.join(download_directory, aia_file)\n",
    "    try:\n",
    "        hdul = fits.open(aia_file_path, mode=\"update\")\n",
    "        header = hdul[1].header\n",
    "        data = hdul[1].data\n",
    "    except Exception as e:\n",
    "        return f\"FILE CORRUPTED: {aia_file_path}, Error: {e}\"\n",
    "\n",
    "    aia_map = sunpy.map.Map((data, header))\n",
    "    aia_map_corrected = correct_degradation(aia_map, correction_table=correction_table)\n",
    "    hdul[1].data = aia_map_corrected.data\n",
    "    hdul.close()\n",
    "    return None\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    list(tqdm(executor.map(correct_degradation_file, aia_files), total=len(aia_files), desc=\"Degradation Correction\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exposure Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139d789aec794836b78e07eb69e9ff52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing by Exposure Time:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\site-packages\\astropy\\units\\quantity.py:671: RuntimeWarning: divide by zero encountered in divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n",
      "c:\\Users\\mirzi\\miniconda3\\envs\\sundata\\lib\\site-packages\\astropy\\units\\quantity.py:671: RuntimeWarning: invalid value encountered in multiply\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def normalize_by_exposure(aia_file):\n",
    "    aia_file_path = os.path.join(download_directory, aia_file)\n",
    "    try:\n",
    "        hdul = fits.open(aia_file_path, mode=\"update\")\n",
    "        header = hdul[1].header\n",
    "        data = hdul[1].data\n",
    "    except Exception as e:\n",
    "        return f\"FILE CORRUPTED: {aia_file_path}, Error: {e}\"\n",
    "    \n",
    "    aia_map = sunpy.map.Map((data, header))\n",
    "    aia_map_normalized = aia_map / aia_map.exposure_time\n",
    "    hdul[1].data = aia_map_normalized.data\n",
    "    hdul.close()\n",
    "    return None\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    list(tqdm(executor.map(normalize_by_exposure, aia_files), total=len(aia_files), desc=\"Normalizing by Exposure Time\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clipping values between 100 and 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b75fe8d91b141a295c0a38612e61f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clipping Values:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clip_values(aia_file):\n",
    "    aia_file_path = os.path.join(download_directory, aia_file)\n",
    "    try:\n",
    "        hdul = fits.open(aia_file_path, mode=\"update\")\n",
    "        header = hdul[1].header\n",
    "        data = hdul[1].data\n",
    "    except Exception as e:\n",
    "        return f\"FILE CORRUPTED: {aia_file_path}, Error: {e}\"\n",
    "    \n",
    "    # Clip the values between 100 and 5000\n",
    "    data_clipped = data.clip(100, 5000)\n",
    "    \n",
    "    # Save the clipped image\n",
    "    hdul[1].data = data_clipped\n",
    "    hdul.close()\n",
    "    return None\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    list(tqdm(executor.map(clip_values, aia_files), total=len(aia_files), desc=\"Clipping Values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescaling Images by Log10 (Cant do this because it causes issues while copying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for aia_file in tqdm(aia_files, desc=\"Rescaling to log10\"):\n",
    "#     aia_file_path = os.path.join(download_directory, aia_file)\n",
    "#     # Open the FITS file\n",
    "#     try:\n",
    "#         hdul = fits.open(aia_file_path, mode=\"update\")\n",
    "#         header = hdul[1].header\n",
    "#         data = hdul[1].data\n",
    "#     except Exception as e:\n",
    "#         print(f\"FILE CORRUPTED: {aia_file_path}, Error: {e}\")\n",
    "#         continue\n",
    "    \n",
    "#     # Rescale the data to log10 scale\n",
    "#     data_log10 = np.log10(data + 1)  # Adding 1 to avoid log(0)\n",
    "    \n",
    "#     # Save the rescaled image\n",
    "#     hdul[1].data = data_log10\n",
    "    \n",
    "#     hdul.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resizing images to 224 x 224 using pillow/opencv with bilinear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963814409032472fa5b715412c6328ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downscaling to 224x224:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for aia_file in tqdm(aia_files, desc=\"Downscaling to 224x224\"):\n",
    "    aia_file_path = os.path.join(download_directory, aia_file)\n",
    "    # Open the FITS file\n",
    "    # Specify the new directory where you want to copy the files\n",
    "    new_directory = os.path.join(download_directory, 'non_anti_aliasing/')\n",
    "    os.makedirs(new_directory, exist_ok=True)\n",
    "\n",
    "    # Copy the file to the new directory\n",
    "    new_file_path = os.path.join(new_directory, aia_file)\n",
    "    shutil.copy2(aia_file_path, new_file_path)\n",
    "\n",
    "    # open first file\n",
    "    try:\n",
    "        hdul1 = fits.open(aia_file_path, mode=\"update\")\n",
    "        header1 = hdul1[1].header\n",
    "        data1 = hdul1[1].data\n",
    "    except Exception as e:\n",
    "        print(f\"FILE CORRUPTED: {aia_file_path}, Error: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Rescale the data to log10 scale AGAIN BECUASE FOR SOME REASON COPYING THE FILE UNDOS IT!!!\n",
    "    data_log10_1 = np.log10(data1 + 1)  # Adding 1 to avoid log(0)\n",
    "    # Using pillow or anti-aliasing\n",
    "    image_pil = Image.fromarray(data_log10_1)\n",
    "    resized_data_aa = image_pil.resize((224, 224), Image.BILINEAR)\n",
    "    resized_image_np = np.array(resized_data_aa)\n",
    "    # Save the downscaled anti-aliased image\n",
    "    hdul1[1].data = resized_image_np\n",
    "    \n",
    "    hdul1.close()\n",
    "    \n",
    "    # Open second file\n",
    "    try:\n",
    "        hdul2 = fits.open(new_file_path, mode=\"update\")\n",
    "        header2 = hdul2[1].header\n",
    "        data2 = hdul2[1].data\n",
    "    except Exception as e:\n",
    "        print(f\"FILE CORRUPTED: {new_file_path}, Error: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Rescale the data to log10 scale AGAIN BECUASE FOR SOME REASON COPYING THE FILE UNDOS IT!!!\n",
    "    data_log10_2 = np.log10(data2 + 1)  # Adding 1 to avoid log(0)\n",
    "    \n",
    "    # Using OpenCV for non-anti-aliasing\n",
    "    resized_data = cv2.resize(data_log10_2, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    # Save the downscaled non-anti-aliased image\n",
    "    hdul2[1].data = resized_data\n",
    "\n",
    "\n",
    "    hdul2.close()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sundata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
