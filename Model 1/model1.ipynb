{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import torchsummary\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dataloader import SunImageDataset\n",
    "\n",
    "from torch.func import vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 224*224\n",
    "hidden_size = 166\n",
    "num_epochs = 10\n",
    "batch_size = 5\n",
    "learning_rate = 0.001\n",
    "# dropout = 0.6990787087509548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SunImageDataset(csv_file='D:/Dissertation/pytorch trial/dataset.csv', offset=0, transform=transforms.ToTensor())\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "trainset, testset = torch.utils.data.Subset(dataset, range(train_size)), torch.utils.data.Subset(dataset, range(train_size, len(dataset)))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Get a batch of training data\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# print(images)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─BatchNorm2d: 1-1                                 [-1, 3, 224, 224]         6\n",
      "├─SwinTransformer: 1                               []                        --\n",
      "|    └─PatchEmbed: 2-1                             [-1, 56, 56, 128]         --\n",
      "|    |    └─Conv2d: 3-1                            [-1, 128, 56, 56]         6,272\n",
      "|    |    └─LayerNorm: 3-2                         [-1, 56, 56, 128]         256\n",
      "|    └─Sequential: 2-2                             [-1, 7, 7, 1024]          --\n",
      "|    |    └─SwinTransformerStage: 3-3              [-1, 56, 56, 128]         397,896\n",
      "|    |    └─SwinTransformerStage: 3-4              [-1, 28, 28, 256]         1,714,320\n",
      "|    |    └─SwinTransformerStage: 3-5              [-1, 14, 14, 512]         57,317,920\n",
      "|    |    └─SwinTransformerStage: 3-6              [-1, 7, 7, 1024]          27,304,512\n",
      "|    └─LayerNorm: 2-3                              [-1, 7, 7, 1024]          2,048\n",
      "|    └─ClassifierHead: 2-4                         [-1, 166]                 --\n",
      "|    |    └─SelectAdaptivePool2d: 3-7              [-1, 1024]                --\n",
      "|    |    └─Dropout: 3-8                           [-1, 1024]                --\n",
      "|    |    └─Linear: 3-9                            [-1, 166]                 170,150\n",
      "|    |    └─Identity: 3-10                         [-1, 166]                 --\n",
      "├─BatchNorm2d: 1-2                                 [-1, 3, 224, 224]         (recursive)\n",
      "├─SwinTransformer: 1                               []                        --\n",
      "|    └─PatchEmbed: 2-5                             [-1, 56, 56, 128]         (recursive)\n",
      "|    |    └─Conv2d: 3-11                           [-1, 128, 56, 56]         (recursive)\n",
      "|    |    └─LayerNorm: 3-12                        [-1, 56, 56, 128]         (recursive)\n",
      "|    └─Sequential: 2-6                             [-1, 7, 7, 1024]          (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-13             [-1, 56, 56, 128]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-14             [-1, 28, 28, 256]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-15             [-1, 14, 14, 512]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-16             [-1, 7, 7, 1024]          (recursive)\n",
      "|    └─LayerNorm: 2-7                              [-1, 7, 7, 1024]          (recursive)\n",
      "|    └─ClassifierHead: 2-8                         [-1, 166]                 (recursive)\n",
      "|    |    └─SelectAdaptivePool2d: 3-17             [-1, 1024]                --\n",
      "|    |    └─Dropout: 3-18                          [-1, 1024]                --\n",
      "|    |    └─Linear: 3-19                           [-1, 166]                 (recursive)\n",
      "|    |    └─Identity: 3-20                         [-1, 166]                 --\n",
      "├─Sequential: 1-3                                  [-1, 1]                   --\n",
      "|    └─LeakyReLU: 2-9                              [-1, 1660]                --\n",
      "|    └─Linear: 2-10                                [-1, 166]                 275,726\n",
      "|    └─Dropout: 2-11                               [-1, 166]                 --\n",
      "|    └─LeakyReLU: 2-12                             [-1, 166]                 --\n",
      "|    └─Linear: 2-13                                [-1, 1]                   167\n",
      "|    └─LeakyReLU: 2-14                             [-1, 1]                   --\n",
      "====================================================================================================\n",
      "Total params: 87,189,273\n",
      "Trainable params: 87,189,273\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 558.99\n",
      "====================================================================================================\n",
      "Input size (MB): 1.91\n",
      "Forward/backward pass size (MB): 7.66\n",
      "Params size (MB): 332.60\n",
      "Estimated Total Size (MB): 342.17\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─BatchNorm2d: 1-1                                 [-1, 3, 224, 224]         6\n",
      "├─SwinTransformer: 1                               []                        --\n",
      "|    └─PatchEmbed: 2-1                             [-1, 56, 56, 128]         --\n",
      "|    |    └─Conv2d: 3-1                            [-1, 128, 56, 56]         6,272\n",
      "|    |    └─LayerNorm: 3-2                         [-1, 56, 56, 128]         256\n",
      "|    └─Sequential: 2-2                             [-1, 7, 7, 1024]          --\n",
      "|    |    └─SwinTransformerStage: 3-3              [-1, 56, 56, 128]         397,896\n",
      "|    |    └─SwinTransformerStage: 3-4              [-1, 28, 28, 256]         1,714,320\n",
      "|    |    └─SwinTransformerStage: 3-5              [-1, 14, 14, 512]         57,317,920\n",
      "|    |    └─SwinTransformerStage: 3-6              [-1, 7, 7, 1024]          27,304,512\n",
      "|    └─LayerNorm: 2-3                              [-1, 7, 7, 1024]          2,048\n",
      "|    └─ClassifierHead: 2-4                         [-1, 166]                 --\n",
      "|    |    └─SelectAdaptivePool2d: 3-7              [-1, 1024]                --\n",
      "|    |    └─Dropout: 3-8                           [-1, 1024]                --\n",
      "|    |    └─Linear: 3-9                            [-1, 166]                 170,150\n",
      "|    |    └─Identity: 3-10                         [-1, 166]                 --\n",
      "├─BatchNorm2d: 1-2                                 [-1, 3, 224, 224]         (recursive)\n",
      "├─SwinTransformer: 1                               []                        --\n",
      "|    └─PatchEmbed: 2-5                             [-1, 56, 56, 128]         (recursive)\n",
      "|    |    └─Conv2d: 3-11                           [-1, 128, 56, 56]         (recursive)\n",
      "|    |    └─LayerNorm: 3-12                        [-1, 56, 56, 128]         (recursive)\n",
      "|    └─Sequential: 2-6                             [-1, 7, 7, 1024]          (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-13             [-1, 56, 56, 128]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-14             [-1, 28, 28, 256]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-15             [-1, 14, 14, 512]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-16             [-1, 7, 7, 1024]          (recursive)\n",
      "|    └─LayerNorm: 2-7                              [-1, 7, 7, 1024]          (recursive)\n",
      "|    └─ClassifierHead: 2-8                         [-1, 166]                 (recursive)\n",
      "|    |    └─SelectAdaptivePool2d: 3-17             [-1, 1024]                --\n",
      "|    |    └─Dropout: 3-18                          [-1, 1024]                --\n",
      "|    |    └─Linear: 3-19                           [-1, 166]                 (recursive)\n",
      "|    |    └─Identity: 3-20                         [-1, 166]                 --\n",
      "├─Sequential: 1-3                                  [-1, 1]                   --\n",
      "|    └─LeakyReLU: 2-9                              [-1, 1660]                --\n",
      "|    └─Linear: 2-10                                [-1, 166]                 275,726\n",
      "|    └─Dropout: 2-11                               [-1, 166]                 --\n",
      "|    └─LeakyReLU: 2-12                             [-1, 166]                 --\n",
      "|    └─Linear: 2-13                                [-1, 1]                   167\n",
      "|    └─LeakyReLU: 2-14                             [-1, 1]                   --\n",
      "====================================================================================================\n",
      "Total params: 87,189,273\n",
      "Trainable params: 87,189,273\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 558.99\n",
      "====================================================================================================\n",
      "Input size (MB): 1.91\n",
      "Forward/backward pass size (MB): 7.66\n",
      "Params size (MB): 332.60\n",
      "Estimated Total Size (MB): 342.17\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GmiSwinTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(GmiSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Batch normalization for 3 channels\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # Initialize Swin Transformer\n",
    "        self.pretrained_model = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',\n",
    "            pretrained=True,\n",
    "            num_classes=hidden_size\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size*10, hidden_size),\n",
    "            nn.Dropout(p=0.5),  # Added dropout probability\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Batch should be in format:\n",
    "        {\n",
    "            'images': torch.FloatTensor((10, 1, 224, 224))\n",
    "        }\n",
    "        \"\"\"\n",
    "        # Print input shape for debugging\n",
    "        # print(\"Input shape:\", images.shape)\n",
    "        image_features = torch.zeros(images.shape[0],images.shape[1], hidden_size).to(device)\n",
    "        for i in range(images.shape[0]):\n",
    "            image = images[i, :, :, :]\n",
    "            # Pretrained swin transformer accepts three channel images\n",
    "            three_channel = torch.stack([image,image,image], dim=1).squeeze(2)\n",
    "            # print(\"three_channel\", three_channel.size())\n",
    "            # Model learns optimal initial normalisation\n",
    "            normalized_images = self.bn(three_channel)\n",
    "            # Get image features\n",
    "            image_feature = self.pretrained_model.forward(normalized_images)\n",
    "            image_features[i] = image_feature\n",
    "        # print(\"image_features before reshaping\", image_features.size())\n",
    "        image_features = image_features.view(image_features.size(0), -1)\n",
    "        # print(\"image_features after reshaping\", image_features.size())\n",
    "        output = self.fc(image_features)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GmiSwinTransformer(hidden_size=hidden_size).to(device)\n",
    "\n",
    "print(torchsummary.summary(model, (10, 1, 224, 224)))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff268ac6efca404bbc846abfcf7a3c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/1519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "n_total_steps = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in tqdm(enumerate(trainloader), desc=\"Training Progress\", total=len(trainloader)):\n",
    "        # Move images and labels to device\n",
    "        images = torch.stack(images).to(device)\n",
    "        # print(\"images before reshaping\", images.size())\n",
    "        images = images.permute(1, 0, 2, 3, 4)  # Change shape to [5, 10, 1, 224, 224]\n",
    "        # print(\"images after reshaping\", images.size())\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass with autograd\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aafc917e034be3bb9dabdf299b8134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Progress:   0%|          | 0/950 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(testloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      8\u001b[0m         images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(images)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\Dissertation\\pytorch trial\\dataloader.py:27\u001b[0m, in \u001b[0;36mSunImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     25\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(path)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m):\n\u001b[1;32m---> 27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:2345\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mresize(size, resample, box)\n\u001b[0;32m   2343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m-> 2345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reducing_gap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resample \u001b[38;5;241m!=\u001b[39m Resampling\u001b[38;5;241m.\u001b[39mNEAREST:\n\u001b[0;32m   2348\u001b[0m     factor_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m reducing_gap) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Jpeg2KImagePlugin.py:351\u001b[0m, in \u001b[0;36mJpeg2KImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    348\u001b[0m     t3 \u001b[38;5;241m=\u001b[39m (t[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, t[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m3\u001b[39m], t[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile \u001b[38;5;241m=\u001b[39m [ImageFile\u001b[38;5;241m.\u001b[39m_Tile(t[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, t[\u001b[38;5;241m2\u001b[39m], t3)]\n\u001b[1;32m--> 351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\ImageFile.py:275\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mpulls_fd:\n\u001b[0;32m    274\u001b[0m     decoder\u001b[38;5;241m.\u001b[39msetfd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp)\n\u001b[1;32m--> 275\u001b[0m     err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     b \u001b[38;5;241m=\u001b[39m prefix\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False)\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(testloader, desc=\"Testing Progress\"):\n",
    "        images = torch.stack(images).to(device)\n",
    "        images = images.permute(1, 0, 2, 3, 4)\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "print(f'Average test loss: {avg_test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
