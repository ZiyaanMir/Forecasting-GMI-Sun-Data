{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import torchsummary\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../DataLoader')\n",
    "\n",
    "from dataloader import SunImageDataset\n",
    "\n",
    "from torch.func import vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 224*224\n",
    "hidden_size = 166\n",
    "num_epochs = 10\n",
    "batch_size = 5\n",
    "learning_rate = 0.001\n",
    "# dropout = 0.6990787087509548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SunImageDataset(csv_file='D:/Dissertation/pytorch trial/dataset.csv', offset=0, transform=transforms.ToTensor())\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "trainset, testset = torch.utils.data.Subset(dataset, range(train_size)), torch.utils.data.Subset(dataset, range(train_size, len(dataset)))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Get a batch of training data\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# print(images)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─BatchNorm2d: 1-1                                 [-1, 3, 224, 224]         6\n",
      "├─SwinTransformer: 1                               []                        --\n",
      "|    └─PatchEmbed: 2-1                             [-1, 56, 56, 128]         --\n",
      "|    |    └─Conv2d: 3-1                            [-1, 128, 56, 56]         6,272\n",
      "|    |    └─LayerNorm: 3-2                         [-1, 56, 56, 128]         256\n",
      "|    └─Sequential: 2-2                             [-1, 7, 7, 1024]          --\n",
      "|    |    └─SwinTransformerStage: 3-3              [-1, 56, 56, 128]         397,896\n",
      "|    |    └─SwinTransformerStage: 3-4              [-1, 28, 28, 256]         1,714,320\n",
      "|    |    └─SwinTransformerStage: 3-5              [-1, 14, 14, 512]         57,317,920\n",
      "|    |    └─SwinTransformerStage: 3-6              [-1, 7, 7, 1024]          27,304,512\n",
      "|    └─LayerNorm: 2-3                              [-1, 7, 7, 1024]          2,048\n",
      "|    └─ClassifierHead: 2-4                         [-1, 166]                 --\n",
      "|    |    └─SelectAdaptivePool2d: 3-7              [-1, 1024]                --\n",
      "|    |    └─Dropout: 3-8                           [-1, 1024]                --\n",
      "|    |    └─Linear: 3-9                            [-1, 166]                 170,150\n",
      "|    |    └─Identity: 3-10                         [-1, 166]                 --\n",
      "├─BatchNorm2d: 1-2                                 [-1, 3, 224, 224]         (recursive)\n",
      "├─SwinTransformer: 1                               []                        --\n",
      "|    └─PatchEmbed: 2-5                             [-1, 56, 56, 128]         (recursive)\n",
      "|    |    └─Conv2d: 3-11                           [-1, 128, 56, 56]         (recursive)\n",
      "|    |    └─LayerNorm: 3-12                        [-1, 56, 56, 128]         (recursive)\n",
      "|    └─Sequential: 2-6                             [-1, 7, 7, 1024]          (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-13             [-1, 56, 56, 128]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-14             [-1, 28, 28, 256]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-15             [-1, 14, 14, 512]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-16             [-1, 7, 7, 1024]          (recursive)\n",
      "|    └─LayerNorm: 2-7                              [-1, 7, 7, 1024]          (recursive)\n",
      "|    └─ClassifierHead: 2-8                         [-1, 166]                 (recursive)\n",
      "|    |    └─SelectAdaptivePool2d: 3-17             [-1, 1024]                --\n",
      "|    |    └─Dropout: 3-18                          [-1, 1024]                --\n",
      "|    |    └─Linear: 3-19                           [-1, 166]                 (recursive)\n",
      "|    |    └─Identity: 3-20                         [-1, 166]                 --\n",
      "├─Sequential: 1-3                                  [-1, 1]                   --\n",
      "|    └─LeakyReLU: 2-9                              [-1, 1660]                --\n",
      "|    └─Linear: 2-10                                [-1, 166]                 275,726\n",
      "|    └─Dropout: 2-11                               [-1, 166]                 --\n",
      "|    └─LeakyReLU: 2-12                             [-1, 166]                 --\n",
      "|    └─Linear: 2-13                                [-1, 1]                   167\n",
      "|    └─LeakyReLU: 2-14                             [-1, 1]                   --\n",
      "====================================================================================================\n",
      "Total params: 87,189,273\n",
      "Trainable params: 87,189,273\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 558.99\n",
      "====================================================================================================\n",
      "Input size (MB): 1.91\n",
      "Forward/backward pass size (MB): 7.66\n",
      "Params size (MB): 332.60\n",
      "Estimated Total Size (MB): 342.17\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─BatchNorm2d: 1-1                                 [-1, 3, 224, 224]         6\n",
      "├─SwinTransformer: 1                               []                        --\n",
      "|    └─PatchEmbed: 2-1                             [-1, 56, 56, 128]         --\n",
      "|    |    └─Conv2d: 3-1                            [-1, 128, 56, 56]         6,272\n",
      "|    |    └─LayerNorm: 3-2                         [-1, 56, 56, 128]         256\n",
      "|    └─Sequential: 2-2                             [-1, 7, 7, 1024]          --\n",
      "|    |    └─SwinTransformerStage: 3-3              [-1, 56, 56, 128]         397,896\n",
      "|    |    └─SwinTransformerStage: 3-4              [-1, 28, 28, 256]         1,714,320\n",
      "|    |    └─SwinTransformerStage: 3-5              [-1, 14, 14, 512]         57,317,920\n",
      "|    |    └─SwinTransformerStage: 3-6              [-1, 7, 7, 1024]          27,304,512\n",
      "|    └─LayerNorm: 2-3                              [-1, 7, 7, 1024]          2,048\n",
      "|    └─ClassifierHead: 2-4                         [-1, 166]                 --\n",
      "|    |    └─SelectAdaptivePool2d: 3-7              [-1, 1024]                --\n",
      "|    |    └─Dropout: 3-8                           [-1, 1024]                --\n",
      "|    |    └─Linear: 3-9                            [-1, 166]                 170,150\n",
      "|    |    └─Identity: 3-10                         [-1, 166]                 --\n",
      "├─BatchNorm2d: 1-2                                 [-1, 3, 224, 224]         (recursive)\n",
      "├─SwinTransformer: 1                               []                        --\n",
      "|    └─PatchEmbed: 2-5                             [-1, 56, 56, 128]         (recursive)\n",
      "|    |    └─Conv2d: 3-11                           [-1, 128, 56, 56]         (recursive)\n",
      "|    |    └─LayerNorm: 3-12                        [-1, 56, 56, 128]         (recursive)\n",
      "|    └─Sequential: 2-6                             [-1, 7, 7, 1024]          (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-13             [-1, 56, 56, 128]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-14             [-1, 28, 28, 256]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-15             [-1, 14, 14, 512]         (recursive)\n",
      "|    |    └─SwinTransformerStage: 3-16             [-1, 7, 7, 1024]          (recursive)\n",
      "|    └─LayerNorm: 2-7                              [-1, 7, 7, 1024]          (recursive)\n",
      "|    └─ClassifierHead: 2-8                         [-1, 166]                 (recursive)\n",
      "|    |    └─SelectAdaptivePool2d: 3-17             [-1, 1024]                --\n",
      "|    |    └─Dropout: 3-18                          [-1, 1024]                --\n",
      "|    |    └─Linear: 3-19                           [-1, 166]                 (recursive)\n",
      "|    |    └─Identity: 3-20                         [-1, 166]                 --\n",
      "├─Sequential: 1-3                                  [-1, 1]                   --\n",
      "|    └─LeakyReLU: 2-9                              [-1, 1660]                --\n",
      "|    └─Linear: 2-10                                [-1, 166]                 275,726\n",
      "|    └─Dropout: 2-11                               [-1, 166]                 --\n",
      "|    └─LeakyReLU: 2-12                             [-1, 166]                 --\n",
      "|    └─Linear: 2-13                                [-1, 1]                   167\n",
      "|    └─LeakyReLU: 2-14                             [-1, 1]                   --\n",
      "====================================================================================================\n",
      "Total params: 87,189,273\n",
      "Trainable params: 87,189,273\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 558.99\n",
      "====================================================================================================\n",
      "Input size (MB): 1.91\n",
      "Forward/backward pass size (MB): 7.66\n",
      "Params size (MB): 332.60\n",
      "Estimated Total Size (MB): 342.17\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "class GmiSwinTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(GmiSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Batch normalization for 3 channels\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # Initialize Swin Transformer\n",
    "        self.pretrained_model = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',\n",
    "            pretrained=True,\n",
    "            num_classes=hidden_size\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size*10, hidden_size),\n",
    "            nn.Dropout(p=0.5),  # Added dropout probability\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def signle_image_feature_extractor(self, image):\n",
    "        normalized_image = self.bn(image)\n",
    "        return self.pretrained_model(normalized_image)\n",
    "\n",
    "    def gray_to_rgb(self, x):\n",
    "        \"\"\"Convert grayscale to RGB by replicating channels\"\"\"\n",
    "        return torch.cat([x, x, x], dim=1)  # [B, 3, H, W]\n",
    "    \n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Batch should be in format:\n",
    "        {\n",
    "            'images': torch.FloatTensor((10, 1, 224, 224))\n",
    "        }\n",
    "        \"\"\"\n",
    "        # # Print input shape for debugging\n",
    "        # # print(\"Input shape:\", images.shape)\n",
    "        # image_features = torch.zeros(images.shape[0],images.shape[1], hidden_size).to(device)\n",
    "        # for i in range(images.shape[0]):\n",
    "        #     image = images[i, :, :, :]\n",
    "        #     # Pretrained swin transformer accepts three channel images\n",
    "        #     three_channel = torch.stack([image,image,image], dim=1).squeeze(2)\n",
    "        #     # print(\"three_channel\", three_channel.size())\n",
    "        #     # Model learns optimal initial normalisation\n",
    "        #     normalized_images = self.bn(three_channel)\n",
    "        #     # Get image features\n",
    "        #     image_feature = self.pretrained_model.forward(normalized_images)\n",
    "        #     image_features[i] = image_feature\n",
    "        # # print(\"image_features before reshaping\", image_features.size())\n",
    "        # image_features = image_features.view(image_features.size(0), -1)\n",
    "        # # print(\"image_features after reshaping\", image_features.size())\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        images = images.reshape(-1, 1, 224, 224)\n",
    "        images = torch.cat([images, images, images], dim=1)\n",
    "        normalized_images = self.bn(images)\n",
    "        features = self.pretrained_model(normalized_images)\n",
    "\n",
    "        image_features = features.view(batch_size, -1)\n",
    "        \n",
    "        output = self.fc(image_features)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GmiSwinTransformer(hidden_size=hidden_size).to(device)\n",
    "\n",
    "print(torchsummary.summary(model, (10, 1, 224, 224)))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d337e34e0ce146b7a0f5699a31d6d360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/1519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(6.6071, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4254, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(24.5684, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2818, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.0480, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.4438, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3636, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.7433, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6246, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.8304, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1191, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.2709, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8496, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.2738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2230, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8580, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7581, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8264, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6840, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9691, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2743, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7744, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2654, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7180, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8822, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2933, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.7104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2929, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8248, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8596, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1479, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.5049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1935, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2132, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.6173, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3513, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3397, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9133, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2509, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.5828, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1714, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5193, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7886, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1124, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1710, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6979, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1256, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9243, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0213, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.1417, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1614, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7518, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1851, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6935, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6959, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2339, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.7707, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4501, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9417, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(8.6311, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8748, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8457, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7737, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.7004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2750, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0963, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6708, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7731, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3744, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8799, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7927, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9680, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7950, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2815, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8274, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.6024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.8142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2817, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1447, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0705, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.2145, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6858, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.2080, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1209, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3132, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6734, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch [1/10], Step [100/1519], Loss: 1.6734\n",
      "Loss tensor(0.9600, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.6969, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.7395, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4092, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3878, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2524, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0877, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0916, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7133, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7144, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9255, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9591, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3640, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5232, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1591, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5301, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0534, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4945, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4881, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8250, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.2869, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4941, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1483, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.2075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4261, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.6638, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5671, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9630, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.4760, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.1154, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1111, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.7206, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.0213, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.4915, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3611, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(7.8175, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1358, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0891, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.9098, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6421, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.9899, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9251, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5624, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3206, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.0890, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3747, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.2845, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.7398, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3526, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.0247, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1908, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5673, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6106, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7995, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2366, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.4354, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6197, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8205, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0676, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1482, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7831, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.7420, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1585, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1885, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6116, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.7870, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4426, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9607, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6182, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0182, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.7123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3522, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5275, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8835, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1076, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0757, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8329, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.1001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0438, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2980, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch [1/10], Step [200/1519], Loss: 2.2980\n",
      "Loss tensor(1.4332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3526, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.8337, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8180, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0615, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.5931, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0975, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.3843, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4923, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6397, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8724, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5208, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.3155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.6435, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3585, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.4113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.5928, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5166, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2954, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.6617, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.5074, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4504, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9340, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.1816, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6815, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4106, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1647, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1587, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.2423, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3428, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9263, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4907, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6502, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.9386, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7870, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8633, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0705, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.2377, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(6.9025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.3343, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1213, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.3763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.9220, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.5747, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1226, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1914, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.5717, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.4717, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9331, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.0449, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1865, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6265, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2292, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4389, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8291, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.8344, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.5839, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6244, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2559, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3903, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.6977, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.8087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.0506, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(4.1436, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.6642, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.1307, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.2161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.0925, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8744, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4886, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3909, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.2231, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.7854, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.2140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.4655, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9866, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(5.9967, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(3.3739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(2.1331, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.8342, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3883, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.1425, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(1.7008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.9475, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Loss tensor(0.3727, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass with autograd\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 52\u001b[0m, in \u001b[0;36mGmiSwinTransformer.forward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     50\u001b[0m     normalized_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(three_channel)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Get image features\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     image_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     image_features[i] \u001b[38;5;241m=\u001b[39m image_feature\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# print(\"image_features before reshaping\", image_features.size())\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\models\\swin_transformer.py:838\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 838\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\models\\swin_transformer.py:830\u001b[0m, in \u001b[0;36mSwinTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    829\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[1;32m--> 830\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    831\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\models\\swin_transformer.py:559\u001b[0m, in \u001b[0;36mSwinTransformerStage.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    557\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\models\\swin_transformer.py:408\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    406\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)))\n\u001b[0;32m    407\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\n\u001b[1;32m--> 408\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_path2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, H, W, C)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\layers\\drop.py:179\u001b[0m, in \u001b[0;36mDropPath.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdrop_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_by_keep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\layers\\drop.py:164\u001b[0m, in \u001b[0;36mdrop_path\u001b[1;34m(x, drop_prob, training, scale_by_keep)\u001b[0m\n\u001b[0;32m    162\u001b[0m keep_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m drop_prob\n\u001b[0;32m    163\u001b[0m shape \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# work with diff dim tensors, not just 2D ConvNets\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m random_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_empty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbernoulli_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m scale_by_keep:\n\u001b[0;32m    166\u001b[0m     random_tensor\u001b[38;5;241m.\u001b[39mdiv_(keep_prob)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "n_total_steps = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in tqdm(enumerate(trainloader), desc=\"Training Progress\", total=len(trainloader)):\n",
    "        # Move images and labels to device\n",
    "        images = torch.stack(images).to(device)\n",
    "        # print(\"images before reshaping\", images.size())\n",
    "        images = images.permute(1, 0, 2, 3, 4)  # Change shape to [5, 10, 1, 224, 224]\n",
    "        # print(\"images after reshaping\", images.size())\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass with autograd\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            print(\"Loss\", loss)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa18624b1743468294bb6963fa1ac958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Progress:   0%|          | 0/380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss [2.029275417327881]\n",
      "Test loss [2.029275417327881, 3.117931365966797]\n",
      "Test loss [2.029275417327881, 3.117931365966797, 4.787830352783203]\n",
      "Test loss [2.029275417327881, 3.117931365966797, 4.787830352783203, 1.1730252504348755]\n",
      "Test loss [2.029275417327881, 3.117931365966797, 4.787830352783203, 1.1730252504348755, 1.936651587486267]\n",
      "Test loss [2.029275417327881, 3.117931365966797, 4.787830352783203, 1.1730252504348755, 1.936651587486267, 1.4046493768692017]\n",
      "Test loss [2.029275417327881, 3.117931365966797, 4.787830352783203, 1.1730252504348755, 1.936651587486267, 1.4046493768692017, 1.4569870233535767]\n",
      "Test loss [2.029275417327881, 3.117931365966797, 4.787830352783203, 1.1730252504348755, 1.936651587486267, 1.4046493768692017, 1.4569870233535767, 0.587260365486145]\n",
      "Test loss [2.029275417327881, 3.117931365966797, 4.787830352783203, 1.1730252504348755, 1.936651587486267, 1.4046493768692017, 1.4569870233535767, 0.587260365486145, 1.9755123853683472]\n",
      "Test loss [2.029275417327881, 3.117931365966797, 4.787830352783203, 1.1730252504348755, 1.936651587486267, 1.4046493768692017, 1.4569870233535767, 0.587260365486145, 1.9755123853683472, 1.3687286376953125]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     14\u001b[0m test_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 52\u001b[0m, in \u001b[0;36mGmiSwinTransformer.forward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     50\u001b[0m     normalized_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(three_channel)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Get image features\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     image_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     image_features[i] \u001b[38;5;241m=\u001b[39m image_feature\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# print(\"image_features before reshaping\", image_features.size())\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\models\\swin_transformer.py:838\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 838\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\models\\swin_transformer.py:830\u001b[0m, in \u001b[0;36mSwinTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    829\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[1;32m--> 830\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    831\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\models\\swin_transformer.py:559\u001b[0m, in \u001b[0;36mSwinTransformerStage.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    557\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\models\\swin_transformer.py:408\u001b[0m, in \u001b[0;36mSwinTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    406\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)))\n\u001b[0;32m    407\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\n\u001b[1;32m--> 408\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    409\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, H, W, C)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\timm\\layers\\mlp.py:48\u001b[0m, in \u001b[0;36mMlp.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m---> 48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2(x)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mirzi\\miniconda3\\envs\\gmimodel\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False)\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(testloader, desc=\"Testing Progress\"):\n",
    "        images = torch.stack(images).to(device)\n",
    "        images = images.permute(1, 0, 2, 3, 4)\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        print(\"Test loss\", test_losses)\n",
    "\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "print(f'Average test loss: {avg_test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmimodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
