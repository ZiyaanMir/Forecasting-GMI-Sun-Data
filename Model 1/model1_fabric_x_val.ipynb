{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('../DataLoader')\n",
    "\n",
    "from dataloader_fits import SunImageDataset\n",
    "\n",
    "from lightning.fabric import Fabric\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "fabric = Fabric(accelerator='cuda', devices=1, precision=\"bf16-mixed\")\n",
    "fabric.launch()\n",
    "print(fabric.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 224*224\n",
    "hidden_size = 166\n",
    "num_epochs = 10\n",
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "dropout = 0.5\n",
    "# dropout = 0.6990787087509548\n",
    "\n",
    "k_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "400\n",
      "400\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "dataset = SunImageDataset(csv_file=\"D:\\\\New folder (2)\\\\dataset.csv\", offset=0)\n",
    "\n",
    "total_size = len(dataset)\n",
    "fold_size = total_size // k_folds\n",
    "indices = list(range(total_size))\n",
    "\n",
    "k_fold_1_indices = indices[:fold_size]\n",
    "k_fold_2_indices = indices[fold_size:2*fold_size]\n",
    "k_fold_3_indices = indices[2*fold_size:3*fold_size]\n",
    "k_fold_4_indices = indices[3*fold_size:4*fold_size]\n",
    "k_fold_5_indices = indices[4*fold_size:]\n",
    "\n",
    "print(len(k_fold_1_indices))\n",
    "print(len(k_fold_2_indices))\n",
    "print(len(k_fold_3_indices))\n",
    "print(len(k_fold_4_indices))\n",
    "print(len(k_fold_5_indices))\n",
    "\n",
    "dataloaders = [k_fold_1_indices, k_fold_2_indices, k_fold_3_indices, k_fold_4_indices, k_fold_5_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GmiSwinTransformer(\n",
       "  (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pretrained_model): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): Sequential(\n",
       "      (0): SwinTransformerStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.004)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.004)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.009)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.013)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.013)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.017)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.017)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.022)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.022)\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.026)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.026)\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.030)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.030)\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.035)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.035)\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.039)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.039)\n",
       "          )\n",
       "          (6): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.043)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.043)\n",
       "          )\n",
       "          (7): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.048)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.048)\n",
       "          )\n",
       "          (8): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.052)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.052)\n",
       "          )\n",
       "          (9): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.057)\n",
       "          )\n",
       "          (10): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.061)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.061)\n",
       "          )\n",
       "          (11): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.065)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.065)\n",
       "          )\n",
       "          (12): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.070)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.070)\n",
       "          )\n",
       "          (13): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.074)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.074)\n",
       "          )\n",
       "          (14): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.078)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.078)\n",
       "          )\n",
       "          (15): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.083)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.083)\n",
       "          )\n",
       "          (16): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.087)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.087)\n",
       "          )\n",
       "          (17): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.091)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.096)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.096)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): ClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Linear(in_features=1024, out_features=166, bias=True)\n",
       "      (flatten): Identity()\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1660, out_features=166, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Linear(in_features=166, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class GmiSwinTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(GmiSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Batch normalization for 3 channels\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # Initialize Swin Transformer\n",
    "        self.pretrained_model = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',\n",
    "            pretrained=True,\n",
    "            num_classes=hidden_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size*10, hidden_size),\n",
    "            nn.Dropout(p=dropout),  # Added dropout probability\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            # nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Batch should be in format:\n",
    "        {\n",
    "            'images': torch.FloatTensor((10, 1, 224, 224))\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        images = images.reshape(-1, 1, 224, 224)\n",
    "        images = torch.cat([images, images, images], dim=1)\n",
    "        normalized_images = self.bn(images)\n",
    "        features = self.pretrained_model(normalized_images)\n",
    "        image_features = features.view(batch_size, -1)\n",
    "        \n",
    "        output = self.fc(image_features)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GmiSwinTransformer(hidden_size=hidden_size).to(device)\n",
    "model = GmiSwinTransformer(hidden_size=hidden_size)\n",
    "\n",
    "# print(torchsummary.summary(model, (10, 1, 224, 224)))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# model, optimizer = fabric.setup(model, optimizer)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_skill_score(all_outputs, all_labels):\n",
    "    storm_labels = []\n",
    "    for output in all_labels:\n",
    "        if output < 5:\n",
    "            storm_labels.append(0)\n",
    "        else:\n",
    "            storm_labels.append(1)\n",
    "    storm_outputs = []\n",
    "    for output in all_outputs:\n",
    "        if output < 5:\n",
    "            storm_outputs.append(0)\n",
    "        else:\n",
    "            storm_outputs.append(1)\n",
    "    \n",
    "    # Calculate true positive, true negative, false positive, false negative\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(len(storm_labels)):\n",
    "        if storm_labels[i] == 1 and storm_outputs[i] == 1:\n",
    "            tp += 1\n",
    "        elif storm_labels[i] == 0 and storm_outputs[i] == 0:\n",
    "            tn += 1\n",
    "        elif storm_labels[i] == 0 and storm_outputs[i] == 1:\n",
    "            fp += 1\n",
    "        elif storm_labels[i] == 1 and storm_outputs[i] == 0:\n",
    "            fn += 1\n",
    "            \n",
    "    # Calculate true skill score\n",
    "    tss = (tp / (tp + fp)) - (fp / (fp + tn))\n",
    "    return tss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3be57478cc4403b37af79a6a5c0b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Index: 0, Loss: 4.1873\n",
      "Epoch: 1, Index: 1, Loss: 19.5254\n",
      "Epoch: 1, Index: 2, Loss: 6.1440\n",
      "Epoch: 1, Index: 3, Loss: 8.6639\n",
      "Epoch: 1, Index: 4, Loss: 2.6539\n",
      "Epoch: 1, Index: 5, Loss: 3.2553\n",
      "Epoch: 1, Index: 6, Loss: 6.0870\n",
      "Epoch: 1, Index: 7, Loss: 4.8735\n",
      "Epoch: 1, Index: 8, Loss: 1.2123\n",
      "Epoch: 1, Index: 9, Loss: 2.1052\n",
      "Epoch: 1, Index: 10, Loss: 0.5993\n",
      "Epoch: 1, Index: 11, Loss: 0.1962\n",
      "Epoch: 1, Index: 12, Loss: 0.9216\n",
      "Epoch: 1, Index: 13, Loss: 2.1197\n",
      "Epoch: 1, Index: 14, Loss: 2.0162\n",
      "Epoch: 1, Index: 15, Loss: 2.0538\n",
      "Epoch: 1, Index: 16, Loss: 3.1388\n",
      "Epoch: 1, Index: 17, Loss: 1.5171\n",
      "Epoch: 1, Index: 18, Loss: 1.6811\n",
      "Epoch: 1, Index: 19, Loss: 3.8057\n",
      "Epoch: 1, Index: 20, Loss: 1.4771\n",
      "Epoch: 1, Index: 21, Loss: 3.9986\n",
      "Epoch: 1, Index: 22, Loss: 1.0838\n",
      "Epoch: 1, Index: 23, Loss: 2.6180\n",
      "Epoch: 1, Index: 24, Loss: 2.7115\n",
      "Epoch: 1, Index: 25, Loss: 0.9319\n",
      "Epoch: 1, Index: 26, Loss: 1.8686\n",
      "Epoch: 1, Index: 27, Loss: 0.3673\n",
      "Epoch: 1, Index: 28, Loss: 1.9875\n",
      "Epoch: 1, Index: 29, Loss: 0.1867\n",
      "Epoch: 1, Index: 30, Loss: 2.0969\n",
      "Epoch: 1, Index: 31, Loss: 1.9986\n",
      "Epoch: 1, Index: 32, Loss: 3.2020\n",
      "Epoch: 1, Index: 33, Loss: 6.9327\n",
      "Epoch: 1, Index: 34, Loss: 1.3635\n",
      "Epoch: 1, Index: 35, Loss: 0.6634\n",
      "Epoch: 1, Index: 36, Loss: 2.9913\n",
      "Epoch: 1, Index: 37, Loss: 10.5659\n",
      "Epoch: 1, Index: 38, Loss: 8.4376\n",
      "Epoch: 1, Index: 39, Loss: 10.1759\n",
      "Epoch: 1, Index: 40, Loss: 2.5347\n",
      "Epoch: 1, Index: 41, Loss: 0.6073\n",
      "Epoch: 1, Index: 42, Loss: 0.8300\n",
      "Epoch: 1, Index: 43, Loss: 0.7089\n",
      "Epoch: 1, Index: 44, Loss: 0.1103\n",
      "Epoch: 1, Index: 45, Loss: 12.3331\n",
      "Epoch: 1, Index: 46, Loss: 2.1679\n",
      "Epoch: 1, Index: 47, Loss: 1.6389\n",
      "Epoch: 1, Index: 48, Loss: 8.7966\n",
      "Epoch: 1, Index: 49, Loss: 2.1931\n",
      "Epoch: 1, Index: 50, Loss: 0.0850\n",
      "Epoch: 1, Index: 51, Loss: 8.4635\n",
      "Epoch: 1, Index: 52, Loss: 30.3843\n",
      "Epoch: 1, Index: 53, Loss: 17.8696\n",
      "Epoch: 1, Index: 54, Loss: 11.6039\n",
      "Epoch: 1, Index: 55, Loss: 7.9158\n",
      "Epoch: 1, Index: 56, Loss: 2.5695\n",
      "Epoch: 1, Index: 57, Loss: 0.0338\n",
      "Epoch: 1, Index: 58, Loss: 0.1518\n",
      "Epoch: 1, Index: 59, Loss: 2.6170\n",
      "Epoch: 1, Index: 60, Loss: 0.8006\n",
      "Epoch: 1, Index: 61, Loss: 5.4581\n",
      "Epoch: 1, Index: 62, Loss: 2.1872\n",
      "Epoch: 1, Index: 63, Loss: 0.1464\n",
      "Epoch: 1, Index: 64, Loss: 4.4988\n",
      "Epoch: 1, Index: 65, Loss: 16.8261\n",
      "Epoch: 1, Index: 66, Loss: 0.0651\n",
      "Epoch: 1, Index: 67, Loss: 1.5953\n",
      "Epoch: 1, Index: 68, Loss: 0.4475\n",
      "Epoch: 1, Index: 69, Loss: 1.4405\n",
      "Epoch: 1, Index: 70, Loss: 5.3871\n",
      "Epoch: 1, Index: 71, Loss: 1.2479\n",
      "Epoch: 1, Index: 72, Loss: 0.1381\n",
      "Epoch: 1, Index: 73, Loss: 1.5101\n",
      "Epoch: 1, Index: 74, Loss: 0.2883\n",
      "Epoch: 1, Index: 75, Loss: 7.7475\n",
      "Epoch: 1, Index: 76, Loss: 9.9156\n",
      "Epoch: 1, Index: 77, Loss: 3.9142\n",
      "Epoch: 1, Index: 78, Loss: 0.1523\n",
      "Epoch: 1, Index: 79, Loss: 4.1467\n",
      "Epoch: 1, Index: 80, Loss: 2.8700\n",
      "Epoch: 1, Index: 81, Loss: 5.5373\n",
      "Epoch: 1, Index: 82, Loss: 7.5952\n",
      "Epoch: 1, Index: 83, Loss: 8.9879\n",
      "Epoch: 1, Index: 84, Loss: 0.8647\n",
      "Epoch: 1, Index: 85, Loss: 0.1745\n",
      "Epoch: 1, Index: 86, Loss: 0.4078\n",
      "Epoch: 1, Index: 87, Loss: 0.4904\n",
      "Epoch: 1, Index: 88, Loss: 3.9204\n",
      "Epoch: 1, Index: 89, Loss: 1.2921\n",
      "Epoch: 1, Index: 90, Loss: 1.1134\n",
      "Epoch: 1, Index: 91, Loss: 0.9263\n",
      "Epoch: 1, Index: 92, Loss: 0.1232\n",
      "Epoch: 1, Index: 93, Loss: 0.1427\n",
      "Epoch: 1, Index: 94, Loss: 1.7516\n",
      "Epoch: 1, Index: 95, Loss: 3.6658\n",
      "Epoch: 1, Index: 96, Loss: 3.4291\n",
      "Epoch: 1, Index: 97, Loss: 0.2114\n",
      "Epoch: 1, Index: 98, Loss: 0.3737\n",
      "Epoch: 1, Index: 99, Loss: 0.6325\n",
      "Epoch: 1, Index: 100, Loss: 2.6944\n",
      "Epoch: 1, Index: 101, Loss: 0.2545\n",
      "Epoch: 1, Index: 102, Loss: 4.8187\n",
      "Epoch: 1, Index: 103, Loss: 5.8129\n",
      "Epoch: 1, Index: 104, Loss: 9.0369\n",
      "Epoch: 1, Index: 105, Loss: 0.5529\n",
      "Epoch: 1, Index: 106, Loss: 4.7642\n",
      "Epoch: 1, Index: 107, Loss: 0.5921\n",
      "Epoch: 1, Index: 108, Loss: 12.9151\n",
      "Epoch: 1, Index: 109, Loss: 2.4158\n",
      "Epoch: 1, Index: 110, Loss: 1.9133\n",
      "Epoch: 1, Index: 111, Loss: 3.9475\n",
      "Epoch: 1, Index: 112, Loss: 1.9438\n",
      "Epoch: 1, Index: 113, Loss: 5.1452\n",
      "Epoch: 1, Index: 114, Loss: 2.1471\n",
      "Epoch: 1, Index: 115, Loss: 2.3655\n",
      "Epoch: 1, Index: 116, Loss: 1.2763\n",
      "Epoch: 1, Index: 117, Loss: 1.6978\n",
      "Epoch: 1, Index: 118, Loss: 0.3709\n",
      "Epoch: 1, Index: 119, Loss: 0.3093\n",
      "Epoch: 1, Index: 120, Loss: 6.0081\n",
      "Epoch: 1, Index: 121, Loss: 4.3648\n",
      "Epoch: 1, Index: 122, Loss: 4.2545\n",
      "Epoch: 1, Index: 123, Loss: 2.2323\n",
      "Epoch: 1, Index: 124, Loss: 1.2353\n",
      "Epoch: 1, Index: 125, Loss: 2.4171\n",
      "Epoch: 1, Index: 126, Loss: 1.1306\n",
      "Epoch: 1, Index: 127, Loss: 4.5805\n",
      "Epoch: 1, Index: 128, Loss: 1.7798\n",
      "Epoch: 1, Index: 129, Loss: 1.4173\n",
      "Epoch: 1, Index: 130, Loss: 0.0132\n",
      "Epoch: 1, Index: 131, Loss: 3.1457\n",
      "Epoch: 1, Index: 132, Loss: 0.0359\n",
      "Epoch: 1, Index: 133, Loss: 1.4944\n",
      "Epoch: 1, Index: 134, Loss: 6.0909\n",
      "Epoch: 1, Index: 135, Loss: 1.5025\n",
      "Epoch: 1, Index: 136, Loss: 0.2706\n",
      "Epoch: 1, Index: 137, Loss: 1.8211\n",
      "Epoch: 1, Index: 138, Loss: 0.9922\n",
      "Epoch: 1, Index: 139, Loss: 0.6777\n",
      "Epoch: 1, Index: 140, Loss: 1.1299\n",
      "Epoch: 1, Index: 141, Loss: 2.9999\n",
      "Epoch: 1, Index: 142, Loss: 1.3161\n",
      "Epoch: 1, Index: 143, Loss: 0.2209\n",
      "Epoch: 1, Index: 144, Loss: 1.7137\n",
      "Epoch: 1, Index: 145, Loss: 0.0891\n",
      "Epoch: 1, Index: 146, Loss: 2.2374\n",
      "Epoch: 1, Index: 147, Loss: 1.4069\n",
      "Epoch: 1, Index: 148, Loss: 0.5032\n",
      "Epoch: 1, Index: 149, Loss: 7.1489\n",
      "Epoch: 1, Index: 150, Loss: 4.4167\n",
      "Epoch: 1, Index: 151, Loss: 5.9601\n",
      "Epoch: 1, Index: 152, Loss: 4.9631\n",
      "Epoch: 1, Index: 153, Loss: 0.2154\n",
      "Epoch: 1, Index: 154, Loss: 0.1497\n",
      "Epoch: 1, Index: 155, Loss: 1.3457\n",
      "Epoch: 1, Index: 156, Loss: 5.8434\n",
      "Epoch: 1, Index: 157, Loss: 0.2659\n",
      "Epoch: 1, Index: 158, Loss: 6.6081\n",
      "Epoch: 1, Index: 159, Loss: 3.0228\n",
      "Epoch: 1, Index: 160, Loss: 2.1674\n",
      "Epoch: 1, Index: 161, Loss: 0.8217\n",
      "Epoch: 1, Index: 162, Loss: 16.2136\n",
      "Epoch: 1, Index: 163, Loss: 0.2180\n",
      "Epoch: 1, Index: 164, Loss: 3.9637\n",
      "Epoch: 1, Index: 165, Loss: 1.0435\n",
      "Epoch: 1, Index: 166, Loss: 0.5547\n",
      "Epoch: 1, Index: 167, Loss: 4.2091\n",
      "Epoch: 1, Index: 168, Loss: 4.2185\n",
      "Epoch: 1, Index: 169, Loss: 0.2408\n",
      "Epoch: 1, Index: 170, Loss: 1.0250\n",
      "Epoch: 1, Index: 171, Loss: 6.5818\n",
      "Epoch: 1, Index: 172, Loss: 0.9313\n",
      "Epoch: 1, Index: 173, Loss: 0.1005\n",
      "Epoch: 1, Index: 174, Loss: 0.4018\n",
      "Epoch: 1, Index: 175, Loss: 1.4815\n",
      "Epoch: 1, Index: 176, Loss: 0.4477\n",
      "Epoch: 1, Index: 177, Loss: 0.6121\n",
      "Epoch: 1, Index: 178, Loss: 0.2691\n",
      "Epoch: 1, Index: 179, Loss: 0.5630\n",
      "Epoch: 1, Index: 180, Loss: 0.3116\n",
      "Epoch: 1, Index: 181, Loss: 3.2401\n",
      "Epoch: 1, Index: 182, Loss: 1.0706\n",
      "Epoch: 1, Index: 183, Loss: 3.8354\n",
      "Epoch: 1, Index: 184, Loss: 2.5418\n",
      "Epoch: 1, Index: 185, Loss: 2.0705\n",
      "Epoch: 1, Index: 186, Loss: 0.8328\n",
      "Epoch: 1, Index: 187, Loss: 1.4683\n",
      "Epoch: 1, Index: 188, Loss: 9.3098\n",
      "Epoch: 1, Index: 189, Loss: 1.2463\n",
      "Epoch: 1, Index: 190, Loss: 4.4006\n",
      "Epoch: 1, Index: 191, Loss: 4.4348\n",
      "Epoch: 1, Index: 192, Loss: 0.8658\n",
      "Epoch: 1, Index: 193, Loss: 1.2542\n",
      "Epoch: 1, Index: 194, Loss: 0.1983\n",
      "Epoch: 1, Index: 195, Loss: 0.2562\n",
      "Epoch: 1, Index: 196, Loss: 0.4822\n",
      "Epoch: 1, Index: 197, Loss: 2.0026\n",
      "Epoch: 1, Index: 198, Loss: 3.1196\n",
      "Epoch: 1, Index: 199, Loss: 7.5268\n",
      "Epoch: 1, Index: 200, Loss: 0.3886\n",
      "Epoch: 1, Index: 201, Loss: 3.6129\n",
      "Epoch: 1, Index: 202, Loss: 3.8960\n",
      "Epoch: 1, Index: 203, Loss: 2.4680\n",
      "Epoch: 1, Index: 204, Loss: 5.1578\n",
      "Epoch: 1, Index: 205, Loss: 2.1768\n",
      "Epoch: 1, Index: 206, Loss: 0.0049\n",
      "Epoch: 1, Index: 207, Loss: 0.5460\n",
      "Epoch: 1, Index: 208, Loss: 0.3664\n",
      "Epoch: 1, Index: 209, Loss: 10.2043\n",
      "Epoch: 1, Index: 210, Loss: 0.9682\n",
      "Epoch: 1, Index: 211, Loss: 0.5467\n",
      "Epoch: 1, Index: 212, Loss: 0.7541\n",
      "Epoch: 1, Index: 213, Loss: 2.6429\n",
      "Epoch: 1, Index: 214, Loss: 0.1659\n",
      "Epoch: 1, Index: 215, Loss: 4.1042\n",
      "Epoch: 1, Index: 216, Loss: 1.4211\n",
      "Epoch: 1, Index: 217, Loss: 0.1071\n",
      "Epoch: 1, Index: 218, Loss: 3.4426\n",
      "Epoch: 1, Index: 219, Loss: 7.6716\n",
      "Epoch: 1, Index: 220, Loss: 0.2175\n",
      "Epoch: 1, Index: 221, Loss: 1.1924\n",
      "Epoch: 1, Index: 222, Loss: 1.1499\n",
      "Epoch: 1, Index: 223, Loss: 0.0767\n",
      "Epoch: 1, Index: 224, Loss: 0.6724\n",
      "Epoch: 1, Index: 225, Loss: 1.1378\n",
      "Epoch: 1, Index: 226, Loss: 3.7913\n",
      "Epoch: 1, Index: 227, Loss: 1.4661\n",
      "Epoch: 1, Index: 228, Loss: 0.2739\n",
      "Epoch: 1, Index: 229, Loss: 2.5642\n",
      "Epoch: 1, Index: 230, Loss: 0.9680\n",
      "Epoch: 1, Index: 231, Loss: 0.3882\n",
      "Epoch: 1, Index: 232, Loss: 0.3345\n",
      "Epoch: 1, Index: 233, Loss: 2.1693\n",
      "Epoch: 1, Index: 234, Loss: 1.3193\n",
      "Epoch: 1, Index: 235, Loss: 0.5867\n",
      "Epoch: 1, Index: 236, Loss: 0.8818\n",
      "Epoch: 1, Index: 237, Loss: 18.3907\n",
      "Epoch: 1, Index: 238, Loss: 2.0618\n",
      "Epoch: 1, Index: 239, Loss: 0.9178\n",
      "Epoch: 1, Index: 240, Loss: 2.7872\n",
      "Epoch: 1, Index: 241, Loss: 1.2352\n",
      "Epoch: 1, Index: 242, Loss: 2.7226\n",
      "Epoch: 1, Index: 243, Loss: 1.2084\n",
      "Epoch: 1, Index: 244, Loss: 1.8206\n",
      "Epoch: 1, Index: 245, Loss: 0.8858\n",
      "Epoch: 1, Index: 246, Loss: 0.6777\n",
      "Epoch: 1, Index: 247, Loss: 1.5361\n",
      "Epoch: 1, Index: 248, Loss: 1.9619\n",
      "Epoch: 1, Index: 249, Loss: 0.5694\n",
      "Epoch: 1, Index: 250, Loss: 3.7707\n",
      "Epoch: 1, Index: 251, Loss: 0.4768\n",
      "Epoch: 1, Index: 252, Loss: 1.8850\n",
      "Epoch: 1, Index: 253, Loss: 0.4820\n",
      "Epoch: 1, Index: 254, Loss: 0.3976\n",
      "Epoch: 1, Index: 255, Loss: 1.0431\n",
      "Epoch: 1, Index: 256, Loss: 4.3542\n",
      "Epoch: 1, Index: 257, Loss: 11.6207\n",
      "Epoch: 1, Index: 258, Loss: 2.8549\n",
      "Epoch: 1, Index: 259, Loss: 1.4623\n",
      "Epoch: 1, Index: 260, Loss: 6.9552\n",
      "Epoch: 1, Index: 261, Loss: 0.8731\n",
      "Epoch: 1, Index: 262, Loss: 0.1253\n",
      "Epoch: 1, Index: 263, Loss: 8.8461\n",
      "Epoch: 1, Index: 264, Loss: 5.7335\n",
      "Epoch: 1, Index: 265, Loss: 2.2884\n",
      "Epoch: 1, Index: 266, Loss: 2.5230\n",
      "Epoch: 1, Index: 267, Loss: 1.2250\n",
      "Epoch: 1, Index: 268, Loss: 3.3202\n",
      "Epoch: 1, Index: 269, Loss: 1.1488\n",
      "Epoch: 1, Index: 270, Loss: 1.5426\n",
      "Epoch: 1, Index: 271, Loss: 6.3696\n",
      "Epoch: 1, Index: 272, Loss: 0.0011\n",
      "Epoch: 1, Index: 273, Loss: 3.5567\n",
      "Epoch: 1, Index: 274, Loss: 2.2498\n",
      "Epoch: 1, Index: 275, Loss: 0.6034\n",
      "Epoch: 1, Index: 276, Loss: 3.5222\n",
      "Epoch: 1, Index: 277, Loss: 1.7944\n",
      "Epoch: 1, Index: 278, Loss: 0.2897\n",
      "Epoch: 1, Index: 279, Loss: 3.3136\n",
      "Epoch: 1, Index: 280, Loss: 5.5517\n",
      "Epoch: 1, Index: 281, Loss: 1.2642\n",
      "Epoch: 1, Index: 282, Loss: 0.4740\n",
      "Epoch: 1, Index: 283, Loss: 0.3485\n",
      "Epoch: 1, Index: 284, Loss: 1.7685\n",
      "Epoch: 1, Index: 285, Loss: 0.7210\n",
      "Epoch: 1, Index: 286, Loss: 0.2328\n",
      "Epoch: 1, Index: 287, Loss: 2.5400\n",
      "Epoch: 1, Index: 288, Loss: 1.9929\n",
      "Epoch: 1, Index: 289, Loss: 1.9163\n",
      "Epoch: 1, Index: 290, Loss: 2.2633\n",
      "Epoch: 1, Index: 291, Loss: 2.8528\n",
      "Epoch: 1, Index: 292, Loss: 2.2200\n",
      "Epoch: 1, Index: 293, Loss: 2.2631\n",
      "Epoch: 1, Index: 294, Loss: 2.9268\n",
      "Epoch: 1, Index: 295, Loss: 0.7029\n",
      "Epoch: 1, Index: 296, Loss: 4.2181\n",
      "Epoch: 1, Index: 297, Loss: 1.9258\n",
      "Epoch: 1, Index: 298, Loss: 1.2302\n",
      "Epoch: 1, Index: 299, Loss: 0.9187\n",
      "Epoch: 1, Index: 300, Loss: 2.5078\n",
      "Epoch: 1, Index: 301, Loss: 2.4135\n",
      "Epoch: 1, Index: 302, Loss: 0.3601\n",
      "Epoch: 1, Index: 303, Loss: 0.6516\n",
      "Epoch: 1, Index: 304, Loss: 1.3564\n",
      "Epoch: 1, Index: 305, Loss: 5.0575\n",
      "Epoch: 1, Index: 306, Loss: 0.4996\n",
      "Epoch: 1, Index: 307, Loss: 0.0908\n",
      "Epoch: 1, Index: 308, Loss: 0.0388\n",
      "Epoch: 1, Index: 309, Loss: 1.2416\n",
      "Epoch: 1, Index: 310, Loss: 1.0774\n",
      "Epoch: 1, Index: 311, Loss: 0.5337\n",
      "Epoch: 1, Index: 312, Loss: 4.7617\n",
      "Epoch: 1, Index: 313, Loss: 0.3231\n",
      "Epoch: 1, Index: 314, Loss: 0.3368\n",
      "Epoch: 1, Index: 315, Loss: 3.8518\n",
      "Epoch: 1, Index: 316, Loss: 2.3113\n",
      "Epoch: 1, Index: 317, Loss: 3.8252\n",
      "Epoch: 1, Index: 318, Loss: 2.9943\n",
      "Epoch: 1, Index: 319, Loss: 3.7854\n",
      "Epoch: 1, Index: 320, Loss: 0.6789\n",
      "Epoch: 1, Index: 321, Loss: 1.3687\n",
      "Epoch: 1, Index: 322, Loss: 1.4926\n",
      "Epoch: 1, Index: 323, Loss: 0.6165\n",
      "Epoch: 1, Index: 324, Loss: 2.3492\n",
      "Epoch: 1, Index: 325, Loss: 0.8723\n",
      "Epoch: 1, Index: 326, Loss: 1.6297\n",
      "Epoch: 1, Index: 327, Loss: 1.6210\n",
      "Epoch: 1, Index: 328, Loss: 1.5167\n",
      "Epoch: 1, Index: 329, Loss: 1.4970\n",
      "Epoch: 1, Index: 330, Loss: 0.3383\n",
      "Epoch: 1, Index: 331, Loss: 1.2530\n",
      "Epoch: 1, Index: 332, Loss: 1.5820\n",
      "Epoch: 1, Index: 333, Loss: 1.5916\n",
      "Epoch: 1, Index: 334, Loss: 0.6640\n",
      "Epoch: 1, Index: 335, Loss: 1.2731\n",
      "Epoch: 1, Index: 336, Loss: 0.0743\n",
      "Epoch: 1, Index: 337, Loss: 0.5572\n",
      "Epoch: 1, Index: 338, Loss: 2.1866\n",
      "Epoch: 1, Index: 339, Loss: 0.1743\n",
      "Epoch: 1, Index: 340, Loss: 0.7398\n",
      "Epoch: 1, Index: 341, Loss: 0.3227\n",
      "Epoch: 1, Index: 342, Loss: 1.2734\n",
      "Epoch: 1, Index: 343, Loss: 4.3523\n",
      "Epoch: 1, Index: 344, Loss: 1.5225\n",
      "Epoch: 1, Index: 345, Loss: 2.0197\n",
      "Epoch: 1, Index: 346, Loss: 3.4545\n",
      "Epoch: 1, Index: 347, Loss: 0.0443\n",
      "Epoch: 1, Index: 348, Loss: 0.8625\n",
      "Epoch: 1, Index: 349, Loss: 4.9073\n",
      "Epoch: 1, Index: 350, Loss: 4.2542\n",
      "Epoch: 1, Index: 351, Loss: 1.3705\n",
      "Epoch: 1, Index: 352, Loss: 3.2827\n",
      "Epoch: 1, Index: 353, Loss: 5.8152\n",
      "Epoch: 1, Index: 354, Loss: 6.7353\n",
      "Epoch: 1, Index: 355, Loss: 1.3076\n",
      "Epoch: 1, Index: 356, Loss: 6.8497\n",
      "Epoch: 1, Index: 357, Loss: 3.2132\n",
      "Epoch: 1, Index: 358, Loss: 4.8233\n",
      "Epoch: 1, Index: 359, Loss: 0.1039\n",
      "Epoch: 1, Index: 360, Loss: 0.8074\n",
      "Epoch: 1, Index: 361, Loss: 9.4988\n",
      "Epoch: 1, Index: 362, Loss: 1.8333\n",
      "Epoch: 1, Index: 363, Loss: 0.6281\n",
      "Epoch: 1, Index: 364, Loss: 26.5026\n",
      "Epoch: 1, Index: 365, Loss: 3.5104\n",
      "Epoch: 1, Index: 366, Loss: 6.7753\n",
      "Epoch: 1, Index: 367, Loss: 5.7793\n",
      "Epoch: 1, Index: 368, Loss: 0.7290\n",
      "Epoch: 1, Index: 369, Loss: 1.8409\n",
      "Epoch: 1, Index: 370, Loss: 0.8622\n",
      "Epoch: 1, Index: 371, Loss: 3.2675\n",
      "Epoch: 1, Index: 372, Loss: 0.6544\n",
      "Epoch: 1, Index: 373, Loss: 0.6828\n",
      "Epoch: 1, Index: 374, Loss: 6.4570\n",
      "Epoch: 1, Index: 375, Loss: 1.2600\n",
      "Epoch: 1, Index: 376, Loss: 0.4749\n",
      "Epoch: 1, Index: 377, Loss: 2.0490\n",
      "Epoch: 1, Index: 378, Loss: 0.5214\n",
      "Epoch: 1, Index: 379, Loss: 0.0824\n",
      "Epoch: 1, Index: 380, Loss: 1.4996\n",
      "Epoch: 1, Index: 381, Loss: 1.9984\n",
      "Epoch: 1, Index: 382, Loss: 0.4879\n",
      "Epoch: 1, Index: 383, Loss: 0.4702\n",
      "Epoch: 1, Index: 384, Loss: 1.5816\n",
      "Epoch: 1, Index: 385, Loss: 1.9034\n",
      "Epoch: 1, Index: 386, Loss: 1.8171\n",
      "Epoch: 1, Index: 387, Loss: 2.7126\n",
      "Epoch: 1, Index: 388, Loss: 0.3150\n",
      "Epoch: 1, Index: 389, Loss: 5.6818\n",
      "Epoch: 1, Index: 390, Loss: 0.5533\n",
      "Epoch: 1, Index: 391, Loss: 4.7442\n",
      "Epoch: 1, Index: 392, Loss: 3.2311\n",
      "Epoch: 1, Index: 393, Loss: 0.0981\n",
      "Epoch: 1, Index: 394, Loss: 13.6628\n",
      "Epoch: 1, Index: 395, Loss: 1.6745\n",
      "Epoch: 1, Index: 396, Loss: 0.1652\n",
      "Epoch: 1, Index: 397, Loss: 1.1771\n",
      "Epoch: 1, Index: 398, Loss: 0.1651\n",
      "Epoch: 1, Index: 399, Loss: 1.0647\n",
      "Epoch: 1, Index: 400, Loss: 1.7931\n",
      "Epoch: 1, Index: 401, Loss: 3.3362\n",
      "Epoch: 1, Index: 402, Loss: 0.2545\n",
      "Epoch: 1, Index: 403, Loss: 0.3258\n",
      "Epoch: 1, Index: 404, Loss: 3.8616\n",
      "Epoch: 1, Index: 405, Loss: 6.8427\n",
      "Epoch: 1, Index: 406, Loss: 2.1265\n",
      "Epoch: 1, Index: 407, Loss: 4.1330\n",
      "Epoch: 1, Index: 408, Loss: 7.5274\n",
      "Epoch: 1, Index: 409, Loss: 0.9081\n",
      "Epoch: 1, Index: 410, Loss: 0.0517\n",
      "Epoch: 1, Index: 411, Loss: 2.9903\n",
      "Epoch: 1, Index: 412, Loss: 0.3189\n",
      "Epoch: 1, Index: 413, Loss: 0.1423\n",
      "Epoch: 1, Index: 414, Loss: 1.1383\n",
      "Epoch: 1, Index: 415, Loss: 4.3570\n",
      "Epoch: 1, Index: 416, Loss: 1.2150\n",
      "Epoch: 1, Index: 417, Loss: 3.1224\n",
      "Epoch: 1, Index: 418, Loss: 3.6540\n",
      "Epoch: 1, Index: 419, Loss: 0.1881\n",
      "Epoch: 1, Index: 420, Loss: 1.5791\n",
      "Epoch: 1, Index: 421, Loss: 0.5893\n",
      "Epoch: 1, Index: 422, Loss: 1.2107\n",
      "Epoch: 1, Index: 423, Loss: 0.6019\n",
      "Epoch: 1, Index: 424, Loss: 1.0629\n",
      "Epoch: 1, Index: 425, Loss: 0.4699\n",
      "Epoch: 1, Index: 426, Loss: 0.0166\n",
      "Epoch: 1, Index: 427, Loss: 4.5418\n",
      "Epoch: 1, Index: 428, Loss: 0.7629\n",
      "Epoch: 1, Index: 429, Loss: 0.5898\n",
      "Epoch: 1, Index: 430, Loss: 6.7600\n",
      "Epoch: 1, Index: 431, Loss: 2.0332\n",
      "Epoch: 1, Index: 432, Loss: 1.7545\n",
      "Epoch: 1, Index: 433, Loss: 4.7228\n",
      "Epoch: 1, Index: 434, Loss: 1.2237\n",
      "Epoch: 1, Index: 435, Loss: 1.2237\n",
      "Epoch: 1, Index: 436, Loss: 2.4182\n",
      "Epoch: 1, Index: 437, Loss: 1.8815\n",
      "Epoch: 1, Index: 438, Loss: 0.3138\n",
      "Epoch: 1, Index: 439, Loss: 0.5957\n",
      "Epoch: 1, Index: 440, Loss: 1.3202\n",
      "Epoch: 1, Index: 441, Loss: 11.9505\n",
      "Epoch: 1, Index: 442, Loss: 4.6033\n",
      "Epoch: 1, Index: 443, Loss: 0.6728\n",
      "Epoch: 1, Index: 444, Loss: 0.1414\n",
      "Epoch: 1, Index: 445, Loss: 3.5214\n",
      "Epoch: 1, Index: 446, Loss: 1.3890\n",
      "Epoch: 1, Index: 447, Loss: 0.1663\n",
      "Epoch: 1, Index: 448, Loss: 0.3245\n",
      "Epoch: 1, Index: 449, Loss: 1.7485\n",
      "Epoch: 1, Index: 450, Loss: 2.3877\n",
      "Epoch: 1, Index: 451, Loss: 1.0520\n",
      "Epoch: 1, Index: 452, Loss: 2.9838\n",
      "Epoch: 1, Index: 453, Loss: 2.4879\n",
      "Epoch: 1, Index: 454, Loss: 0.1540\n",
      "Epoch: 1, Index: 455, Loss: 0.7665\n",
      "Epoch: 1, Index: 456, Loss: 2.4244\n",
      "Epoch: 1, Index: 457, Loss: 0.4444\n",
      "Epoch: 1, Index: 458, Loss: 0.6506\n",
      "Epoch: 1, Index: 459, Loss: 0.6598\n",
      "Epoch: 1, Index: 460, Loss: 1.9228\n",
      "Epoch: 1, Index: 461, Loss: 0.2928\n",
      "Epoch: 1, Index: 462, Loss: 0.1871\n",
      "Epoch: 1, Index: 463, Loss: 1.1183\n",
      "Epoch: 1, Index: 464, Loss: 3.9723\n",
      "Epoch: 1, Index: 465, Loss: 9.0532\n",
      "Epoch: 1, Index: 466, Loss: 0.9150\n",
      "Epoch: 1, Index: 467, Loss: 0.3693\n",
      "Epoch: 1, Index: 468, Loss: 1.0322\n",
      "Epoch: 1, Index: 469, Loss: 0.0214\n",
      "Epoch: 1, Index: 470, Loss: 2.7320\n",
      "Epoch: 1, Index: 471, Loss: 0.3105\n",
      "Epoch: 1, Index: 472, Loss: 1.0823\n",
      "Epoch: 1, Index: 473, Loss: 8.9034\n",
      "Epoch: 1, Index: 474, Loss: 1.4574\n",
      "Epoch: 1, Index: 475, Loss: 1.1196\n",
      "Epoch: 1, Index: 476, Loss: 4.1424\n",
      "Epoch: 1, Index: 477, Loss: 0.5670\n",
      "Epoch: 1, Index: 478, Loss: 1.1054\n",
      "Epoch: 1, Index: 479, Loss: 0.0702\n",
      "Epoch: 1, Index: 480, Loss: 1.7605\n",
      "Epoch: 1, Index: 481, Loss: 2.0485\n",
      "Epoch: 1, Index: 482, Loss: 2.7388\n",
      "Epoch: 1, Index: 483, Loss: 0.9290\n",
      "Epoch: 1, Index: 484, Loss: 4.3272\n",
      "Epoch: 1, Index: 485, Loss: 0.9783\n",
      "Epoch: 1, Index: 486, Loss: 6.7740\n",
      "Epoch: 1, Index: 487, Loss: 1.4557\n",
      "Epoch: 1, Index: 488, Loss: 0.0966\n",
      "Epoch: 1, Index: 489, Loss: 9.5090\n",
      "Epoch: 1, Index: 490, Loss: 0.4759\n",
      "Epoch: 1, Index: 491, Loss: 1.2239\n",
      "Epoch: 1, Index: 492, Loss: 1.3216\n",
      "Epoch: 1, Index: 493, Loss: 1.3302\n",
      "Epoch: 1, Index: 494, Loss: 0.7754\n",
      "Epoch: 1, Index: 495, Loss: 4.9328\n",
      "Epoch: 1, Index: 496, Loss: 1.5194\n",
      "Epoch: 1, Index: 497, Loss: 2.4205\n",
      "Epoch: 1, Index: 498, Loss: 6.8228\n",
      "Epoch: 1, Index: 499, Loss: 2.6840\n",
      "Epoch: 1, Index: 500, Loss: 0.5651\n",
      "Epoch: 1, Index: 501, Loss: 3.2409\n",
      "Epoch: 1, Index: 502, Loss: 2.5281\n",
      "Epoch: 1, Index: 503, Loss: 2.4004\n",
      "Epoch: 1, Index: 504, Loss: 2.8254\n",
      "Epoch: 1, Index: 505, Loss: 5.0432\n",
      "Epoch: 1, Index: 506, Loss: 1.6673\n",
      "Epoch: 1, Index: 507, Loss: 0.9181\n",
      "Epoch: 1, Index: 508, Loss: 0.7305\n",
      "Epoch: 1, Index: 509, Loss: 13.9073\n",
      "Epoch: 1, Index: 510, Loss: 1.0246\n",
      "Epoch: 1, Index: 511, Loss: 0.0315\n",
      "Epoch: 1, Index: 512, Loss: 0.0807\n",
      "Epoch: 1, Index: 513, Loss: 0.8405\n",
      "Epoch: 1, Index: 514, Loss: 1.9382\n",
      "Epoch: 1, Index: 515, Loss: 3.9357\n",
      "Epoch: 1, Index: 516, Loss: 6.4467\n",
      "Epoch: 1, Index: 517, Loss: 1.8758\n",
      "Epoch: 1, Index: 518, Loss: 5.4529\n",
      "Epoch: 1, Index: 519, Loss: 0.1738\n",
      "Epoch: 1, Index: 520, Loss: 0.4269\n",
      "Epoch: 1, Index: 521, Loss: 3.4593\n",
      "Epoch: 1, Index: 522, Loss: 2.0139\n",
      "Epoch: 1, Index: 523, Loss: 1.9374\n",
      "Epoch: 1, Index: 524, Loss: 1.5905\n",
      "Epoch: 1, Index: 525, Loss: 6.8396\n",
      "Epoch: 1, Index: 526, Loss: 0.7143\n",
      "Epoch: 1, Index: 527, Loss: 1.4182\n",
      "Epoch: 1, Index: 528, Loss: 0.0879\n",
      "Epoch: 1, Index: 529, Loss: 0.8423\n",
      "Epoch: 1, Index: 530, Loss: 0.9677\n",
      "Epoch: 1, Index: 531, Loss: 0.3931\n",
      "Epoch: 1, Index: 532, Loss: 0.3247\n",
      "Epoch: 1, Index: 533, Loss: 0.8951\n",
      "Epoch: 1, Index: 534, Loss: 1.7049\n",
      "Epoch: 1, Index: 535, Loss: 1.7310\n",
      "Epoch: 1, Index: 536, Loss: 2.0977\n",
      "Epoch: 1, Index: 537, Loss: 0.3759\n",
      "Epoch: 1, Index: 538, Loss: 0.1345\n",
      "Epoch: 1, Index: 539, Loss: 3.4290\n",
      "Epoch: 1, Index: 540, Loss: 4.6921\n",
      "Epoch: 1, Index: 541, Loss: 2.5050\n",
      "Epoch: 1, Index: 542, Loss: 2.4373\n",
      "Epoch: 1, Index: 543, Loss: 1.1698\n",
      "Epoch: 1, Index: 544, Loss: 0.2049\n",
      "Epoch: 1, Index: 545, Loss: 4.9828\n",
      "Epoch: 1, Index: 546, Loss: 3.1237\n",
      "Epoch: 1, Index: 547, Loss: 3.5648\n",
      "Epoch: 1, Index: 548, Loss: 0.9557\n",
      "Epoch: 1, Index: 549, Loss: 0.8711\n",
      "Epoch: 1, Index: 550, Loss: 1.7688\n",
      "Epoch: 1, Index: 551, Loss: 1.4363\n",
      "Epoch: 1, Index: 552, Loss: 0.0232\n",
      "Epoch: 1, Index: 553, Loss: 0.6809\n",
      "Epoch: 1, Index: 554, Loss: 7.9914\n",
      "Epoch: 1, Index: 555, Loss: 8.2515\n",
      "Epoch: 1, Index: 556, Loss: 1.6448\n",
      "Epoch: 1, Index: 557, Loss: 3.2777\n",
      "Epoch: 1, Index: 558, Loss: 0.6897\n",
      "Epoch: 1, Index: 559, Loss: 2.9403\n",
      "Epoch: 1, Index: 560, Loss: 3.2797\n",
      "Epoch: 1, Index: 561, Loss: 0.7123\n",
      "Epoch: 1, Index: 562, Loss: 0.8640\n",
      "Epoch: 1, Index: 563, Loss: 1.9956\n",
      "Epoch: 1, Index: 564, Loss: 1.9727\n",
      "Epoch: 1, Index: 565, Loss: 2.8885\n",
      "Epoch: 1, Index: 566, Loss: 0.7229\n",
      "Epoch: 1, Index: 567, Loss: 1.4705\n",
      "Epoch: 1, Index: 568, Loss: 7.7550\n",
      "Epoch: 1, Index: 569, Loss: 1.0219\n",
      "Epoch: 1, Index: 570, Loss: 0.5793\n",
      "Epoch: 1, Index: 571, Loss: 4.3521\n",
      "Epoch: 1, Index: 572, Loss: 0.3041\n",
      "Epoch: 1, Index: 573, Loss: 1.4218\n",
      "Epoch: 1, Index: 574, Loss: 4.6422\n",
      "Epoch: 1, Index: 575, Loss: 1.5440\n",
      "Epoch: 1, Index: 576, Loss: 0.2841\n",
      "Epoch: 1, Index: 577, Loss: 1.5953\n",
      "Epoch: 1, Index: 578, Loss: 0.4219\n",
      "Epoch: 1, Index: 579, Loss: 1.6011\n",
      "Epoch: 1, Index: 580, Loss: 1.0929\n",
      "Epoch: 1, Index: 581, Loss: 3.1012\n",
      "Epoch: 1, Index: 582, Loss: 0.7674\n",
      "Epoch: 1, Index: 583, Loss: 1.0085\n",
      "Epoch: 1, Index: 584, Loss: 0.1318\n",
      "Epoch: 1, Index: 585, Loss: 1.0369\n",
      "Epoch: 1, Index: 586, Loss: 0.5456\n",
      "Epoch: 1, Index: 587, Loss: 0.1859\n",
      "Epoch: 1, Index: 588, Loss: 0.9934\n",
      "Epoch: 1, Index: 589, Loss: 1.4772\n",
      "Epoch: 1, Index: 590, Loss: 4.2042\n",
      "Epoch: 1, Index: 591, Loss: 3.1730\n",
      "Epoch: 1, Index: 592, Loss: 1.2688\n",
      "Epoch: 1, Index: 593, Loss: 1.0706\n",
      "Epoch: 1, Index: 594, Loss: 4.1719\n",
      "Epoch: 1, Index: 595, Loss: 0.0224\n",
      "Epoch: 1, Index: 596, Loss: 1.0315\n",
      "Epoch: 1, Index: 597, Loss: 10.1053\n",
      "Epoch: 1, Index: 598, Loss: 4.7193\n",
      "Epoch: 1, Index: 599, Loss: 2.6859\n",
      "Epoch: 1, Index: 600, Loss: 4.0853\n",
      "Epoch: 1, Index: 601, Loss: 1.9963\n",
      "Epoch: 1, Index: 602, Loss: 0.3168\n",
      "Epoch: 1, Index: 603, Loss: 0.2127\n",
      "Epoch: 1, Index: 604, Loss: 0.3745\n",
      "Epoch: 1, Index: 605, Loss: 2.1117\n",
      "Epoch: 1, Index: 606, Loss: 0.6891\n",
      "Epoch: 1, Index: 607, Loss: 1.6719\n",
      "Epoch: 1, Index: 608, Loss: 0.8837\n",
      "Epoch: 1, Index: 609, Loss: 0.0653\n",
      "Epoch: 1, Index: 610, Loss: 1.3313\n",
      "Epoch: 1, Index: 611, Loss: 0.4952\n",
      "Epoch: 1, Index: 612, Loss: 3.4559\n",
      "Epoch: 1, Index: 613, Loss: 2.5485\n",
      "Epoch: 1, Index: 614, Loss: 0.3840\n",
      "Epoch: 1, Index: 615, Loss: 4.4023\n",
      "Epoch: 1, Index: 616, Loss: 0.1887\n",
      "Epoch: 1, Index: 617, Loss: 3.7081\n",
      "Epoch: 1, Index: 618, Loss: 2.4378\n",
      "Epoch: 1, Index: 619, Loss: 0.1099\n",
      "Epoch: 1, Index: 620, Loss: 3.2699\n",
      "Epoch: 1, Index: 621, Loss: 0.1162\n",
      "Epoch: 1, Index: 622, Loss: 4.6218\n",
      "Epoch: 1, Index: 623, Loss: 0.0285\n",
      "Epoch: 1, Index: 624, Loss: 0.7542\n",
      "Epoch: 1, Index: 625, Loss: 3.5108\n",
      "Epoch: 1, Index: 626, Loss: 0.1808\n",
      "Epoch: 1, Index: 627, Loss: 0.3744\n",
      "Epoch: 1, Index: 628, Loss: 1.3709\n",
      "Epoch: 1, Index: 629, Loss: 1.6647\n",
      "Epoch: 1, Index: 630, Loss: 2.8486\n",
      "Epoch: 1, Index: 631, Loss: 1.6995\n",
      "Epoch: 1, Index: 632, Loss: 1.2048\n",
      "Epoch: 1, Index: 633, Loss: 4.1252\n",
      "Epoch: 1, Index: 634, Loss: 0.0086\n",
      "Epoch: 1, Index: 635, Loss: 0.8773\n",
      "Epoch: 1, Index: 636, Loss: 2.3545\n",
      "Epoch: 1, Index: 637, Loss: 0.3180\n",
      "Epoch: 1, Index: 638, Loss: 1.0480\n",
      "Epoch: 1, Index: 639, Loss: 3.2167\n",
      "Epoch: 1, Index: 640, Loss: 2.2618\n",
      "Epoch: 1, Index: 641, Loss: 3.5838\n",
      "Epoch: 1, Index: 642, Loss: 2.0531\n",
      "Epoch: 1, Index: 643, Loss: 0.9716\n",
      "Epoch: 1, Index: 644, Loss: 0.2696\n",
      "Epoch: 1, Index: 645, Loss: 6.3167\n",
      "Epoch: 1, Index: 646, Loss: 0.8261\n",
      "Epoch: 1, Index: 647, Loss: 0.6839\n",
      "Epoch: 1, Index: 648, Loss: 0.4218\n",
      "Epoch: 1, Index: 649, Loss: 0.1239\n",
      "Epoch: 1, Index: 650, Loss: 1.1295\n",
      "Epoch: 1, Index: 651, Loss: 1.8731\n",
      "Epoch: 1, Index: 652, Loss: 0.6513\n",
      "Epoch: 1, Index: 653, Loss: 1.1340\n",
      "Epoch: 1, Index: 654, Loss: 0.3371\n",
      "Epoch: 1, Index: 655, Loss: 2.0210\n",
      "Epoch: 1, Index: 656, Loss: 0.6071\n",
      "Epoch: 1, Index: 657, Loss: 0.0227\n",
      "Epoch: 1, Index: 658, Loss: 7.1987\n",
      "Epoch: 1, Index: 659, Loss: 3.0388\n",
      "Epoch: 1, Index: 660, Loss: 0.2385\n",
      "Epoch: 1, Index: 661, Loss: 3.2574\n",
      "Epoch: 1, Index: 662, Loss: 2.7513\n",
      "Epoch: 1, Index: 663, Loss: 4.9987\n",
      "Epoch: 1, Index: 664, Loss: 1.3287\n",
      "Epoch: 1, Index: 665, Loss: 3.6184\n",
      "Epoch: 1, Index: 666, Loss: 2.2037\n",
      "Epoch: 1, Index: 667, Loss: 1.0427\n",
      "Epoch: 1, Index: 668, Loss: 0.1582\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_avg_test_loss = 0\n",
    "total_rmse = 0\n",
    "total_mse = 0\n",
    "total_mae = 0\n",
    "total_r2 = 0\n",
    "total_tts = 0\n",
    "\n",
    "for i in range(k_folds):\n",
    "    print(f'FOLD {i + 1}')\n",
    "    print('--------------------------------')\n",
    "    test_indices = dataloaders[i]\n",
    "    train_indices = []\n",
    "    for j in range(k_folds):\n",
    "        if j != i:\n",
    "            train_indices += dataloaders[j]\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(dataset=torch.utils.data.Subset(dataset, train_indices), batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "    testloader = torch.utils.data.DataLoader(dataset=torch.utils.data.Subset(dataset, test_indices), batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "    \n",
    "    trainloader = fabric.setup_dataloaders(trainloader)\n",
    "    testloader = fabric.setup_dataloaders(testloader)\n",
    "    \n",
    "    # Reinitialize model\n",
    "    \n",
    "    model = GmiSwinTransformer(hidden_size=hidden_size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    model, optimizer = fabric.setup(model, optimizer)\n",
    "    \n",
    "    # Training the model\n",
    "    n_total_steps = len(trainloader)\n",
    "    avg_train_loss_over_epochs = []\n",
    "    avg_val_loss_over_epochs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for i, (images, labels) in tqdm(enumerate(trainloader), desc=\"Training Progress\", total=len(trainloader)):\n",
    "            # Move images and labels to device\n",
    "            images = images.float()\n",
    "            # images = images.permute(1, 0, 2, 3, 4)  # Change shape to [5, 10, 1, 224, 224]\n",
    "            labels = labels.float()\n",
    "\n",
    "            # Forward pass with autograd\n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # tqdm.write(f\"Epoch: {epoch+1}, Index: {i}, Loss: {loss.item():.4f}\")\n",
    "            tqdm.desc = f\"Training Progress: Epoch: {epoch+1}\"\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            fabric.backward(loss)\n",
    "            optimizer.step()\n",
    "            # Store the loss\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Store the average training loss for this epoch\n",
    "        avg_train_loss_over_epochs.append(sum(train_losses) / len(train_losses))\n",
    "        print(\"Average Training Loss: \", sum(train_losses) / len(train_losses))\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(testloader, desc=\"Validation Progress:\"):\n",
    "                images = images.float()\n",
    "                # images = images.permute(1, 0, 2, 3, 4)\n",
    "                labels = labels.float()\n",
    "                outputs = model(images).squeeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # print(\"Validation Loss: \", loss.item())\n",
    "                tqdm.desc = f\"Validation Progress: Epoch: {epoch+1}\"\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        # Store the average validation loss for this epoch\n",
    "        avg_val_loss_over_epochs.append(sum(val_losses) / len(val_losses))\n",
    "        print(\"Average Validation Loss: \", sum(val_losses) / len(val_losses))\n",
    "\n",
    "    # Plot loss over epochs\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), avg_train_loss_over_epochs, label='Average Training Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), avg_val_loss_over_epochs, label='Average Validation Loss', marker='o')\n",
    "    plt.xticks(range(1, num_epochs + 1))  # Ensure x-axis includes all epoch numbers\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.show()\n",
    "    \n",
    "    test_losses = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(testloader, desc=\"Testing Progress\"):\n",
    "            images = images.float()\n",
    "            # images = images.permute(1, 0, 2, 3, 4)\n",
    "            labels = labels.float()\n",
    "            print(\"Label: \", labels)\n",
    "            outputs = model(images).squeeze(1)\n",
    "            print(\"Output: \", outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_losses.append(loss.item())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "    rmse = math.sqrt(avg_test_loss)\n",
    "    mse = mean_squared_error(all_labels, all_outputs)\n",
    "    mae = mean_absolute_error(all_labels, all_outputs)\n",
    "    r2 = r2_score(all_labels, all_outputs)\n",
    "    tts = true_skill_score(all_outputs, all_labels)\n",
    "    \n",
    "    # Accumulate metrics for averaging across folds\n",
    "    total_avg_test_loss += avg_test_loss\n",
    "    total_rmse += rmse\n",
    "    total_mse += mse\n",
    "    total_mae += mae\n",
    "    total_r2 += r2\n",
    "    total_tts += tts\n",
    "    \n",
    "\n",
    "    print(f'Average test loss: {avg_test_loss:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'R: {r2:.4f}')\n",
    "    print(f'True Skill Score: {tts:.4f}')\n",
    "    \n",
    "    plt.figure(figsize=(50, 6))\n",
    "\n",
    "    plt.plot(all_labels, '-', label='Actual Labels')\n",
    "    plt.plot(all_outputs, '--', label='Predicted Labels', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Label Value')\n",
    "    plt.title('Overlay of Predicted and Actual Labels')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(all_outputs)\n",
    "    \n",
    "# Average metrics across all folds\n",
    "total_avg_test_loss /= k_folds\n",
    "total_rmse /= k_folds\n",
    "total_mse /= k_folds\n",
    "total_mae /= k_folds\n",
    "total_r2 /= k_folds\n",
    "total_tts /= k_folds\n",
    "\n",
    "print(f'Average test loss across all folds: {total_avg_test_loss:.4f}')\n",
    "print(f'Average RMSE across all folds: {total_rmse:.4f}')\n",
    "print(f'Average MSE across all folds: {total_mse:.4f}')\n",
    "print(f'Average MAE across all folds: {total_mae:.4f}')\n",
    "print(f'Average R across all folds: {total_r2:.4f}')\n",
    "print(f'Average True Skill Score across all folds: {total_tts:.4f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
