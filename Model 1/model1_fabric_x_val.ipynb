{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('../DataLoader')\n",
    "\n",
    "from dataloader_fits import SunImageDataset\n",
    "\n",
    "from lightning.fabric import Fabric\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "fabric = Fabric(accelerator='cuda', devices=1, precision=\"bf16-mixed\")\n",
    "fabric.launch()\n",
    "print(fabric.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 224*224\n",
    "hidden_size = 166\n",
    "num_epochs = 5\n",
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "dropout = 0.5\n",
    "# dropout = 0.6990787087509548\n",
    "\n",
    "k_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "407\n"
     ]
    }
   ],
   "source": [
    "dataset = SunImageDataset(csv_file=\"D:\\\\New folder (2)\\\\dataset.csv\", offset=0)\n",
    "\n",
    "total_size = len(dataset)\n",
    "fold_size = total_size // k_folds\n",
    "indices = list(range(total_size))\n",
    "\n",
    "k_fold_1_indices = indices[:fold_size]\n",
    "k_fold_2_indices = indices[fold_size:2*fold_size]\n",
    "k_fold_3_indices = indices[2*fold_size:3*fold_size]\n",
    "k_fold_4_indices = indices[3*fold_size:4*fold_size]\n",
    "k_fold_5_indices = indices[4*fold_size:]\n",
    "\n",
    "print(len(k_fold_1_indices))\n",
    "print(len(k_fold_2_indices))\n",
    "print(len(k_fold_3_indices))\n",
    "print(len(k_fold_4_indices))\n",
    "print(len(k_fold_5_indices))\n",
    "\n",
    "dataloaders = [k_fold_1_indices, k_fold_2_indices, k_fold_3_indices, k_fold_4_indices, k_fold_5_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "148\n",
      "130\n",
      "73\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "# Get active indices\n",
    "dataset_csv_file = pd.read_csv(\"D:\\\\New folder (2)\\\\dataset.csv\")\n",
    "active_indices = []\n",
    "\n",
    "for i in dataset_csv_file.index:\n",
    "    if dataset_csv_file['Kp'][i] >= 3.667:\n",
    "        active_indices.append(i)\n",
    "        \n",
    "active_dataloaders = []\n",
    "for dataloader in dataloaders:\n",
    "    active_dataloaders.append(list(set(dataloader) & set(active_indices)))\n",
    "    \n",
    "for dataloader in active_dataloaders:\n",
    "    print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GmiSwinTransformer(\n",
       "  (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pretrained_model): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): Sequential(\n",
       "      (0): SwinTransformerStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.004)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.004)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.009)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.013)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.013)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.017)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.017)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.022)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.022)\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.026)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.026)\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.030)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.030)\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.035)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.035)\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.039)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.039)\n",
       "          )\n",
       "          (6): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.043)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.043)\n",
       "          )\n",
       "          (7): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.048)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.048)\n",
       "          )\n",
       "          (8): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.052)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.052)\n",
       "          )\n",
       "          (9): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.057)\n",
       "          )\n",
       "          (10): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.061)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.061)\n",
       "          )\n",
       "          (11): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.065)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.065)\n",
       "          )\n",
       "          (12): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.070)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.070)\n",
       "          )\n",
       "          (13): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.074)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.074)\n",
       "          )\n",
       "          (14): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.078)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.078)\n",
       "          )\n",
       "          (15): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.083)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.083)\n",
       "          )\n",
       "          (16): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.087)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.087)\n",
       "          )\n",
       "          (17): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.091)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.096)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.096)\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path1): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path2): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): ClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Linear(in_features=1024, out_features=166, bias=True)\n",
       "      (flatten): Identity()\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): LeakyReLU(negative_slope=0.01)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=1660, out_features=166, bias=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Linear(in_features=166, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class GmiSwinTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(GmiSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Batch normalization for 3 channels\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # Initialize Swin Transformer\n",
    "        self.pretrained_model = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',\n",
    "            pretrained=True,\n",
    "            num_classes=hidden_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_size*10, hidden_size),\n",
    "            nn.Dropout(p=dropout),  # Added dropout probability\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            # nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Batch should be in format:\n",
    "        {\n",
    "            'images': torch.FloatTensor((10, 1, 224, 224))\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        images = images.reshape(-1, 1, 224, 224)\n",
    "        images = torch.cat([images, images, images], dim=1)\n",
    "        normalized_images = self.bn(images)\n",
    "        features = self.pretrained_model(normalized_images)\n",
    "        image_features = features.view(batch_size, -1)\n",
    "        \n",
    "        output = self.fc(image_features)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GmiSwinTransformer(hidden_size=hidden_size).to(device)\n",
    "model = GmiSwinTransformer(hidden_size=hidden_size)\n",
    "\n",
    "# print(torchsummary.summary(model, (10, 1, 224, 224)))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# model, optimizer = fabric.setup(model, optimizer)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rates(all_outputs, all_labels):\n",
    "    storm_labels = []\n",
    "    for output in all_labels:\n",
    "        if output < 3.667:\n",
    "            storm_labels.append(0)\n",
    "        else:\n",
    "            storm_labels.append(1)\n",
    "    storm_outputs = []\n",
    "    for output in all_outputs:\n",
    "        if output < 3.667:\n",
    "            storm_outputs.append(0)\n",
    "        else:\n",
    "            storm_outputs.append(1)\n",
    "\n",
    "    # Calculate true positive, true negative, false positive, false negative\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for i in range(len(storm_labels)):\n",
    "        if storm_labels[i] == 1 and storm_outputs[i] == 1:\n",
    "            tp += 1\n",
    "        elif storm_labels[i] == 0 and storm_outputs[i] == 0:\n",
    "            tn += 1\n",
    "        elif storm_labels[i] == 0 and storm_outputs[i] == 1:\n",
    "            fp += 1\n",
    "        elif storm_labels[i] == 1 and storm_outputs[i] == 0:\n",
    "            fn += 1\n",
    "            \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def true_skill_score(tp, tn, fp, fn):  \n",
    "            \n",
    "    # Calculate true skill score\n",
    "    tss = (tp / (tp + fp)) - (fp / (fp + tn)) if ((tp > 0 and tn > 0) or fp > 0) else 0\n",
    "    return tss\n",
    "    \n",
    "def calculate_metrics(tp, tn, fp, fn):\n",
    "    # Calculate accuracy\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bcb94a28de4a73afa71de24643b594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress: Epoch: 1:   0%|          | 0/808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss:  2.306759813495296\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36abc3f52b7f4f89af77f83433a0e012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress: Epoch: 1:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "total_avg_test_loss = 0\n",
    "total_rmse = 0\n",
    "total_mse = 0\n",
    "total_mae = 0\n",
    "total_r2 = 0\n",
    "total_tts = 0\n",
    "total_accuracy = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_f1 = 0\n",
    "\n",
    "active_total_avg_test_loss = 0\n",
    "active_total_rmse = 0\n",
    "active_total_mse = 0\n",
    "active_total_mae = 0\n",
    "active_total_r2 = 0\n",
    "active_total_tts = 0\n",
    "active_total_accuracy = 0\n",
    "active_total_precision = 0\n",
    "active_total_recall = 0\n",
    "active_total_f1 = 0\n",
    "\n",
    "\n",
    "for i in range(k_folds):\n",
    "    print(f'FOLD {i + 1}')\n",
    "    print('--------------------------------')\n",
    "    test_indices = dataloaders[i]\n",
    "    active_test_indices = active_dataloaders[i]\n",
    "    train_indices = []\n",
    "    for j in range(k_folds):\n",
    "        if j != i:\n",
    "            train_indices += dataloaders[j]\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(dataset=torch.utils.data.Subset(dataset, train_indices), batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "    testloader = torch.utils.data.DataLoader(dataset=torch.utils.data.Subset(dataset, test_indices), batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "    active_testloader = torch.utils.data.DataLoader(dataset=torch.utils.data.Subset(dataset, active_test_indices), batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "    trainloader = fabric.setup_dataloaders(trainloader)\n",
    "    testloader = fabric.setup_dataloaders(testloader)\n",
    "    active_testloader = fabric.setup_dataloaders(active_testloader)\n",
    "    \n",
    "    # Reinitialize model\n",
    "    \n",
    "    model = GmiSwinTransformer(hidden_size=hidden_size)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    model, optimizer = fabric.setup(model, optimizer)\n",
    "    model.train()\n",
    "    \n",
    "    # Training the model\n",
    "    n_total_steps = len(trainloader)\n",
    "    avg_train_loss_over_epochs = []\n",
    "    avg_val_loss_over_epochs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for i, (images, labels) in tqdm(enumerate(trainloader), desc=f\"Training Progress: Epoch: {epoch + 1}\", total=len(trainloader)):\n",
    "            # Move images and labels to device\n",
    "            images = images.float()\n",
    "            # images = images.permute(1, 0, 2, 3, 4)  # Change shape to [5, 10, 1, 224, 224]\n",
    "            labels = labels.float()\n",
    "\n",
    "            # Forward pass with autograd\n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # tqdm.write(f\"Epoch: {epoch+1}, Index: {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            fabric.backward(loss)\n",
    "            optimizer.step()\n",
    "            # Store the loss\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Store the average training loss for this epoch\n",
    "        avg_train_loss_over_epochs.append(sum(train_losses) / len(train_losses))\n",
    "        print(\"Average Training Loss: \", sum(train_losses) / len(train_losses))\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(testloader, desc=f\"Validation Progress: Epoch: {epoch + 1}\", total=len(testloader)):\n",
    "                images = images.float()\n",
    "                # images = images.permute(1, 0, 2, 3, 4)\n",
    "                labels = labels.float()\n",
    "                outputs = model(images).squeeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # print(\"Validation Loss: \", loss.item())\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        # Store the average validation loss for this epoch\n",
    "        avg_val_loss_over_epochs.append(sum(val_losses) / len(val_losses))\n",
    "        print(\"Average Validation Loss: \", sum(val_losses) / len(val_losses))\n",
    "\n",
    "    # Plot loss over epochs\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), avg_train_loss_over_epochs, label='Average Training Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), avg_val_loss_over_epochs, label='Average Validation Loss', marker='o')\n",
    "    plt.xticks(range(1, num_epochs + 1))  # Ensure x-axis includes all epoch numbers\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.show()\n",
    "    \n",
    "    test_losses = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    print('--------------------------------')\n",
    "    print('Test for Full set')\n",
    "    print('--------------------------------')\n",
    "\n",
    "# Test for Full set\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(testloader, desc=\"Testing Progress\"):\n",
    "            images = images.float()\n",
    "            # images = images.permute(1, 0, 2, 3, 4)\n",
    "            labels = labels.float()\n",
    "            # print(\"Label: \", labels)\n",
    "            outputs = model(images).squeeze(1)\n",
    "            # print(\"Output: \", outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_losses.append(loss.item())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "    rmse = math.sqrt(avg_test_loss)\n",
    "    mse = mean_squared_error(all_labels, all_outputs)\n",
    "    mae = mean_absolute_error(all_labels, all_outputs)\n",
    "    r2 = r2_score(all_labels, all_outputs)\n",
    "    tp, tn, fp, fn = calculate_rates(all_outputs, all_labels)\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(tp, tn, fp, fn)\n",
    "    tts = true_skill_score(tp, tn, fp, fn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Accumulate metrics for averaging across folds\n",
    "    total_avg_test_loss += avg_test_loss\n",
    "    total_rmse += rmse\n",
    "    total_mse += mse\n",
    "    total_mae += mae\n",
    "    total_r2 += r2\n",
    "    total_tts += tts\n",
    "    total_accuracy += accuracy\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    total_f1 += f1\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'Average test loss: {avg_test_loss:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'R: {r2:.4f}')\n",
    "    print(f'True Skill Score: {tts:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(50, 6))\n",
    "\n",
    "    plt.plot(all_labels, '-', label='Actual Labels')\n",
    "    plt.plot(all_outputs, '--', label='Predicted Labels', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Label Value')\n",
    "    plt.title('Overlay of Predicted and Actual Labels')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(all_outputs)\n",
    "        \n",
    "    print('--------------------------------')\n",
    "    print('Test for Active Periods')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    test_losses = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "# Test for active periods\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(active_testloader, desc=\"Testing Progress\"):\n",
    "            images = images.float()\n",
    "            labels = labels.float()\n",
    "            print(\"Label: \", labels)\n",
    "            outputs = model(images).squeeze(1)\n",
    "            print(\"Output: \", outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_losses.append(loss.item())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "    rmse = math.sqrt(avg_test_loss)\n",
    "    mse = mean_squared_error(all_labels, all_outputs)\n",
    "    mae = mean_absolute_error(all_labels, all_outputs)\n",
    "    r2 = r2_score(all_labels, all_outputs)\n",
    "    tp, tn, fp, fn = calculate_rates(all_outputs, all_labels)\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(tp, tn, fp, fn)\n",
    "    tts = true_skill_score(tp, tn, fp, fn)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Accumulate metrics for averaging across folds\n",
    "    active_total_avg_test_loss += avg_test_loss\n",
    "    active_total_rmse += rmse\n",
    "    active_total_mse += mse\n",
    "    active_total_mae += mae\n",
    "    active_total_r2 += r2\n",
    "    active_total_tts += tts\n",
    "    active_total_accuracy += accuracy\n",
    "    active_total_precision += precision\n",
    "    active_total_recall += recall\n",
    "    active_total_f1 += f1\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'Average test loss: {avg_test_loss:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'R: {r2:.4f}')\n",
    "    print(f'True Skill Score: {tts:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(50, 6))\n",
    "\n",
    "    plt.plot(all_labels, '-', label='Actual Labels')\n",
    "    plt.plot(all_outputs, '--', label='Predicted Labels', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Label Value')\n",
    "    plt.title('Overlay of Predicted and Actual Labels')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(all_outputs)\n",
    "    \n",
    "# Average metrics across all folds\n",
    "active_total_avg_test_loss /= k_folds\n",
    "active_total_rmse /= k_folds\n",
    "active_total_mse /= k_folds\n",
    "active_total_mae /= k_folds\n",
    "active_total_r2 /= k_folds\n",
    "active_total_tts /= k_folds\n",
    "active_total_accuracy /= k_folds\n",
    "active_total_precision /= k_folds\n",
    "active_total_recall /= k_folds\n",
    "active_total_f1 /= k_folds\n",
    "\n",
    "print(f'Average active test loss across all folds: {active_total_avg_test_loss:.4f}')\n",
    "print(f'Average active RMSE across all folds: {active_total_rmse:.4f}')\n",
    "print(f'Average active MSE across all folds: {active_total_mse:.4f}')\n",
    "print(f'Average active MAE across all folds: {active_total_mae:.4f}')\n",
    "print(f'Average active R across all folds: {active_total_r2:.4f}')\n",
    "print(f'Average active True Skill Score across all folds: {active_total_tts:.4f}')\n",
    "print(f'Average active Accuracy across all folds: {active_total_accuracy:.4f}')\n",
    "print(f'Average active Precision across all folds: {active_total_precision:.4f}')\n",
    "print(f'Average active Recall across all folds: {active_total_recall:.4f}')\n",
    "print(f'Average active F1 Score across all folds: {active_total_f1:.4f}')\n",
    "    \n",
    "# Average metrics across all folds\n",
    "total_avg_test_loss /= k_folds\n",
    "total_rmse /= k_folds\n",
    "total_mse /= k_folds\n",
    "total_mae /= k_folds\n",
    "total_r2 /= k_folds\n",
    "total_tts /= k_folds\n",
    "total_accuracy /= k_folds\n",
    "total_precision /= k_folds\n",
    "total_recall /= k_folds\n",
    "total_f1 /= k_folds\n",
    "\n",
    "print(f'Average test loss across all folds: {total_avg_test_loss:.4f}')\n",
    "print(f'Average RMSE across all folds: {total_rmse:.4f}')\n",
    "print(f'Average MSE across all folds: {total_mse:.4f}')\n",
    "print(f'Average MAE across all folds: {total_mae:.4f}')\n",
    "print(f'Average R across all folds: {total_r2:.4f}')\n",
    "print(f'Average True Skill Score across all folds: {total_tts:.4f}')\n",
    "print(f'Average Accuracy across all folds: {total_accuracy:.4f}')\n",
    "print(f'Average Precision across all folds: {total_precision:.4f}')\n",
    "print(f'Average Recall across all folds: {total_recall:.4f}')\n",
    "print(f'Average F1 Score across all folds: {total_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss:  1.8189665509395379\n",
      "Average RMSE:  1.3417481602872623\n",
      "Average MSE:  1.8169463872909546\n",
      "Average MAE:  1.0365631461143494\n",
      "Average R:  -0.08638903837468133\n",
      "Average True Skill Score:  0.21535680723724213\n",
      "Average Accuracy:  0.9173703367251754\n",
      "Average Precision:  0.22301587301587303\n",
      "Average Recall:  0.031161868390129254\n",
      "Average F1 Score:  0.054070157484791624\n"
     ]
    }
   ],
   "source": [
    "print(\"Average test loss: \", total_avg_test_loss/2)\n",
    "print(\"Average RMSE: \", total_rmse/2)\n",
    "print(\"Average MSE: \", total_mse/2)\n",
    "print(\"Average MAE: \", total_mae/2)\n",
    "print(\"Average R: \", total_r2/2)\n",
    "print(\"Average True Skill Score: \", total_tts/2)\n",
    "print(\"Average Accuracy: \", total_accuracy/2)\n",
    "print(\"Average Precision: \", total_precision/2)\n",
    "print(\"Average Recall: \", total_recall/2)\n",
    "print(\"Average F1 Score: \", total_f1/2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
