{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import torchsummary\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('../DataLoader')\n",
    "\n",
    "from dataloader import SunImageDataset\n",
    "\n",
    "from torch.func import stack_module_state\n",
    "from torch.func import vmap\n",
    "\n",
    "from lightning.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "fabric = Fabric(accelerator='cuda', devices=1, precision=\"bf16-mixed\")\n",
    "fabric.launch()\n",
    "print(fabric.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 224*224\n",
    "hidden_size = 166\n",
    "num_epochs = 20\n",
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "dropout = 0.5\n",
    "# dropout = 0.6990787087509548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SunImageDataset(csv_file=\"D:\\\\dataset.csv\", offset=0, transform=transforms.ToTensor())\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# # Without Validation Set\n",
    "# trainset, testset = torch.utils.data.Subset(dataset, range(train_size)), torch.utils.data.Subset(dataset, range(train_size, len(dataset)))\n",
    "# trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "\n",
    "# trainloader = fabric.setup_dataloaders(trainloader)\n",
    "\n",
    "# With Validation Set\n",
    "# Split dataset into training and test sets\n",
    "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, shuffle=False)\n",
    "\n",
    "# Further split training set into training and validation sets\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.25, shuffle=False)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "trainset = torch.utils.data.Subset(dataset, train_indices)\n",
    "valset = torch.utils.data.Subset(dataset, val_indices)\n",
    "testset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(dataset=valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# # Get a batch of training data\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "# images = torch.stack(images)\n",
    "# print(images.shape)\n",
    "# print(labels.shape)\n",
    "\n",
    "# print(images)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_FabricModule(\n",
       "  (_forward_module): GmiSwinTransformer(\n",
       "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pretrained_model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers): Sequential(\n",
       "        (0): SwinTransformerStage(\n",
       "          (downsample): Identity()\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.004)\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.013)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.017)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.022)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.026)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.030)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.035)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.039)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.043)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.048)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.052)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.057)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.061)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.065)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.070)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.074)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.078)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.083)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.087)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.096)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Linear(in_features=1024, out_features=166, bias=True)\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Linear(in_features=1660, out_features=166, bias=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=166, out_features=1, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (_original_module): GmiSwinTransformer(\n",
       "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pretrained_model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers): Sequential(\n",
       "        (0): SwinTransformerStage(\n",
       "          (downsample): Identity()\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.004)\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.013)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.017)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.022)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.026)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.030)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.035)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.039)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.043)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.048)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.052)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.057)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.061)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.065)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.070)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.074)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.078)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.083)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.087)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.096)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Linear(in_features=1024, out_features=166, bias=True)\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Linear(in_features=1660, out_features=166, bias=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=166, out_features=1, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class GmiSwinTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(GmiSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Batch normalization for 3 channels\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # Initialize Swin Transformer\n",
    "        self.pretrained_model = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',\n",
    "            pretrained=True,\n",
    "            num_classes=hidden_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size*10, hidden_size),\n",
    "            nn.Dropout(p=dropout),  # Added dropout probability\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Batch should be in format:\n",
    "        {\n",
    "            'images': torch.FloatTensor((10, 1, 224, 224))\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        images = images.reshape(-1, 1, 224, 224)\n",
    "        images = torch.cat([images, images, images], dim=1)\n",
    "        normalized_images = self.bn(images)\n",
    "        features = self.pretrained_model(normalized_images)\n",
    "        image_features = features.view(batch_size, -1)\n",
    "        \n",
    "        output = self.fc(image_features)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GmiSwinTransformer(hidden_size=hidden_size).to(device)\n",
    "model = GmiSwinTransformer(hidden_size=hidden_size)\n",
    "\n",
    "# print(torchsummary.summary(model, (10, 1, 224, 224)))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1435ee41734cc18da671a58af8513b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Index: 0, Loss: 14.7169\n",
      "Epoch: 1, Index: 1, Loss: 0.5560\n",
      "Epoch: 1, Index: 2, Loss: 8.1218\n",
      "Epoch: 1, Index: 3, Loss: 1.7745\n",
      "Epoch: 1, Index: 4, Loss: 1.3392\n",
      "Epoch: 1, Index: 5, Loss: 4.3825\n",
      "Epoch: 1, Index: 6, Loss: 1.7601\n",
      "Epoch: 1, Index: 7, Loss: 6.3491\n",
      "Epoch: 1, Index: 8, Loss: 0.5727\n",
      "Epoch: 1, Index: 9, Loss: 2.4135\n",
      "Epoch: 1, Index: 10, Loss: 0.1274\n",
      "Epoch: 1, Index: 11, Loss: 2.8076\n",
      "Epoch: 1, Index: 12, Loss: 6.1586\n",
      "Epoch: 1, Index: 13, Loss: 1.7501\n",
      "Epoch: 1, Index: 14, Loss: 1.5564\n",
      "Epoch: 1, Index: 15, Loss: 0.6503\n",
      "Epoch: 1, Index: 16, Loss: 1.3874\n",
      "Epoch: 1, Index: 17, Loss: 3.6405\n",
      "Epoch: 1, Index: 18, Loss: 0.1482\n",
      "Epoch: 1, Index: 19, Loss: 3.5978\n",
      "Epoch: 1, Index: 20, Loss: 1.8695\n",
      "Epoch: 1, Index: 21, Loss: 7.7711\n",
      "Epoch: 1, Index: 22, Loss: 3.5446\n",
      "Epoch: 1, Index: 23, Loss: 0.4456\n",
      "Epoch: 1, Index: 24, Loss: 1.4370\n",
      "Epoch: 1, Index: 25, Loss: 2.0499\n",
      "Epoch: 1, Index: 26, Loss: 1.0057\n",
      "Epoch: 1, Index: 27, Loss: 5.6461\n",
      "Epoch: 1, Index: 28, Loss: 5.6554\n",
      "Epoch: 1, Index: 29, Loss: 1.9086\n",
      "Epoch: 1, Index: 30, Loss: 3.7153\n",
      "Epoch: 1, Index: 31, Loss: 5.8235\n",
      "Epoch: 1, Index: 32, Loss: 0.4958\n",
      "Epoch: 1, Index: 33, Loss: 0.7174\n",
      "Epoch: 1, Index: 34, Loss: 3.1340\n",
      "Epoch: 1, Index: 35, Loss: 1.3068\n",
      "Epoch: 1, Index: 36, Loss: 1.3323\n",
      "Epoch: 1, Index: 37, Loss: 1.4478\n",
      "Epoch: 1, Index: 38, Loss: 1.6680\n",
      "Epoch: 1, Index: 39, Loss: 0.0112\n",
      "Epoch: 1, Index: 40, Loss: 0.5536\n",
      "Epoch: 1, Index: 41, Loss: 3.4093\n",
      "Epoch: 1, Index: 42, Loss: 3.5156\n",
      "Epoch: 1, Index: 43, Loss: 1.4977\n",
      "Epoch: 1, Index: 44, Loss: 0.2037\n",
      "Epoch: 1, Index: 45, Loss: 0.6772\n",
      "Epoch: 1, Index: 46, Loss: 0.6564\n",
      "Epoch: 1, Index: 47, Loss: 1.8080\n",
      "Epoch: 1, Index: 48, Loss: 0.1850\n",
      "Epoch: 1, Index: 49, Loss: 1.1941\n",
      "Epoch: 1, Index: 50, Loss: 3.9494\n",
      "Epoch: 1, Index: 51, Loss: 1.4082\n",
      "Epoch: 1, Index: 52, Loss: 7.4391\n",
      "Epoch: 1, Index: 53, Loss: 0.4757\n",
      "Epoch: 1, Index: 54, Loss: 7.9586\n",
      "Epoch: 1, Index: 55, Loss: 0.0869\n",
      "Epoch: 1, Index: 56, Loss: 2.0855\n",
      "Epoch: 1, Index: 57, Loss: 6.0942\n",
      "Epoch: 1, Index: 58, Loss: 0.4301\n",
      "Epoch: 1, Index: 59, Loss: 0.7029\n",
      "Epoch: 1, Index: 60, Loss: 0.8314\n",
      "Epoch: 1, Index: 61, Loss: 7.1056\n",
      "Epoch: 1, Index: 62, Loss: 6.3218\n",
      "Epoch: 1, Index: 63, Loss: 1.6399\n",
      "Epoch: 1, Index: 64, Loss: 0.5534\n",
      "Epoch: 1, Index: 65, Loss: 7.5103\n",
      "Epoch: 1, Index: 66, Loss: 0.3625\n",
      "Epoch: 1, Index: 67, Loss: 3.2452\n",
      "Epoch: 1, Index: 68, Loss: 3.9355\n",
      "Epoch: 1, Index: 69, Loss: 3.6746\n",
      "Epoch: 1, Index: 70, Loss: 5.5584\n",
      "Epoch: 1, Index: 71, Loss: 2.6860\n",
      "Epoch: 1, Index: 72, Loss: 1.3901\n",
      "Epoch: 1, Index: 73, Loss: 0.1919\n",
      "Epoch: 1, Index: 74, Loss: 6.9039\n",
      "Epoch: 1, Index: 75, Loss: 0.4780\n",
      "Epoch: 1, Index: 76, Loss: 0.7075\n",
      "Epoch: 1, Index: 77, Loss: 2.2442\n",
      "Epoch: 1, Index: 78, Loss: 1.7763\n",
      "Epoch: 1, Index: 79, Loss: 1.9240\n",
      "Epoch: 1, Index: 80, Loss: 2.3570\n",
      "Epoch: 1, Index: 81, Loss: 0.3290\n",
      "Epoch: 1, Index: 82, Loss: 0.3346\n",
      "Epoch: 1, Index: 83, Loss: 2.5230\n",
      "Epoch: 1, Index: 84, Loss: 0.1846\n",
      "Epoch: 1, Index: 85, Loss: 2.1958\n",
      "Epoch: 1, Index: 86, Loss: 2.8662\n",
      "Epoch: 1, Index: 87, Loss: 5.5160\n",
      "Epoch: 1, Index: 88, Loss: 1.0797\n",
      "Epoch: 1, Index: 89, Loss: 1.5225\n",
      "Epoch: 1, Index: 90, Loss: 1.7692\n",
      "Epoch: 1, Index: 91, Loss: 2.7461\n",
      "Epoch: 1, Index: 92, Loss: 0.9668\n",
      "Epoch: 1, Index: 93, Loss: 0.5803\n",
      "Epoch: 1, Index: 94, Loss: 0.5521\n",
      "Epoch: 1, Index: 95, Loss: 0.7923\n",
      "Epoch: 1, Index: 96, Loss: 2.3935\n",
      "Epoch: 1, Index: 97, Loss: 5.8023\n",
      "Epoch: 1, Index: 98, Loss: 0.1055\n",
      "Epoch: 1, Index: 99, Loss: 0.9352\n",
      "Epoch: 1, Index: 100, Loss: 13.4039\n",
      "Epoch: 1, Index: 101, Loss: 0.2236\n",
      "Epoch: 1, Index: 102, Loss: 3.6832\n",
      "Epoch: 1, Index: 103, Loss: 2.8776\n",
      "Epoch: 1, Index: 104, Loss: 2.5619\n",
      "Epoch: 1, Index: 105, Loss: 2.1509\n",
      "Epoch: 1, Index: 106, Loss: 2.2469\n",
      "Epoch: 1, Index: 107, Loss: 0.6516\n",
      "Epoch: 1, Index: 108, Loss: 1.5432\n",
      "Epoch: 1, Index: 109, Loss: 0.7130\n",
      "Epoch: 1, Index: 110, Loss: 4.7870\n",
      "Epoch: 1, Index: 111, Loss: 7.2345\n",
      "Epoch: 1, Index: 112, Loss: 17.0391\n",
      "Epoch: 1, Index: 113, Loss: 0.9348\n",
      "Epoch: 1, Index: 114, Loss: 1.4417\n",
      "Epoch: 1, Index: 115, Loss: 0.4492\n",
      "Epoch: 1, Index: 116, Loss: 2.8285\n",
      "Epoch: 1, Index: 117, Loss: 2.7669\n",
      "Epoch: 1, Index: 118, Loss: 1.1106\n",
      "Epoch: 1, Index: 119, Loss: 3.4364\n",
      "Epoch: 1, Index: 120, Loss: 1.3283\n",
      "Epoch: 1, Index: 121, Loss: 1.0408\n",
      "Epoch: 1, Index: 122, Loss: 0.1980\n",
      "Epoch: 1, Index: 123, Loss: 1.7117\n",
      "Epoch: 1, Index: 124, Loss: 2.9410\n",
      "Epoch: 1, Index: 125, Loss: 3.4291\n",
      "Epoch: 1, Index: 126, Loss: 9.5969\n",
      "Epoch: 1, Index: 127, Loss: 0.3714\n",
      "Epoch: 1, Index: 128, Loss: 2.8334\n",
      "Epoch: 1, Index: 129, Loss: 0.4303\n",
      "Epoch: 1, Index: 130, Loss: 7.8864\n",
      "Epoch: 1, Index: 131, Loss: 0.1242\n",
      "Epoch: 1, Index: 132, Loss: 3.9368\n",
      "Epoch: 1, Index: 133, Loss: 0.2880\n",
      "Epoch: 1, Index: 134, Loss: 1.4763\n",
      "Epoch: 1, Index: 135, Loss: 0.6301\n",
      "Epoch: 1, Index: 136, Loss: 1.7153\n",
      "Epoch: 1, Index: 137, Loss: 0.4904\n",
      "Epoch: 1, Index: 138, Loss: 2.4598\n",
      "Epoch: 1, Index: 139, Loss: 1.4977\n",
      "Epoch: 1, Index: 140, Loss: 0.5373\n",
      "Epoch: 1, Index: 141, Loss: 2.5376\n",
      "Epoch: 1, Index: 142, Loss: 5.7659\n",
      "Epoch: 1, Index: 143, Loss: 2.6532\n",
      "Epoch: 1, Index: 144, Loss: 0.1312\n",
      "Epoch: 1, Index: 145, Loss: 3.9482\n",
      "Epoch: 1, Index: 146, Loss: 0.9901\n",
      "Epoch: 1, Index: 147, Loss: 0.4232\n",
      "Epoch: 1, Index: 148, Loss: 0.8314\n",
      "Epoch: 1, Index: 149, Loss: 4.3003\n",
      "Epoch: 1, Index: 150, Loss: 6.1569\n",
      "Epoch: 1, Index: 151, Loss: 0.1857\n",
      "Epoch: 1, Index: 152, Loss: 0.4400\n",
      "Epoch: 1, Index: 153, Loss: 2.8954\n",
      "Epoch: 1, Index: 154, Loss: 0.1257\n",
      "Epoch: 1, Index: 155, Loss: 3.4310\n",
      "Epoch: 1, Index: 156, Loss: 1.1898\n",
      "Epoch: 1, Index: 157, Loss: 2.8340\n",
      "Epoch: 1, Index: 158, Loss: 1.8782\n",
      "Epoch: 1, Index: 159, Loss: 0.2952\n",
      "Epoch: 1, Index: 160, Loss: 2.2872\n",
      "Epoch: 1, Index: 161, Loss: 4.6581\n",
      "Epoch: 1, Index: 162, Loss: 6.6330\n",
      "Epoch: 1, Index: 163, Loss: 5.4170\n",
      "Epoch: 1, Index: 164, Loss: 1.0132\n",
      "Epoch: 1, Index: 165, Loss: 2.8001\n",
      "Epoch: 1, Index: 166, Loss: 0.2155\n",
      "Epoch: 1, Index: 167, Loss: 0.4198\n",
      "Epoch: 1, Index: 168, Loss: 0.2048\n",
      "Epoch: 1, Index: 169, Loss: 4.3453\n",
      "Epoch: 1, Index: 170, Loss: 1.4309\n",
      "Epoch: 1, Index: 171, Loss: 0.6047\n",
      "Epoch: 1, Index: 172, Loss: 1.4269\n",
      "Epoch: 1, Index: 173, Loss: 0.4280\n",
      "Epoch: 1, Index: 174, Loss: 1.0878\n",
      "Epoch: 1, Index: 175, Loss: 0.5054\n",
      "Epoch: 1, Index: 176, Loss: 0.9416\n",
      "Epoch: 1, Index: 177, Loss: 0.1684\n",
      "Epoch: 1, Index: 178, Loss: 7.4455\n",
      "Epoch: 1, Index: 179, Loss: 0.5152\n",
      "Epoch: 1, Index: 180, Loss: 7.9715\n",
      "Epoch: 1, Index: 181, Loss: 7.2428\n",
      "Epoch: 1, Index: 182, Loss: 3.3287\n",
      "Epoch: 1, Index: 183, Loss: 1.8381\n",
      "Epoch: 1, Index: 184, Loss: 1.5878\n",
      "Epoch: 1, Index: 185, Loss: 10.7534\n",
      "Epoch: 1, Index: 186, Loss: 6.4727\n",
      "Epoch: 1, Index: 187, Loss: 4.1159\n",
      "Epoch: 1, Index: 188, Loss: 3.8716\n",
      "Epoch: 1, Index: 189, Loss: 1.6532\n",
      "Epoch: 1, Index: 190, Loss: 1.0719\n",
      "Epoch: 1, Index: 191, Loss: 0.1403\n",
      "Epoch: 1, Index: 192, Loss: 3.1758\n",
      "Epoch: 1, Index: 193, Loss: 11.4468\n",
      "Epoch: 1, Index: 194, Loss: 6.4695\n",
      "Epoch: 1, Index: 195, Loss: 0.1064\n",
      "Epoch: 1, Index: 196, Loss: 3.9408\n",
      "Epoch: 1, Index: 197, Loss: 6.2197\n",
      "Epoch: 1, Index: 198, Loss: 2.7557\n",
      "Epoch: 1, Index: 199, Loss: 1.6005\n",
      "Epoch: 1, Index: 200, Loss: 2.2792\n",
      "Epoch: 1, Index: 201, Loss: 1.1094\n",
      "Epoch: 1, Index: 202, Loss: 3.7479\n",
      "Epoch: 1, Index: 203, Loss: 5.5344\n",
      "Epoch: 1, Index: 204, Loss: 6.6278\n",
      "Epoch: 1, Index: 205, Loss: 1.5479\n",
      "Epoch: 1, Index: 206, Loss: 6.5318\n",
      "Epoch: 1, Index: 207, Loss: 1.5953\n",
      "Epoch: 1, Index: 208, Loss: 0.2200\n",
      "Epoch: 1, Index: 209, Loss: 0.2815\n",
      "Epoch: 1, Index: 210, Loss: 0.7554\n",
      "Epoch: 1, Index: 211, Loss: 1.8762\n",
      "Epoch: 1, Index: 212, Loss: 1.5970\n",
      "Epoch: 1, Index: 213, Loss: 0.5904\n",
      "Epoch: 1, Index: 214, Loss: 0.5843\n",
      "Epoch: 1, Index: 215, Loss: 0.9772\n",
      "Epoch: 1, Index: 216, Loss: 6.1475\n",
      "Epoch: 1, Index: 217, Loss: 0.4995\n",
      "Epoch: 1, Index: 218, Loss: 2.8972\n",
      "Epoch: 1, Index: 219, Loss: 4.9519\n",
      "Epoch: 1, Index: 220, Loss: 7.6682\n",
      "Epoch: 1, Index: 221, Loss: 6.1561\n",
      "Epoch: 1, Index: 222, Loss: 1.5903\n",
      "Epoch: 1, Index: 223, Loss: 0.6355\n",
      "Epoch: 1, Index: 224, Loss: 1.6016\n",
      "Epoch: 1, Index: 225, Loss: 7.6273\n",
      "Epoch: 1, Index: 226, Loss: 0.9594\n",
      "Epoch: 1, Index: 227, Loss: 0.1406\n",
      "Epoch: 1, Index: 228, Loss: 1.3847\n",
      "Epoch: 1, Index: 229, Loss: 9.1356\n",
      "Epoch: 1, Index: 230, Loss: 13.3212\n",
      "Epoch: 1, Index: 231, Loss: 4.1615\n",
      "Epoch: 1, Index: 232, Loss: 3.7520\n",
      "Epoch: 1, Index: 233, Loss: 1.8605\n",
      "Epoch: 1, Index: 234, Loss: 1.2194\n",
      "Epoch: 1, Index: 235, Loss: 0.9049\n",
      "Epoch: 1, Index: 236, Loss: 0.2803\n",
      "Epoch: 1, Index: 237, Loss: 0.0791\n",
      "Epoch: 1, Index: 238, Loss: 0.1930\n",
      "Epoch: 1, Index: 239, Loss: 2.7905\n",
      "Epoch: 1, Index: 240, Loss: 0.1422\n",
      "Epoch: 1, Index: 241, Loss: 12.5139\n",
      "Epoch: 1, Index: 242, Loss: 2.9175\n",
      "Epoch: 1, Index: 243, Loss: 1.1414\n",
      "Epoch: 1, Index: 244, Loss: 0.1130\n",
      "Epoch: 1, Index: 245, Loss: 1.3549\n",
      "Epoch: 1, Index: 246, Loss: 2.1866\n",
      "Epoch: 1, Index: 247, Loss: 1.3141\n",
      "Epoch: 1, Index: 248, Loss: 0.4869\n",
      "Epoch: 1, Index: 249, Loss: 0.0105\n",
      "Epoch: 1, Index: 250, Loss: 1.4069\n",
      "Epoch: 1, Index: 251, Loss: 0.6555\n",
      "Epoch: 1, Index: 252, Loss: 2.6377\n",
      "Epoch: 1, Index: 253, Loss: 4.5148\n",
      "Epoch: 1, Index: 254, Loss: 7.4898\n",
      "Epoch: 1, Index: 255, Loss: 0.1755\n",
      "Epoch: 1, Index: 256, Loss: 1.7791\n",
      "Epoch: 1, Index: 257, Loss: 2.8464\n",
      "Epoch: 1, Index: 258, Loss: 0.5108\n",
      "Epoch: 1, Index: 259, Loss: 0.8279\n",
      "Epoch: 1, Index: 260, Loss: 0.1386\n",
      "Epoch: 1, Index: 261, Loss: 0.1647\n",
      "Epoch: 1, Index: 262, Loss: 0.6926\n",
      "Epoch: 1, Index: 263, Loss: 0.3395\n",
      "Epoch: 1, Index: 264, Loss: 2.5876\n",
      "Epoch: 1, Index: 265, Loss: 6.6684\n",
      "Epoch: 1, Index: 266, Loss: 2.0976\n",
      "Epoch: 1, Index: 267, Loss: 2.3759\n",
      "Epoch: 1, Index: 268, Loss: 6.2100\n",
      "Epoch: 1, Index: 269, Loss: 0.7531\n",
      "Epoch: 1, Index: 270, Loss: 2.4520\n",
      "Epoch: 1, Index: 271, Loss: 0.5533\n",
      "Epoch: 1, Index: 272, Loss: 1.1407\n",
      "Epoch: 1, Index: 273, Loss: 0.5239\n",
      "Epoch: 1, Index: 274, Loss: 0.3769\n",
      "Epoch: 1, Index: 275, Loss: 2.0011\n",
      "Epoch: 1, Index: 276, Loss: 1.1987\n",
      "Epoch: 1, Index: 277, Loss: 1.4518\n",
      "Epoch: 1, Index: 278, Loss: 2.2855\n",
      "Epoch: 1, Index: 279, Loss: 0.3377\n",
      "Epoch: 1, Index: 280, Loss: 1.1227\n",
      "Epoch: 1, Index: 281, Loss: 3.6838\n",
      "Epoch: 1, Index: 282, Loss: 1.5011\n",
      "Epoch: 1, Index: 283, Loss: 1.6559\n",
      "Epoch: 1, Index: 284, Loss: 0.4475\n",
      "Epoch: 1, Index: 285, Loss: 0.6926\n",
      "Epoch: 1, Index: 286, Loss: 0.1704\n",
      "Epoch: 1, Index: 287, Loss: 2.3614\n",
      "Epoch: 1, Index: 288, Loss: 2.1080\n",
      "Epoch: 1, Index: 289, Loss: 1.2360\n",
      "Epoch: 1, Index: 290, Loss: 4.6835\n",
      "Epoch: 1, Index: 291, Loss: 2.1046\n",
      "Epoch: 1, Index: 292, Loss: 2.7508\n",
      "Epoch: 1, Index: 293, Loss: 4.7157\n",
      "Epoch: 1, Index: 294, Loss: 0.0638\n",
      "Epoch: 1, Index: 295, Loss: 1.4017\n",
      "Epoch: 1, Index: 296, Loss: 0.9757\n",
      "Epoch: 1, Index: 297, Loss: 1.4264\n",
      "Epoch: 1, Index: 298, Loss: 0.7845\n",
      "Epoch: 1, Index: 299, Loss: 1.0315\n",
      "Epoch: 1, Index: 300, Loss: 4.3868\n",
      "Epoch: 1, Index: 301, Loss: 0.6152\n",
      "Epoch: 1, Index: 302, Loss: 0.5015\n",
      "Epoch: 1, Index: 303, Loss: 1.4919\n",
      "Epoch: 1, Index: 304, Loss: 6.1995\n",
      "Epoch: 1, Index: 305, Loss: 1.1665\n",
      "Epoch: 1, Index: 306, Loss: 0.3543\n",
      "Epoch: 1, Index: 307, Loss: 1.9957\n",
      "Epoch: 1, Index: 308, Loss: 0.6508\n",
      "Epoch: 1, Index: 309, Loss: 1.4777\n",
      "Epoch: 1, Index: 310, Loss: 0.3222\n",
      "Epoch: 1, Index: 311, Loss: 1.2642\n",
      "Epoch: 1, Index: 312, Loss: 3.5839\n",
      "Epoch: 1, Index: 313, Loss: 1.7000\n",
      "Epoch: 1, Index: 314, Loss: 0.3194\n",
      "Epoch: 1, Index: 315, Loss: 0.8526\n",
      "Epoch: 1, Index: 316, Loss: 4.0477\n",
      "Epoch: 1, Index: 317, Loss: 0.2360\n",
      "Epoch: 1, Index: 318, Loss: 0.0426\n",
      "Epoch: 1, Index: 319, Loss: 1.3430\n",
      "Epoch: 1, Index: 320, Loss: 0.5263\n",
      "Epoch: 1, Index: 321, Loss: 0.8206\n",
      "Epoch: 1, Index: 322, Loss: 0.4964\n",
      "Epoch: 1, Index: 323, Loss: 2.0898\n",
      "Epoch: 1, Index: 324, Loss: 0.8357\n",
      "Epoch: 1, Index: 325, Loss: 1.0961\n",
      "Epoch: 1, Index: 326, Loss: 2.5507\n",
      "Epoch: 1, Index: 327, Loss: 0.3400\n",
      "Epoch: 1, Index: 328, Loss: 1.3843\n",
      "Epoch: 1, Index: 329, Loss: 0.6337\n",
      "Epoch: 1, Index: 330, Loss: 4.0508\n",
      "Epoch: 1, Index: 331, Loss: 4.3056\n",
      "Epoch: 1, Index: 332, Loss: 0.0673\n",
      "Epoch: 1, Index: 333, Loss: 13.9480\n",
      "Epoch: 1, Index: 334, Loss: 2.7051\n",
      "Epoch: 1, Index: 335, Loss: 0.0095\n",
      "Epoch: 1, Index: 336, Loss: 6.7159\n",
      "Epoch: 1, Index: 337, Loss: 0.1828\n",
      "Epoch: 1, Index: 338, Loss: 0.9982\n",
      "Epoch: 1, Index: 339, Loss: 2.3712\n",
      "Epoch: 1, Index: 340, Loss: 0.9272\n",
      "Epoch: 1, Index: 341, Loss: 0.8155\n",
      "Epoch: 1, Index: 342, Loss: 0.1806\n",
      "Epoch: 1, Index: 343, Loss: 0.2785\n",
      "Epoch: 1, Index: 344, Loss: 0.3194\n",
      "Epoch: 1, Index: 345, Loss: 2.8721\n",
      "Epoch: 1, Index: 346, Loss: 0.4157\n",
      "Epoch: 1, Index: 347, Loss: 1.4644\n",
      "Epoch: 1, Index: 348, Loss: 1.7837\n",
      "Epoch: 1, Index: 349, Loss: 2.2911\n",
      "Epoch: 1, Index: 350, Loss: 5.1370\n",
      "Epoch: 1, Index: 351, Loss: 0.8770\n",
      "Epoch: 1, Index: 352, Loss: 0.9224\n",
      "Epoch: 1, Index: 353, Loss: 10.1281\n",
      "Epoch: 1, Index: 354, Loss: 1.6884\n",
      "Epoch: 1, Index: 355, Loss: 4.0920\n",
      "Epoch: 1, Index: 356, Loss: 7.6233\n",
      "Epoch: 1, Index: 357, Loss: 0.1160\n",
      "Epoch: 1, Index: 358, Loss: 0.3429\n",
      "Epoch: 1, Index: 359, Loss: 1.0815\n",
      "Epoch: 1, Index: 360, Loss: 1.0894\n",
      "Epoch: 1, Index: 361, Loss: 1.0163\n",
      "Epoch: 1, Index: 362, Loss: 8.6611\n",
      "Epoch: 1, Index: 363, Loss: 0.8706\n",
      "Epoch: 1, Index: 364, Loss: 3.7535\n",
      "Epoch: 1, Index: 365, Loss: 1.9549\n",
      "Epoch: 1, Index: 366, Loss: 0.3943\n",
      "Epoch: 1, Index: 367, Loss: 3.3430\n",
      "Epoch: 1, Index: 368, Loss: 0.5504\n",
      "Epoch: 1, Index: 369, Loss: 3.2668\n",
      "Epoch: 1, Index: 370, Loss: 5.3661\n",
      "Epoch: 1, Index: 371, Loss: 1.7664\n",
      "Epoch: 1, Index: 372, Loss: 0.2642\n",
      "Epoch: 1, Index: 373, Loss: 0.8653\n",
      "Epoch: 1, Index: 374, Loss: 0.5913\n",
      "Epoch: 1, Index: 375, Loss: 3.3041\n",
      "Epoch: 1, Index: 376, Loss: 3.6539\n",
      "Epoch: 1, Index: 377, Loss: 1.7050\n",
      "Epoch: 1, Index: 378, Loss: 1.1818\n",
      "Epoch: 1, Index: 379, Loss: 3.1209\n",
      "Epoch: 1, Index: 380, Loss: 0.5301\n",
      "Epoch: 1, Index: 381, Loss: 3.5901\n",
      "Epoch: 1, Index: 382, Loss: 7.6510\n",
      "Epoch: 1, Index: 383, Loss: 0.0710\n",
      "Epoch: 1, Index: 384, Loss: 3.8078\n",
      "Epoch: 1, Index: 385, Loss: 0.0103\n",
      "Epoch: 1, Index: 386, Loss: 2.9987\n",
      "Epoch: 1, Index: 387, Loss: 0.6462\n",
      "Epoch: 1, Index: 388, Loss: 4.6455\n",
      "Epoch: 1, Index: 389, Loss: 1.0892\n",
      "Epoch: 1, Index: 390, Loss: 0.0364\n",
      "Epoch: 1, Index: 391, Loss: 4.3493\n",
      "Epoch: 1, Index: 392, Loss: 1.1072\n",
      "Epoch: 1, Index: 393, Loss: 1.0336\n",
      "Epoch: 1, Index: 394, Loss: 3.3271\n",
      "Epoch: 1, Index: 395, Loss: 3.7236\n",
      "Epoch: 1, Index: 396, Loss: 4.9705\n",
      "Epoch: 1, Index: 397, Loss: 0.5951\n",
      "Epoch: 1, Index: 398, Loss: 0.7320\n",
      "Epoch: 1, Index: 399, Loss: 2.2688\n",
      "Epoch: 1, Index: 400, Loss: 0.0541\n",
      "Epoch: 1, Index: 401, Loss: 0.1343\n",
      "Epoch: 1, Index: 402, Loss: 0.0877\n",
      "Epoch: 1, Index: 403, Loss: 0.0110\n",
      "Epoch: 1, Index: 404, Loss: 0.4286\n",
      "Epoch: 1, Index: 405, Loss: 3.6040\n",
      "Epoch: 1, Index: 406, Loss: 4.4811\n",
      "Epoch: 1, Index: 407, Loss: 0.7092\n",
      "Epoch: 1, Index: 408, Loss: 4.6122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae74a892728d49c2a0bf089e282dbd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Index: 0, Loss: 1.3702\n",
      "Epoch: 2, Index: 1, Loss: 1.0608\n",
      "Epoch: 2, Index: 2, Loss: 1.5628\n",
      "Epoch: 2, Index: 3, Loss: 0.5301\n",
      "Epoch: 2, Index: 4, Loss: 0.8360\n",
      "Epoch: 2, Index: 5, Loss: 0.2934\n",
      "Epoch: 2, Index: 6, Loss: 1.3662\n",
      "Epoch: 2, Index: 7, Loss: 2.8334\n",
      "Epoch: 2, Index: 8, Loss: 1.8112\n",
      "Epoch: 2, Index: 9, Loss: 18.8837\n",
      "Epoch: 2, Index: 10, Loss: 2.4558\n",
      "Epoch: 2, Index: 11, Loss: 5.7556\n",
      "Epoch: 2, Index: 12, Loss: 0.0360\n",
      "Epoch: 2, Index: 13, Loss: 1.8386\n",
      "Epoch: 2, Index: 14, Loss: 1.5072\n",
      "Epoch: 2, Index: 15, Loss: 4.9690\n",
      "Epoch: 2, Index: 16, Loss: 3.3264\n",
      "Epoch: 2, Index: 17, Loss: 1.9562\n",
      "Epoch: 2, Index: 18, Loss: 0.5781\n",
      "Epoch: 2, Index: 19, Loss: 3.9789\n",
      "Epoch: 2, Index: 20, Loss: 4.2799\n",
      "Epoch: 2, Index: 21, Loss: 3.0659\n",
      "Epoch: 2, Index: 22, Loss: 1.1613\n",
      "Epoch: 2, Index: 23, Loss: 8.5931\n",
      "Epoch: 2, Index: 24, Loss: 0.4543\n",
      "Epoch: 2, Index: 25, Loss: 0.1627\n",
      "Epoch: 2, Index: 26, Loss: 4.2738\n",
      "Epoch: 2, Index: 27, Loss: 3.7924\n",
      "Epoch: 2, Index: 28, Loss: 5.7384\n",
      "Epoch: 2, Index: 29, Loss: 2.4497\n",
      "Epoch: 2, Index: 30, Loss: 1.1928\n",
      "Epoch: 2, Index: 31, Loss: 1.3864\n",
      "Epoch: 2, Index: 32, Loss: 0.3402\n",
      "Epoch: 2, Index: 33, Loss: 0.5335\n",
      "Epoch: 2, Index: 34, Loss: 0.9853\n",
      "Epoch: 2, Index: 35, Loss: 0.2188\n",
      "Epoch: 2, Index: 36, Loss: 0.9287\n",
      "Epoch: 2, Index: 37, Loss: 3.0588\n",
      "Epoch: 2, Index: 38, Loss: 12.4270\n",
      "Epoch: 2, Index: 39, Loss: 1.5817\n",
      "Epoch: 2, Index: 40, Loss: 0.1983\n",
      "Epoch: 2, Index: 41, Loss: 1.2218\n",
      "Epoch: 2, Index: 42, Loss: 2.0893\n",
      "Epoch: 2, Index: 43, Loss: 0.3938\n",
      "Epoch: 2, Index: 44, Loss: 1.1216\n",
      "Epoch: 2, Index: 45, Loss: 19.0084\n",
      "Epoch: 2, Index: 46, Loss: 0.1543\n",
      "Epoch: 2, Index: 47, Loss: 1.2136\n",
      "Epoch: 2, Index: 48, Loss: 0.4964\n",
      "Epoch: 2, Index: 49, Loss: 2.1326\n",
      "Epoch: 2, Index: 50, Loss: 0.5743\n",
      "Epoch: 2, Index: 51, Loss: 6.3760\n",
      "Epoch: 2, Index: 52, Loss: 2.7616\n",
      "Epoch: 2, Index: 53, Loss: 7.5194\n",
      "Epoch: 2, Index: 54, Loss: 6.6317\n",
      "Epoch: 2, Index: 55, Loss: 0.8123\n",
      "Epoch: 2, Index: 56, Loss: 1.4401\n",
      "Epoch: 2, Index: 57, Loss: 3.7028\n",
      "Epoch: 2, Index: 58, Loss: 2.7245\n",
      "Epoch: 2, Index: 59, Loss: 8.4442\n",
      "Epoch: 2, Index: 60, Loss: 1.2669\n",
      "Epoch: 2, Index: 61, Loss: 1.3972\n",
      "Epoch: 2, Index: 62, Loss: 1.1480\n",
      "Epoch: 2, Index: 63, Loss: 0.6948\n",
      "Epoch: 2, Index: 64, Loss: 1.4766\n",
      "Epoch: 2, Index: 65, Loss: 2.1261\n",
      "Epoch: 2, Index: 66, Loss: 0.0590\n",
      "Epoch: 2, Index: 67, Loss: 1.7965\n",
      "Epoch: 2, Index: 68, Loss: 2.2493\n",
      "Epoch: 2, Index: 69, Loss: 1.5224\n",
      "Epoch: 2, Index: 70, Loss: 0.5715\n",
      "Epoch: 2, Index: 71, Loss: 2.3577\n",
      "Epoch: 2, Index: 72, Loss: 0.9313\n",
      "Epoch: 2, Index: 73, Loss: 2.1606\n",
      "Epoch: 2, Index: 74, Loss: 0.3992\n",
      "Epoch: 2, Index: 75, Loss: 0.5311\n",
      "Epoch: 2, Index: 76, Loss: 0.5007\n",
      "Epoch: 2, Index: 77, Loss: 2.0979\n",
      "Epoch: 2, Index: 78, Loss: 0.6468\n",
      "Epoch: 2, Index: 79, Loss: 7.2832\n",
      "Epoch: 2, Index: 80, Loss: 0.3916\n",
      "Epoch: 2, Index: 81, Loss: 1.7319\n",
      "Epoch: 2, Index: 82, Loss: 3.2160\n",
      "Epoch: 2, Index: 83, Loss: 0.0911\n",
      "Epoch: 2, Index: 84, Loss: 1.0249\n",
      "Epoch: 2, Index: 85, Loss: 0.2075\n",
      "Epoch: 2, Index: 86, Loss: 0.8405\n",
      "Epoch: 2, Index: 87, Loss: 0.7148\n",
      "Epoch: 2, Index: 88, Loss: 7.3507\n",
      "Epoch: 2, Index: 89, Loss: 3.2372\n",
      "Epoch: 2, Index: 90, Loss: 4.6719\n",
      "Epoch: 2, Index: 91, Loss: 1.0167\n",
      "Epoch: 2, Index: 92, Loss: 1.8501\n",
      "Epoch: 2, Index: 93, Loss: 0.1961\n",
      "Epoch: 2, Index: 94, Loss: 0.7996\n",
      "Epoch: 2, Index: 95, Loss: 0.8002\n",
      "Epoch: 2, Index: 96, Loss: 4.4447\n",
      "Epoch: 2, Index: 97, Loss: 1.2687\n",
      "Epoch: 2, Index: 98, Loss: 1.6571\n",
      "Epoch: 2, Index: 99, Loss: 3.5197\n",
      "Epoch: 2, Index: 100, Loss: 1.3347\n",
      "Epoch: 2, Index: 101, Loss: 0.7238\n",
      "Epoch: 2, Index: 102, Loss: 0.3345\n",
      "Epoch: 2, Index: 103, Loss: 1.6412\n",
      "Epoch: 2, Index: 104, Loss: 5.2698\n",
      "Epoch: 2, Index: 105, Loss: 1.7677\n",
      "Epoch: 2, Index: 106, Loss: 1.6565\n",
      "Epoch: 2, Index: 107, Loss: 6.3103\n",
      "Epoch: 2, Index: 108, Loss: 0.5458\n",
      "Epoch: 2, Index: 109, Loss: 2.1500\n",
      "Epoch: 2, Index: 110, Loss: 0.9556\n",
      "Epoch: 2, Index: 111, Loss: 0.4239\n",
      "Epoch: 2, Index: 112, Loss: 3.5413\n",
      "Epoch: 2, Index: 113, Loss: 0.2216\n",
      "Epoch: 2, Index: 114, Loss: 1.6657\n",
      "Epoch: 2, Index: 115, Loss: 0.7433\n",
      "Epoch: 2, Index: 116, Loss: 0.8624\n",
      "Epoch: 2, Index: 117, Loss: 6.2029\n",
      "Epoch: 2, Index: 118, Loss: 5.5423\n",
      "Epoch: 2, Index: 119, Loss: 3.6439\n",
      "Epoch: 2, Index: 120, Loss: 0.8883\n",
      "Epoch: 2, Index: 121, Loss: 1.9525\n",
      "Epoch: 2, Index: 122, Loss: 0.1017\n",
      "Epoch: 2, Index: 123, Loss: 4.5942\n",
      "Epoch: 2, Index: 124, Loss: 0.6354\n",
      "Epoch: 2, Index: 125, Loss: 0.8572\n",
      "Epoch: 2, Index: 126, Loss: 1.1947\n",
      "Epoch: 2, Index: 127, Loss: 2.0599\n",
      "Epoch: 2, Index: 128, Loss: 6.2569\n",
      "Epoch: 2, Index: 129, Loss: 0.4518\n",
      "Epoch: 2, Index: 130, Loss: 1.7079\n",
      "Epoch: 2, Index: 131, Loss: 1.3377\n",
      "Epoch: 2, Index: 132, Loss: 1.3513\n",
      "Epoch: 2, Index: 133, Loss: 1.4994\n",
      "Epoch: 2, Index: 134, Loss: 1.9469\n",
      "Epoch: 2, Index: 135, Loss: 0.2252\n",
      "Epoch: 2, Index: 136, Loss: 0.8473\n",
      "Epoch: 2, Index: 137, Loss: 0.6455\n",
      "Epoch: 2, Index: 138, Loss: 4.6937\n",
      "Epoch: 2, Index: 139, Loss: 1.8784\n",
      "Epoch: 2, Index: 140, Loss: 0.5118\n",
      "Epoch: 2, Index: 141, Loss: 3.3567\n",
      "Epoch: 2, Index: 142, Loss: 1.3915\n",
      "Epoch: 2, Index: 143, Loss: 1.2623\n",
      "Epoch: 2, Index: 144, Loss: 0.5247\n",
      "Epoch: 2, Index: 145, Loss: 4.5354\n",
      "Epoch: 2, Index: 146, Loss: 2.0548\n",
      "Epoch: 2, Index: 147, Loss: 3.1557\n",
      "Epoch: 2, Index: 148, Loss: 3.1182\n",
      "Epoch: 2, Index: 149, Loss: 0.4291\n",
      "Epoch: 2, Index: 150, Loss: 1.9661\n",
      "Epoch: 2, Index: 151, Loss: 3.6370\n",
      "Epoch: 2, Index: 152, Loss: 1.8512\n",
      "Epoch: 2, Index: 153, Loss: 2.1300\n",
      "Epoch: 2, Index: 154, Loss: 2.4999\n",
      "Epoch: 2, Index: 155, Loss: 10.2523\n",
      "Epoch: 2, Index: 156, Loss: 3.9474\n",
      "Epoch: 2, Index: 157, Loss: 3.7433\n",
      "Epoch: 2, Index: 158, Loss: 1.6455\n",
      "Epoch: 2, Index: 159, Loss: 0.8068\n",
      "Epoch: 2, Index: 160, Loss: 1.9840\n",
      "Epoch: 2, Index: 161, Loss: 1.7318\n",
      "Epoch: 2, Index: 162, Loss: 0.7368\n",
      "Epoch: 2, Index: 163, Loss: 1.8472\n",
      "Epoch: 2, Index: 164, Loss: 0.6384\n",
      "Epoch: 2, Index: 165, Loss: 1.4353\n",
      "Epoch: 2, Index: 166, Loss: 4.6022\n",
      "Epoch: 2, Index: 167, Loss: 1.3745\n",
      "Epoch: 2, Index: 168, Loss: 0.2383\n",
      "Epoch: 2, Index: 169, Loss: 1.7171\n",
      "Epoch: 2, Index: 170, Loss: 1.3970\n",
      "Epoch: 2, Index: 171, Loss: 8.3006\n",
      "Epoch: 2, Index: 172, Loss: 0.5891\n",
      "Epoch: 2, Index: 173, Loss: 0.3373\n",
      "Epoch: 2, Index: 174, Loss: 2.8625\n",
      "Epoch: 2, Index: 175, Loss: 3.3261\n",
      "Epoch: 2, Index: 176, Loss: 4.0092\n",
      "Epoch: 2, Index: 177, Loss: 9.8399\n",
      "Epoch: 2, Index: 178, Loss: 4.1638\n",
      "Epoch: 2, Index: 179, Loss: 1.3510\n",
      "Epoch: 2, Index: 180, Loss: 2.8247\n",
      "Epoch: 2, Index: 181, Loss: 3.0255\n",
      "Epoch: 2, Index: 182, Loss: 0.5234\n",
      "Epoch: 2, Index: 183, Loss: 2.5612\n",
      "Epoch: 2, Index: 184, Loss: 7.5110\n",
      "Epoch: 2, Index: 185, Loss: 0.7715\n",
      "Epoch: 2, Index: 186, Loss: 0.0326\n",
      "Epoch: 2, Index: 187, Loss: 6.9950\n",
      "Epoch: 2, Index: 188, Loss: 0.1509\n",
      "Epoch: 2, Index: 189, Loss: 0.9424\n",
      "Epoch: 2, Index: 190, Loss: 2.5206\n",
      "Epoch: 2, Index: 191, Loss: 4.3006\n",
      "Epoch: 2, Index: 192, Loss: 9.1165\n",
      "Epoch: 2, Index: 193, Loss: 0.0079\n",
      "Epoch: 2, Index: 194, Loss: 0.4088\n",
      "Epoch: 2, Index: 195, Loss: 0.1792\n",
      "Epoch: 2, Index: 196, Loss: 1.6100\n",
      "Epoch: 2, Index: 197, Loss: 0.3130\n",
      "Epoch: 2, Index: 198, Loss: 13.2307\n",
      "Epoch: 2, Index: 199, Loss: 0.5461\n",
      "Epoch: 2, Index: 200, Loss: 2.6675\n",
      "Epoch: 2, Index: 201, Loss: 2.3974\n",
      "Epoch: 2, Index: 202, Loss: 12.0916\n",
      "Epoch: 2, Index: 203, Loss: 0.2453\n",
      "Epoch: 2, Index: 204, Loss: 4.4254\n",
      "Epoch: 2, Index: 205, Loss: 5.2705\n",
      "Epoch: 2, Index: 206, Loss: 0.3540\n",
      "Epoch: 2, Index: 207, Loss: 5.3626\n",
      "Epoch: 2, Index: 208, Loss: 1.6861\n",
      "Epoch: 2, Index: 209, Loss: 12.5413\n",
      "Epoch: 2, Index: 210, Loss: 1.1483\n",
      "Epoch: 2, Index: 211, Loss: 3.3186\n",
      "Epoch: 2, Index: 212, Loss: 0.0114\n",
      "Epoch: 2, Index: 213, Loss: 3.9903\n",
      "Epoch: 2, Index: 214, Loss: 1.7862\n",
      "Epoch: 2, Index: 215, Loss: 0.3430\n",
      "Epoch: 2, Index: 216, Loss: 0.6580\n",
      "Epoch: 2, Index: 217, Loss: 0.0984\n",
      "Epoch: 2, Index: 218, Loss: 4.0390\n",
      "Epoch: 2, Index: 219, Loss: 0.3879\n",
      "Epoch: 2, Index: 220, Loss: 2.1963\n",
      "Epoch: 2, Index: 221, Loss: 0.4634\n",
      "Epoch: 2, Index: 222, Loss: 1.1641\n",
      "Epoch: 2, Index: 223, Loss: 0.7388\n",
      "Epoch: 2, Index: 224, Loss: 0.0320\n",
      "Epoch: 2, Index: 225, Loss: 1.8377\n",
      "Epoch: 2, Index: 226, Loss: 0.2530\n",
      "Epoch: 2, Index: 227, Loss: 4.5749\n",
      "Epoch: 2, Index: 228, Loss: 5.4314\n",
      "Epoch: 2, Index: 229, Loss: 1.1040\n",
      "Epoch: 2, Index: 230, Loss: 4.8345\n",
      "Epoch: 2, Index: 231, Loss: 1.0602\n",
      "Epoch: 2, Index: 232, Loss: 5.4454\n",
      "Epoch: 2, Index: 233, Loss: 2.0290\n",
      "Epoch: 2, Index: 234, Loss: 3.0734\n",
      "Epoch: 2, Index: 235, Loss: 1.3353\n",
      "Epoch: 2, Index: 236, Loss: 2.1021\n",
      "Epoch: 2, Index: 237, Loss: 1.2002\n",
      "Epoch: 2, Index: 238, Loss: 1.9294\n",
      "Epoch: 2, Index: 239, Loss: 1.7367\n",
      "Epoch: 2, Index: 240, Loss: 1.9963\n",
      "Epoch: 2, Index: 241, Loss: 1.1085\n",
      "Epoch: 2, Index: 242, Loss: 1.7035\n",
      "Epoch: 2, Index: 243, Loss: 2.4186\n",
      "Epoch: 2, Index: 244, Loss: 3.3536\n",
      "Epoch: 2, Index: 245, Loss: 0.0136\n",
      "Epoch: 2, Index: 246, Loss: 0.9983\n",
      "Epoch: 2, Index: 247, Loss: 1.5221\n",
      "Epoch: 2, Index: 248, Loss: 3.4573\n",
      "Epoch: 2, Index: 249, Loss: 6.7170\n",
      "Epoch: 2, Index: 250, Loss: 0.4029\n",
      "Epoch: 2, Index: 251, Loss: 0.9559\n",
      "Epoch: 2, Index: 252, Loss: 6.3206\n",
      "Epoch: 2, Index: 253, Loss: 0.6365\n",
      "Epoch: 2, Index: 254, Loss: 0.3973\n",
      "Epoch: 2, Index: 255, Loss: 5.1424\n",
      "Epoch: 2, Index: 256, Loss: 11.1844\n",
      "Epoch: 2, Index: 257, Loss: 3.3038\n",
      "Epoch: 2, Index: 258, Loss: 0.2225\n",
      "Epoch: 2, Index: 259, Loss: 0.3158\n",
      "Epoch: 2, Index: 260, Loss: 3.1923\n",
      "Epoch: 2, Index: 261, Loss: 2.3880\n",
      "Epoch: 2, Index: 262, Loss: 2.1266\n",
      "Epoch: 2, Index: 263, Loss: 0.6258\n",
      "Epoch: 2, Index: 264, Loss: 5.8194\n",
      "Epoch: 2, Index: 265, Loss: 0.2896\n",
      "Epoch: 2, Index: 266, Loss: 0.0332\n",
      "Epoch: 2, Index: 267, Loss: 0.6723\n",
      "Epoch: 2, Index: 268, Loss: 1.3912\n",
      "Epoch: 2, Index: 269, Loss: 0.5256\n",
      "Epoch: 2, Index: 270, Loss: 0.3582\n",
      "Epoch: 2, Index: 271, Loss: 1.8554\n",
      "Epoch: 2, Index: 272, Loss: 6.4618\n",
      "Epoch: 2, Index: 273, Loss: 1.3186\n",
      "Epoch: 2, Index: 274, Loss: 2.3517\n",
      "Epoch: 2, Index: 275, Loss: 2.8230\n",
      "Epoch: 2, Index: 276, Loss: 0.4769\n",
      "Epoch: 2, Index: 277, Loss: 0.7085\n",
      "Epoch: 2, Index: 278, Loss: 0.3437\n",
      "Epoch: 2, Index: 279, Loss: 4.5358\n",
      "Epoch: 2, Index: 280, Loss: 0.1153\n",
      "Epoch: 2, Index: 281, Loss: 3.7154\n",
      "Epoch: 2, Index: 282, Loss: 0.0475\n",
      "Epoch: 2, Index: 283, Loss: 1.1015\n",
      "Epoch: 2, Index: 284, Loss: 6.8472\n",
      "Epoch: 2, Index: 285, Loss: 1.6676\n",
      "Epoch: 2, Index: 286, Loss: 0.2222\n",
      "Epoch: 2, Index: 287, Loss: 0.8636\n",
      "Epoch: 2, Index: 288, Loss: 3.3364\n",
      "Epoch: 2, Index: 289, Loss: 1.1206\n",
      "Epoch: 2, Index: 290, Loss: 2.6462\n",
      "Epoch: 2, Index: 291, Loss: 0.6678\n",
      "Epoch: 2, Index: 292, Loss: 0.0888\n",
      "Epoch: 2, Index: 293, Loss: 1.0670\n",
      "Epoch: 2, Index: 294, Loss: 0.1436\n",
      "Epoch: 2, Index: 295, Loss: 1.0837\n",
      "Epoch: 2, Index: 296, Loss: 4.5478\n",
      "Epoch: 2, Index: 297, Loss: 9.3342\n",
      "Epoch: 2, Index: 298, Loss: 1.2554\n",
      "Epoch: 2, Index: 299, Loss: 0.0455\n",
      "Epoch: 2, Index: 300, Loss: 2.2862\n",
      "Epoch: 2, Index: 301, Loss: 2.3387\n",
      "Epoch: 2, Index: 302, Loss: 0.0943\n",
      "Epoch: 2, Index: 303, Loss: 3.4325\n",
      "Epoch: 2, Index: 304, Loss: 0.8561\n",
      "Epoch: 2, Index: 305, Loss: 4.6427\n",
      "Epoch: 2, Index: 306, Loss: 1.2581\n",
      "Epoch: 2, Index: 307, Loss: 5.0204\n",
      "Epoch: 2, Index: 308, Loss: 2.0337\n",
      "Epoch: 2, Index: 309, Loss: 0.6199\n",
      "Epoch: 2, Index: 310, Loss: 0.2590\n",
      "Epoch: 2, Index: 311, Loss: 0.3308\n",
      "Epoch: 2, Index: 312, Loss: 3.5737\n",
      "Epoch: 2, Index: 313, Loss: 6.4101\n",
      "Epoch: 2, Index: 314, Loss: 0.6639\n",
      "Epoch: 2, Index: 315, Loss: 1.4348\n",
      "Epoch: 2, Index: 316, Loss: 4.3967\n",
      "Epoch: 2, Index: 317, Loss: 0.1868\n",
      "Epoch: 2, Index: 318, Loss: 5.1440\n",
      "Epoch: 2, Index: 319, Loss: 0.1208\n",
      "Epoch: 2, Index: 320, Loss: 1.3543\n",
      "Epoch: 2, Index: 321, Loss: 0.0666\n",
      "Epoch: 2, Index: 322, Loss: 0.5125\n",
      "Epoch: 2, Index: 323, Loss: 1.8965\n",
      "Epoch: 2, Index: 324, Loss: 0.0466\n",
      "Epoch: 2, Index: 325, Loss: 4.8970\n",
      "Epoch: 2, Index: 326, Loss: 2.8840\n",
      "Epoch: 2, Index: 327, Loss: 5.5301\n",
      "Epoch: 2, Index: 328, Loss: 0.5420\n",
      "Epoch: 2, Index: 329, Loss: 1.6870\n",
      "Epoch: 2, Index: 330, Loss: 0.3384\n",
      "Epoch: 2, Index: 331, Loss: 0.3082\n",
      "Epoch: 2, Index: 332, Loss: 0.3355\n",
      "Epoch: 2, Index: 333, Loss: 0.0306\n",
      "Epoch: 2, Index: 334, Loss: 0.3372\n",
      "Epoch: 2, Index: 335, Loss: 3.7432\n",
      "Epoch: 2, Index: 336, Loss: 0.3422\n",
      "Epoch: 2, Index: 337, Loss: 0.4277\n",
      "Epoch: 2, Index: 338, Loss: 2.1539\n",
      "Epoch: 2, Index: 339, Loss: 0.6778\n",
      "Epoch: 2, Index: 340, Loss: 2.0820\n",
      "Epoch: 2, Index: 341, Loss: 0.0917\n",
      "Epoch: 2, Index: 342, Loss: 0.4952\n",
      "Epoch: 2, Index: 343, Loss: 0.1711\n",
      "Epoch: 2, Index: 344, Loss: 0.9510\n",
      "Epoch: 2, Index: 345, Loss: 0.2525\n",
      "Epoch: 2, Index: 346, Loss: 1.0555\n",
      "Epoch: 2, Index: 347, Loss: 1.2045\n",
      "Epoch: 2, Index: 348, Loss: 2.6027\n",
      "Epoch: 2, Index: 349, Loss: 8.0405\n",
      "Epoch: 2, Index: 350, Loss: 2.6404\n",
      "Epoch: 2, Index: 351, Loss: 4.2153\n",
      "Epoch: 2, Index: 352, Loss: 2.5674\n",
      "Epoch: 2, Index: 353, Loss: 0.5956\n",
      "Epoch: 2, Index: 354, Loss: 2.8624\n",
      "Epoch: 2, Index: 355, Loss: 0.0963\n",
      "Epoch: 2, Index: 356, Loss: 0.5959\n",
      "Epoch: 2, Index: 357, Loss: 2.5209\n",
      "Epoch: 2, Index: 358, Loss: 0.8268\n",
      "Epoch: 2, Index: 359, Loss: 0.7184\n",
      "Epoch: 2, Index: 360, Loss: 2.2956\n",
      "Epoch: 2, Index: 361, Loss: 1.5064\n",
      "Epoch: 2, Index: 362, Loss: 0.4445\n",
      "Epoch: 2, Index: 363, Loss: 3.7750\n",
      "Epoch: 2, Index: 364, Loss: 1.1242\n",
      "Epoch: 2, Index: 365, Loss: 0.8537\n",
      "Epoch: 2, Index: 366, Loss: 6.2679\n",
      "Epoch: 2, Index: 367, Loss: 1.9072\n",
      "Epoch: 2, Index: 368, Loss: 1.2002\n",
      "Epoch: 2, Index: 369, Loss: 4.4484\n",
      "Epoch: 2, Index: 370, Loss: 1.3963\n",
      "Epoch: 2, Index: 371, Loss: 1.0044\n",
      "Epoch: 2, Index: 372, Loss: 1.0162\n",
      "Epoch: 2, Index: 373, Loss: 3.7200\n",
      "Epoch: 2, Index: 374, Loss: 1.1985\n",
      "Epoch: 2, Index: 375, Loss: 5.2590\n",
      "Epoch: 2, Index: 376, Loss: 1.4721\n",
      "Epoch: 2, Index: 377, Loss: 0.2346\n",
      "Epoch: 2, Index: 378, Loss: 3.8581\n",
      "Epoch: 2, Index: 379, Loss: 3.2027\n",
      "Epoch: 2, Index: 380, Loss: 0.6687\n",
      "Epoch: 2, Index: 381, Loss: 1.1580\n",
      "Epoch: 2, Index: 382, Loss: 1.3596\n",
      "Epoch: 2, Index: 383, Loss: 0.0929\n",
      "Epoch: 2, Index: 384, Loss: 1.6079\n",
      "Epoch: 2, Index: 385, Loss: 2.8459\n",
      "Epoch: 2, Index: 386, Loss: 1.8565\n",
      "Epoch: 2, Index: 387, Loss: 1.5185\n",
      "Epoch: 2, Index: 388, Loss: 0.6280\n",
      "Epoch: 2, Index: 389, Loss: 0.0125\n",
      "Epoch: 2, Index: 390, Loss: 0.2678\n",
      "Epoch: 2, Index: 391, Loss: 5.3484\n",
      "Epoch: 2, Index: 392, Loss: 4.0609\n",
      "Epoch: 2, Index: 393, Loss: 0.2549\n",
      "Epoch: 2, Index: 394, Loss: 2.4089\n",
      "Epoch: 2, Index: 395, Loss: 2.9929\n",
      "Epoch: 2, Index: 396, Loss: 0.0515\n",
      "Epoch: 2, Index: 397, Loss: 5.9734\n",
      "Epoch: 2, Index: 398, Loss: 1.3532\n",
      "Epoch: 2, Index: 399, Loss: 4.6269\n",
      "Epoch: 2, Index: 400, Loss: 0.4595\n",
      "Epoch: 2, Index: 401, Loss: 3.1016\n",
      "Epoch: 2, Index: 402, Loss: 0.4082\n",
      "Epoch: 2, Index: 403, Loss: 0.9431\n",
      "Epoch: 2, Index: 404, Loss: 6.8535\n",
      "Epoch: 2, Index: 405, Loss: 0.2278\n",
      "Epoch: 2, Index: 406, Loss: 1.9533\n",
      "Epoch: 2, Index: 407, Loss: 4.1042\n",
      "Epoch: 2, Index: 408, Loss: 1.3568\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38c620946e6430abe8b7b6fe43202d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Index: 0, Loss: 0.9707\n",
      "Epoch: 3, Index: 1, Loss: 2.0259\n",
      "Epoch: 3, Index: 2, Loss: 2.0327\n",
      "Epoch: 3, Index: 3, Loss: 1.2208\n",
      "Epoch: 3, Index: 4, Loss: 1.6115\n",
      "Epoch: 3, Index: 5, Loss: 1.2056\n",
      "Epoch: 3, Index: 6, Loss: 1.0026\n",
      "Epoch: 3, Index: 7, Loss: 0.2998\n",
      "Epoch: 3, Index: 8, Loss: 1.2013\n",
      "Epoch: 3, Index: 9, Loss: 0.3754\n",
      "Epoch: 3, Index: 10, Loss: 4.0846\n",
      "Epoch: 3, Index: 11, Loss: 2.2815\n",
      "Epoch: 3, Index: 12, Loss: 3.1079\n",
      "Epoch: 3, Index: 13, Loss: 3.7588\n",
      "Epoch: 3, Index: 14, Loss: 1.9057\n",
      "Epoch: 3, Index: 15, Loss: 4.3893\n",
      "Epoch: 3, Index: 16, Loss: 3.1478\n",
      "Epoch: 3, Index: 17, Loss: 1.9405\n",
      "Epoch: 3, Index: 18, Loss: 4.1772\n",
      "Epoch: 3, Index: 19, Loss: 1.2151\n",
      "Epoch: 3, Index: 20, Loss: 4.5946\n",
      "Epoch: 3, Index: 21, Loss: 2.0171\n",
      "Epoch: 3, Index: 22, Loss: 1.4949\n",
      "Epoch: 3, Index: 23, Loss: 7.7120\n",
      "Epoch: 3, Index: 24, Loss: 3.6524\n",
      "Epoch: 3, Index: 25, Loss: 0.0291\n",
      "Epoch: 3, Index: 26, Loss: 0.1058\n",
      "Epoch: 3, Index: 27, Loss: 1.9414\n",
      "Epoch: 3, Index: 28, Loss: 0.2209\n",
      "Epoch: 3, Index: 29, Loss: 1.0366\n",
      "Epoch: 3, Index: 30, Loss: 1.7051\n",
      "Epoch: 3, Index: 31, Loss: 3.1056\n",
      "Epoch: 3, Index: 32, Loss: 5.1938\n",
      "Epoch: 3, Index: 33, Loss: 10.9010\n",
      "Epoch: 3, Index: 34, Loss: 1.5759\n",
      "Epoch: 3, Index: 35, Loss: 0.1983\n",
      "Epoch: 3, Index: 36, Loss: 0.9766\n",
      "Epoch: 3, Index: 37, Loss: 1.6292\n",
      "Epoch: 3, Index: 38, Loss: 7.1351\n",
      "Epoch: 3, Index: 39, Loss: 1.6087\n",
      "Epoch: 3, Index: 40, Loss: 0.0913\n",
      "Epoch: 3, Index: 41, Loss: 0.1681\n",
      "Epoch: 3, Index: 42, Loss: 3.4180\n",
      "Epoch: 3, Index: 43, Loss: 0.2223\n",
      "Epoch: 3, Index: 44, Loss: 0.0240\n",
      "Epoch: 3, Index: 45, Loss: 2.9066\n",
      "Epoch: 3, Index: 46, Loss: 0.2024\n",
      "Epoch: 3, Index: 47, Loss: 8.0417\n",
      "Epoch: 3, Index: 48, Loss: 3.6316\n",
      "Epoch: 3, Index: 49, Loss: 2.0358\n",
      "Epoch: 3, Index: 50, Loss: 2.4742\n",
      "Epoch: 3, Index: 51, Loss: 4.0462\n",
      "Epoch: 3, Index: 52, Loss: 0.6835\n",
      "Epoch: 3, Index: 53, Loss: 3.2913\n",
      "Epoch: 3, Index: 54, Loss: 1.2010\n",
      "Epoch: 3, Index: 55, Loss: 3.4989\n",
      "Epoch: 3, Index: 56, Loss: 0.3880\n",
      "Epoch: 3, Index: 57, Loss: 0.1666\n",
      "Epoch: 3, Index: 58, Loss: 2.6665\n",
      "Epoch: 3, Index: 59, Loss: 0.0123\n",
      "Epoch: 3, Index: 60, Loss: 0.1734\n",
      "Epoch: 3, Index: 61, Loss: 2.9376\n",
      "Epoch: 3, Index: 62, Loss: 2.1275\n",
      "Epoch: 3, Index: 63, Loss: 0.7643\n",
      "Epoch: 3, Index: 64, Loss: 4.6469\n",
      "Epoch: 3, Index: 65, Loss: 1.5282\n",
      "Epoch: 3, Index: 66, Loss: 2.5419\n",
      "Epoch: 3, Index: 67, Loss: 7.4822\n",
      "Epoch: 3, Index: 68, Loss: 1.0936\n",
      "Epoch: 3, Index: 69, Loss: 0.6946\n",
      "Epoch: 3, Index: 70, Loss: 0.3834\n",
      "Epoch: 3, Index: 71, Loss: 1.5862\n",
      "Epoch: 3, Index: 72, Loss: 1.1723\n",
      "Epoch: 3, Index: 73, Loss: 0.3309\n",
      "Epoch: 3, Index: 74, Loss: 0.2826\n",
      "Epoch: 3, Index: 75, Loss: 0.5738\n",
      "Epoch: 3, Index: 76, Loss: 0.7986\n",
      "Epoch: 3, Index: 77, Loss: 0.9549\n",
      "Epoch: 3, Index: 78, Loss: 0.3687\n",
      "Epoch: 3, Index: 79, Loss: 1.2697\n",
      "Epoch: 3, Index: 80, Loss: 4.6852\n",
      "Epoch: 3, Index: 81, Loss: 3.4610\n",
      "Epoch: 3, Index: 82, Loss: 1.4106\n",
      "Epoch: 3, Index: 83, Loss: 0.1530\n",
      "Epoch: 3, Index: 84, Loss: 0.2741\n",
      "Epoch: 3, Index: 85, Loss: 1.0361\n",
      "Epoch: 3, Index: 86, Loss: 0.4278\n",
      "Epoch: 3, Index: 87, Loss: 2.9544\n",
      "Epoch: 3, Index: 88, Loss: 0.8442\n",
      "Epoch: 3, Index: 89, Loss: 1.9226\n",
      "Epoch: 3, Index: 90, Loss: 7.4549\n",
      "Epoch: 3, Index: 91, Loss: 1.3506\n",
      "Epoch: 3, Index: 92, Loss: 7.0321\n",
      "Epoch: 3, Index: 93, Loss: 0.8898\n",
      "Epoch: 3, Index: 94, Loss: 2.3860\n",
      "Epoch: 3, Index: 95, Loss: 2.4581\n",
      "Epoch: 3, Index: 96, Loss: 1.2217\n",
      "Epoch: 3, Index: 97, Loss: 1.3448\n",
      "Epoch: 3, Index: 98, Loss: 1.5722\n",
      "Epoch: 3, Index: 99, Loss: 1.3721\n",
      "Epoch: 3, Index: 100, Loss: 11.1071\n",
      "Epoch: 3, Index: 101, Loss: 2.8784\n",
      "Epoch: 3, Index: 102, Loss: 0.5314\n",
      "Epoch: 3, Index: 103, Loss: 0.0523\n",
      "Epoch: 3, Index: 104, Loss: 6.5845\n",
      "Epoch: 3, Index: 105, Loss: 7.2869\n",
      "Epoch: 3, Index: 106, Loss: 0.8833\n",
      "Epoch: 3, Index: 107, Loss: 1.5145\n",
      "Epoch: 3, Index: 108, Loss: 0.9483\n",
      "Epoch: 3, Index: 109, Loss: 1.2171\n",
      "Epoch: 3, Index: 110, Loss: 0.3776\n",
      "Epoch: 3, Index: 111, Loss: 1.3083\n",
      "Epoch: 3, Index: 112, Loss: 0.7982\n",
      "Epoch: 3, Index: 113, Loss: 2.2609\n",
      "Epoch: 3, Index: 114, Loss: 0.0607\n",
      "Epoch: 3, Index: 115, Loss: 1.4679\n",
      "Epoch: 3, Index: 116, Loss: 0.2915\n",
      "Epoch: 3, Index: 117, Loss: 1.2419\n",
      "Epoch: 3, Index: 118, Loss: 2.6853\n",
      "Epoch: 3, Index: 119, Loss: 1.4215\n",
      "Epoch: 3, Index: 120, Loss: 0.4953\n",
      "Epoch: 3, Index: 121, Loss: 1.2490\n",
      "Epoch: 3, Index: 122, Loss: 0.1880\n",
      "Epoch: 3, Index: 123, Loss: 1.1453\n",
      "Epoch: 3, Index: 124, Loss: 2.3313\n",
      "Epoch: 3, Index: 125, Loss: 2.1052\n",
      "Epoch: 3, Index: 126, Loss: 1.5915\n",
      "Epoch: 3, Index: 127, Loss: 2.4501\n",
      "Epoch: 3, Index: 128, Loss: 0.0994\n",
      "Epoch: 3, Index: 129, Loss: 0.3331\n",
      "Epoch: 3, Index: 130, Loss: 3.5211\n",
      "Epoch: 3, Index: 131, Loss: 2.7842\n",
      "Epoch: 3, Index: 132, Loss: 3.1146\n",
      "Epoch: 3, Index: 133, Loss: 0.0046\n",
      "Epoch: 3, Index: 134, Loss: 0.9508\n",
      "Epoch: 3, Index: 135, Loss: 0.1376\n",
      "Epoch: 3, Index: 136, Loss: 0.2913\n",
      "Epoch: 3, Index: 137, Loss: 23.9439\n",
      "Epoch: 3, Index: 138, Loss: 0.5855\n",
      "Epoch: 3, Index: 139, Loss: 2.7375\n",
      "Epoch: 3, Index: 140, Loss: 0.8260\n",
      "Epoch: 3, Index: 141, Loss: 0.0111\n",
      "Epoch: 3, Index: 142, Loss: 1.4321\n",
      "Epoch: 3, Index: 143, Loss: 0.8898\n",
      "Epoch: 3, Index: 144, Loss: 0.6992\n",
      "Epoch: 3, Index: 145, Loss: 1.4886\n",
      "Epoch: 3, Index: 146, Loss: 1.5989\n",
      "Epoch: 3, Index: 147, Loss: 0.2200\n",
      "Epoch: 3, Index: 148, Loss: 0.5325\n",
      "Epoch: 3, Index: 149, Loss: 1.9093\n",
      "Epoch: 3, Index: 150, Loss: 0.3594\n",
      "Epoch: 3, Index: 151, Loss: 5.1763\n",
      "Epoch: 3, Index: 152, Loss: 1.4256\n",
      "Epoch: 3, Index: 153, Loss: 0.5507\n",
      "Epoch: 3, Index: 154, Loss: 10.6773\n",
      "Epoch: 3, Index: 155, Loss: 0.4362\n",
      "Epoch: 3, Index: 156, Loss: 2.1332\n",
      "Epoch: 3, Index: 157, Loss: 0.0752\n",
      "Epoch: 3, Index: 158, Loss: 2.1188\n",
      "Epoch: 3, Index: 159, Loss: 0.6264\n",
      "Epoch: 3, Index: 160, Loss: 0.2593\n",
      "Epoch: 3, Index: 161, Loss: 0.4425\n",
      "Epoch: 3, Index: 162, Loss: 0.0458\n",
      "Epoch: 3, Index: 163, Loss: 0.4038\n",
      "Epoch: 3, Index: 164, Loss: 0.8076\n",
      "Epoch: 3, Index: 165, Loss: 0.6999\n",
      "Epoch: 3, Index: 166, Loss: 0.5701\n",
      "Epoch: 3, Index: 167, Loss: 0.1933\n",
      "Epoch: 3, Index: 168, Loss: 0.4434\n",
      "Epoch: 3, Index: 169, Loss: 8.9134\n",
      "Epoch: 3, Index: 170, Loss: 0.3942\n",
      "Epoch: 3, Index: 171, Loss: 1.1783\n",
      "Epoch: 3, Index: 172, Loss: 0.4069\n",
      "Epoch: 3, Index: 173, Loss: 5.0169\n",
      "Epoch: 3, Index: 174, Loss: 4.0555\n",
      "Epoch: 3, Index: 175, Loss: 2.3938\n",
      "Epoch: 3, Index: 176, Loss: 2.5742\n",
      "Epoch: 3, Index: 177, Loss: 2.0746\n",
      "Epoch: 3, Index: 178, Loss: 3.1704\n",
      "Epoch: 3, Index: 179, Loss: 3.1317\n",
      "Epoch: 3, Index: 180, Loss: 2.4906\n",
      "Epoch: 3, Index: 181, Loss: 0.3175\n",
      "Epoch: 3, Index: 182, Loss: 5.2692\n",
      "Epoch: 3, Index: 183, Loss: 1.7809\n",
      "Epoch: 3, Index: 184, Loss: 3.9251\n",
      "Epoch: 3, Index: 185, Loss: 0.4620\n",
      "Epoch: 3, Index: 186, Loss: 1.0953\n",
      "Epoch: 3, Index: 187, Loss: 3.8521\n",
      "Epoch: 3, Index: 188, Loss: 3.4826\n",
      "Epoch: 3, Index: 189, Loss: 1.8361\n",
      "Epoch: 3, Index: 190, Loss: 1.0060\n",
      "Epoch: 3, Index: 191, Loss: 1.0819\n",
      "Epoch: 3, Index: 192, Loss: 8.7265\n",
      "Epoch: 3, Index: 193, Loss: 0.2787\n",
      "Epoch: 3, Index: 194, Loss: 0.1340\n",
      "Epoch: 3, Index: 195, Loss: 0.5563\n",
      "Epoch: 3, Index: 196, Loss: 0.6754\n",
      "Epoch: 3, Index: 197, Loss: 0.0469\n",
      "Epoch: 3, Index: 198, Loss: 1.2503\n",
      "Epoch: 3, Index: 199, Loss: 1.7909\n",
      "Epoch: 3, Index: 200, Loss: 2.6102\n",
      "Epoch: 3, Index: 201, Loss: 0.4834\n",
      "Epoch: 3, Index: 202, Loss: 1.2704\n",
      "Epoch: 3, Index: 203, Loss: 2.4269\n",
      "Epoch: 3, Index: 204, Loss: 0.0994\n",
      "Epoch: 3, Index: 205, Loss: 2.6158\n",
      "Epoch: 3, Index: 206, Loss: 1.9857\n",
      "Epoch: 3, Index: 207, Loss: 3.0568\n",
      "Epoch: 3, Index: 208, Loss: 0.0585\n",
      "Epoch: 3, Index: 209, Loss: 2.7247\n",
      "Epoch: 3, Index: 210, Loss: 2.2176\n",
      "Epoch: 3, Index: 211, Loss: 1.3810\n",
      "Epoch: 3, Index: 212, Loss: 1.1687\n",
      "Epoch: 3, Index: 213, Loss: 1.3540\n",
      "Epoch: 3, Index: 214, Loss: 4.7747\n",
      "Epoch: 3, Index: 215, Loss: 3.6071\n",
      "Epoch: 3, Index: 216, Loss: 0.2516\n",
      "Epoch: 3, Index: 217, Loss: 2.5258\n",
      "Epoch: 3, Index: 218, Loss: 1.3932\n",
      "Epoch: 3, Index: 219, Loss: 0.3362\n",
      "Epoch: 3, Index: 220, Loss: 1.0073\n",
      "Epoch: 3, Index: 221, Loss: 0.5073\n",
      "Epoch: 3, Index: 222, Loss: 0.1063\n",
      "Epoch: 3, Index: 223, Loss: 0.4689\n",
      "Epoch: 3, Index: 224, Loss: 0.5811\n",
      "Epoch: 3, Index: 225, Loss: 0.2894\n",
      "Epoch: 3, Index: 226, Loss: 0.6062\n",
      "Epoch: 3, Index: 227, Loss: 3.1579\n",
      "Epoch: 3, Index: 228, Loss: 4.1030\n",
      "Epoch: 3, Index: 229, Loss: 3.0695\n",
      "Epoch: 3, Index: 230, Loss: 2.0001\n",
      "Epoch: 3, Index: 231, Loss: 4.2622\n",
      "Epoch: 3, Index: 232, Loss: 0.0093\n",
      "Epoch: 3, Index: 233, Loss: 2.3785\n",
      "Epoch: 3, Index: 234, Loss: 0.3308\n",
      "Epoch: 3, Index: 235, Loss: 4.5157\n",
      "Epoch: 3, Index: 236, Loss: 1.0550\n",
      "Epoch: 3, Index: 237, Loss: 0.6390\n",
      "Epoch: 3, Index: 238, Loss: 0.2846\n",
      "Epoch: 3, Index: 239, Loss: 4.3874\n",
      "Epoch: 3, Index: 240, Loss: 0.7726\n",
      "Epoch: 3, Index: 241, Loss: 0.3010\n",
      "Epoch: 3, Index: 242, Loss: 0.1168\n",
      "Epoch: 3, Index: 243, Loss: 0.7386\n",
      "Epoch: 3, Index: 244, Loss: 0.7146\n",
      "Epoch: 3, Index: 245, Loss: 0.8636\n",
      "Epoch: 3, Index: 246, Loss: 6.0359\n",
      "Epoch: 3, Index: 247, Loss: 1.6809\n",
      "Epoch: 3, Index: 248, Loss: 5.8313\n",
      "Epoch: 3, Index: 249, Loss: 0.0433\n",
      "Epoch: 3, Index: 250, Loss: 0.5696\n",
      "Epoch: 3, Index: 251, Loss: 1.2718\n",
      "Epoch: 3, Index: 252, Loss: 3.1560\n",
      "Epoch: 3, Index: 253, Loss: 0.9790\n",
      "Epoch: 3, Index: 254, Loss: 1.7002\n",
      "Epoch: 3, Index: 255, Loss: 0.2913\n",
      "Epoch: 3, Index: 256, Loss: 9.7697\n",
      "Epoch: 3, Index: 257, Loss: 0.0013\n",
      "Epoch: 3, Index: 258, Loss: 0.4755\n",
      "Epoch: 3, Index: 259, Loss: 3.2442\n",
      "Epoch: 3, Index: 260, Loss: 0.3459\n",
      "Epoch: 3, Index: 261, Loss: 2.0316\n",
      "Epoch: 3, Index: 262, Loss: 3.1699\n",
      "Epoch: 3, Index: 263, Loss: 0.2010\n",
      "Epoch: 3, Index: 264, Loss: 2.0935\n",
      "Epoch: 3, Index: 265, Loss: 2.5777\n",
      "Epoch: 3, Index: 266, Loss: 6.5472\n",
      "Epoch: 3, Index: 267, Loss: 1.0999\n",
      "Epoch: 3, Index: 268, Loss: 1.5566\n",
      "Epoch: 3, Index: 269, Loss: 3.9072\n",
      "Epoch: 3, Index: 270, Loss: 2.8820\n",
      "Epoch: 3, Index: 271, Loss: 0.6006\n",
      "Epoch: 3, Index: 272, Loss: 3.7280\n",
      "Epoch: 3, Index: 273, Loss: 0.2482\n",
      "Epoch: 3, Index: 274, Loss: 3.9867\n",
      "Epoch: 3, Index: 275, Loss: 2.8747\n",
      "Epoch: 3, Index: 276, Loss: 2.1785\n",
      "Epoch: 3, Index: 277, Loss: 1.0500\n",
      "Epoch: 3, Index: 278, Loss: 0.9580\n",
      "Epoch: 3, Index: 279, Loss: 5.4431\n",
      "Epoch: 3, Index: 280, Loss: 4.0959\n",
      "Epoch: 3, Index: 281, Loss: 1.9185\n",
      "Epoch: 3, Index: 282, Loss: 0.6185\n",
      "Epoch: 3, Index: 283, Loss: 0.8582\n",
      "Epoch: 3, Index: 284, Loss: 2.5020\n",
      "Epoch: 3, Index: 285, Loss: 0.6089\n",
      "Epoch: 3, Index: 286, Loss: 0.4074\n",
      "Epoch: 3, Index: 287, Loss: 0.9531\n",
      "Epoch: 3, Index: 288, Loss: 1.5996\n",
      "Epoch: 3, Index: 289, Loss: 3.5703\n",
      "Epoch: 3, Index: 290, Loss: 0.3569\n",
      "Epoch: 3, Index: 291, Loss: 0.0236\n",
      "Epoch: 3, Index: 292, Loss: 1.8519\n",
      "Epoch: 3, Index: 293, Loss: 2.7393\n",
      "Epoch: 3, Index: 294, Loss: 0.0891\n",
      "Epoch: 3, Index: 295, Loss: 0.8664\n",
      "Epoch: 3, Index: 296, Loss: 4.1293\n",
      "Epoch: 3, Index: 297, Loss: 5.9207\n",
      "Epoch: 3, Index: 298, Loss: 3.0831\n",
      "Epoch: 3, Index: 299, Loss: 0.0112\n",
      "Epoch: 3, Index: 300, Loss: 1.0448\n",
      "Epoch: 3, Index: 301, Loss: 1.1848\n",
      "Epoch: 3, Index: 302, Loss: 2.3986\n",
      "Epoch: 3, Index: 303, Loss: 1.0632\n",
      "Epoch: 3, Index: 304, Loss: 1.3101\n",
      "Epoch: 3, Index: 305, Loss: 2.4962\n",
      "Epoch: 3, Index: 306, Loss: 0.4807\n",
      "Epoch: 3, Index: 307, Loss: 1.5447\n",
      "Epoch: 3, Index: 308, Loss: 0.9820\n",
      "Epoch: 3, Index: 309, Loss: 0.6847\n",
      "Epoch: 3, Index: 310, Loss: 1.8118\n",
      "Epoch: 3, Index: 311, Loss: 1.7145\n",
      "Epoch: 3, Index: 312, Loss: 0.6753\n",
      "Epoch: 3, Index: 313, Loss: 0.2932\n",
      "Epoch: 3, Index: 314, Loss: 0.4110\n",
      "Epoch: 3, Index: 315, Loss: 7.7099\n",
      "Epoch: 3, Index: 316, Loss: 5.0242\n",
      "Epoch: 3, Index: 317, Loss: 1.4571\n",
      "Epoch: 3, Index: 318, Loss: 0.3382\n",
      "Epoch: 3, Index: 319, Loss: 1.0048\n",
      "Epoch: 3, Index: 320, Loss: 5.4104\n",
      "Epoch: 3, Index: 321, Loss: 0.9537\n",
      "Epoch: 3, Index: 322, Loss: 1.3898\n",
      "Epoch: 3, Index: 323, Loss: 0.2280\n",
      "Epoch: 3, Index: 324, Loss: 0.6584\n",
      "Epoch: 3, Index: 325, Loss: 0.0001\n",
      "Epoch: 3, Index: 326, Loss: 1.2307\n",
      "Epoch: 3, Index: 327, Loss: 2.5731\n",
      "Epoch: 3, Index: 328, Loss: 0.3624\n",
      "Epoch: 3, Index: 329, Loss: 2.2076\n",
      "Epoch: 3, Index: 330, Loss: 0.0422\n",
      "Epoch: 3, Index: 331, Loss: 1.1204\n",
      "Epoch: 3, Index: 332, Loss: 0.4431\n",
      "Epoch: 3, Index: 333, Loss: 0.5450\n",
      "Epoch: 3, Index: 334, Loss: 7.7697\n",
      "Epoch: 3, Index: 335, Loss: 0.2381\n",
      "Epoch: 3, Index: 336, Loss: 4.6417\n",
      "Epoch: 3, Index: 337, Loss: 0.0625\n",
      "Epoch: 3, Index: 338, Loss: 0.2579\n",
      "Epoch: 3, Index: 339, Loss: 0.1913\n",
      "Epoch: 3, Index: 340, Loss: 2.6066\n",
      "Epoch: 3, Index: 341, Loss: 0.8858\n",
      "Epoch: 3, Index: 342, Loss: 1.4468\n",
      "Epoch: 3, Index: 343, Loss: 1.2861\n",
      "Epoch: 3, Index: 344, Loss: 0.2493\n",
      "Epoch: 3, Index: 345, Loss: 3.0670\n",
      "Epoch: 3, Index: 346, Loss: 2.7514\n",
      "Epoch: 3, Index: 347, Loss: 0.2663\n",
      "Epoch: 3, Index: 348, Loss: 2.6844\n",
      "Epoch: 3, Index: 349, Loss: 1.4115\n",
      "Epoch: 3, Index: 350, Loss: 0.4959\n",
      "Epoch: 3, Index: 351, Loss: 0.9686\n",
      "Epoch: 3, Index: 352, Loss: 0.3051\n",
      "Epoch: 3, Index: 353, Loss: 18.5026\n",
      "Epoch: 3, Index: 354, Loss: 0.9395\n",
      "Epoch: 3, Index: 355, Loss: 6.3557\n",
      "Epoch: 3, Index: 356, Loss: 4.7061\n",
      "Epoch: 3, Index: 357, Loss: 0.9622\n",
      "Epoch: 3, Index: 358, Loss: 3.1145\n",
      "Epoch: 3, Index: 359, Loss: 4.8778\n",
      "Epoch: 3, Index: 360, Loss: 0.2199\n",
      "Epoch: 3, Index: 361, Loss: 2.3531\n",
      "Epoch: 3, Index: 362, Loss: 0.0003\n",
      "Epoch: 3, Index: 363, Loss: 0.3373\n",
      "Epoch: 3, Index: 364, Loss: 1.7978\n",
      "Epoch: 3, Index: 365, Loss: 0.8799\n",
      "Epoch: 3, Index: 366, Loss: 0.2583\n",
      "Epoch: 3, Index: 367, Loss: 0.3306\n",
      "Epoch: 3, Index: 368, Loss: 1.2401\n",
      "Epoch: 3, Index: 369, Loss: 1.2226\n",
      "Epoch: 3, Index: 370, Loss: 3.6219\n",
      "Epoch: 3, Index: 371, Loss: 2.0886\n",
      "Epoch: 3, Index: 372, Loss: 2.5286\n",
      "Epoch: 3, Index: 373, Loss: 2.5962\n",
      "Epoch: 3, Index: 374, Loss: 1.1974\n",
      "Epoch: 3, Index: 375, Loss: 0.6100\n",
      "Epoch: 3, Index: 376, Loss: 0.7650\n",
      "Epoch: 3, Index: 377, Loss: 2.1154\n",
      "Epoch: 3, Index: 378, Loss: 0.4625\n",
      "Epoch: 3, Index: 379, Loss: 1.2256\n",
      "Epoch: 3, Index: 380, Loss: 2.0482\n",
      "Epoch: 3, Index: 381, Loss: 0.8874\n",
      "Epoch: 3, Index: 382, Loss: 1.0885\n",
      "Epoch: 3, Index: 383, Loss: 0.0552\n",
      "Epoch: 3, Index: 384, Loss: 4.3219\n",
      "Epoch: 3, Index: 385, Loss: 1.7255\n",
      "Epoch: 3, Index: 386, Loss: 7.2565\n",
      "Epoch: 3, Index: 387, Loss: 5.4324\n",
      "Epoch: 3, Index: 388, Loss: 1.5981\n",
      "Epoch: 3, Index: 389, Loss: 0.1985\n",
      "Epoch: 3, Index: 390, Loss: 0.5426\n",
      "Epoch: 3, Index: 391, Loss: 7.7052\n",
      "Epoch: 3, Index: 392, Loss: 2.1732\n",
      "Epoch: 3, Index: 393, Loss: 0.1175\n",
      "Epoch: 3, Index: 394, Loss: 1.0283\n",
      "Epoch: 3, Index: 395, Loss: 1.6587\n",
      "Epoch: 3, Index: 396, Loss: 10.5083\n",
      "Epoch: 3, Index: 397, Loss: 12.7064\n",
      "Epoch: 3, Index: 398, Loss: 5.0337\n",
      "Epoch: 3, Index: 399, Loss: 0.1163\n",
      "Epoch: 3, Index: 400, Loss: 0.1141\n",
      "Epoch: 3, Index: 401, Loss: 3.1500\n",
      "Epoch: 3, Index: 402, Loss: 1.3586\n",
      "Epoch: 3, Index: 403, Loss: 0.1208\n",
      "Epoch: 3, Index: 404, Loss: 0.0311\n",
      "Epoch: 3, Index: 405, Loss: 5.5134\n",
      "Epoch: 3, Index: 406, Loss: 0.7891\n",
      "Epoch: 3, Index: 407, Loss: 1.1689\n",
      "Epoch: 3, Index: 408, Loss: 0.8481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3aaea53d3274d0e887db2c78bb6763b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Index: 0, Loss: 2.8838\n",
      "Epoch: 4, Index: 1, Loss: 3.7780\n",
      "Epoch: 4, Index: 2, Loss: 1.7586\n",
      "Epoch: 4, Index: 3, Loss: 0.8103\n",
      "Epoch: 4, Index: 4, Loss: 0.1251\n",
      "Epoch: 4, Index: 5, Loss: 1.5100\n",
      "Epoch: 4, Index: 6, Loss: 2.4424\n",
      "Epoch: 4, Index: 7, Loss: 6.3883\n",
      "Epoch: 4, Index: 8, Loss: 1.7701\n",
      "Epoch: 4, Index: 9, Loss: 2.1457\n",
      "Epoch: 4, Index: 10, Loss: 1.0173\n",
      "Epoch: 4, Index: 11, Loss: 0.5548\n",
      "Epoch: 4, Index: 12, Loss: 2.7623\n",
      "Epoch: 4, Index: 13, Loss: 1.8197\n",
      "Epoch: 4, Index: 14, Loss: 11.8280\n",
      "Epoch: 4, Index: 15, Loss: 0.7392\n",
      "Epoch: 4, Index: 16, Loss: 3.9947\n",
      "Epoch: 4, Index: 17, Loss: 1.0293\n",
      "Epoch: 4, Index: 18, Loss: 0.5185\n",
      "Epoch: 4, Index: 19, Loss: 0.5372\n",
      "Epoch: 4, Index: 20, Loss: 1.7230\n",
      "Epoch: 4, Index: 21, Loss: 2.0194\n",
      "Epoch: 4, Index: 22, Loss: 2.4742\n",
      "Epoch: 4, Index: 23, Loss: 3.8532\n",
      "Epoch: 4, Index: 24, Loss: 2.2944\n",
      "Epoch: 4, Index: 25, Loss: 0.6723\n",
      "Epoch: 4, Index: 26, Loss: 0.0368\n",
      "Epoch: 4, Index: 27, Loss: 1.0914\n",
      "Epoch: 4, Index: 28, Loss: 0.8192\n",
      "Epoch: 4, Index: 29, Loss: 0.0315\n",
      "Epoch: 4, Index: 30, Loss: 1.0014\n",
      "Epoch: 4, Index: 31, Loss: 1.0587\n",
      "Epoch: 4, Index: 32, Loss: 0.2814\n",
      "Epoch: 4, Index: 33, Loss: 5.5232\n",
      "Epoch: 4, Index: 34, Loss: 0.2790\n",
      "Epoch: 4, Index: 35, Loss: 0.3404\n",
      "Epoch: 4, Index: 36, Loss: 7.7180\n",
      "Epoch: 4, Index: 37, Loss: 0.4968\n",
      "Epoch: 4, Index: 38, Loss: 3.3922\n",
      "Epoch: 4, Index: 39, Loss: 3.1131\n",
      "Epoch: 4, Index: 40, Loss: 2.9612\n",
      "Epoch: 4, Index: 41, Loss: 1.0219\n",
      "Epoch: 4, Index: 42, Loss: 0.9501\n",
      "Epoch: 4, Index: 43, Loss: 0.1755\n",
      "Epoch: 4, Index: 44, Loss: 0.8333\n",
      "Epoch: 4, Index: 45, Loss: 0.4980\n",
      "Epoch: 4, Index: 46, Loss: 0.1385\n",
      "Epoch: 4, Index: 47, Loss: 6.8135\n",
      "Epoch: 4, Index: 48, Loss: 3.0722\n",
      "Epoch: 4, Index: 49, Loss: 0.7356\n",
      "Epoch: 4, Index: 50, Loss: 0.1352\n",
      "Epoch: 4, Index: 51, Loss: 1.4767\n",
      "Epoch: 4, Index: 52, Loss: 1.1535\n",
      "Epoch: 4, Index: 53, Loss: 5.0431\n",
      "Epoch: 4, Index: 54, Loss: 2.6761\n",
      "Epoch: 4, Index: 55, Loss: 4.5340\n",
      "Epoch: 4, Index: 56, Loss: 3.2376\n",
      "Epoch: 4, Index: 57, Loss: 2.4829\n",
      "Epoch: 4, Index: 58, Loss: 2.0030\n",
      "Epoch: 4, Index: 59, Loss: 0.5586\n",
      "Epoch: 4, Index: 60, Loss: 5.0462\n",
      "Epoch: 4, Index: 61, Loss: 5.9110\n",
      "Epoch: 4, Index: 62, Loss: 0.4176\n",
      "Epoch: 4, Index: 63, Loss: 0.8664\n",
      "Epoch: 4, Index: 64, Loss: 1.0725\n",
      "Epoch: 4, Index: 65, Loss: 0.1944\n",
      "Epoch: 4, Index: 66, Loss: 1.7629\n",
      "Epoch: 4, Index: 67, Loss: 0.3834\n",
      "Epoch: 4, Index: 68, Loss: 0.2123\n",
      "Epoch: 4, Index: 69, Loss: 0.3539\n",
      "Epoch: 4, Index: 70, Loss: 1.8386\n",
      "Epoch: 4, Index: 71, Loss: 2.9558\n",
      "Epoch: 4, Index: 72, Loss: 1.9228\n",
      "Epoch: 4, Index: 73, Loss: 1.4267\n",
      "Epoch: 4, Index: 74, Loss: 0.6194\n",
      "Epoch: 4, Index: 75, Loss: 2.0166\n",
      "Epoch: 4, Index: 76, Loss: 2.8405\n",
      "Epoch: 4, Index: 77, Loss: 2.1558\n",
      "Epoch: 4, Index: 78, Loss: 3.3143\n",
      "Epoch: 4, Index: 79, Loss: 2.3188\n",
      "Epoch: 4, Index: 80, Loss: 0.8636\n",
      "Epoch: 4, Index: 81, Loss: 0.0898\n",
      "Epoch: 4, Index: 82, Loss: 1.2613\n",
      "Epoch: 4, Index: 83, Loss: 1.8646\n",
      "Epoch: 4, Index: 84, Loss: 5.9523\n",
      "Epoch: 4, Index: 85, Loss: 0.7030\n",
      "Epoch: 4, Index: 86, Loss: 3.7337\n",
      "Epoch: 4, Index: 87, Loss: 0.6532\n",
      "Epoch: 4, Index: 88, Loss: 0.8889\n",
      "Epoch: 4, Index: 89, Loss: 2.1095\n",
      "Epoch: 4, Index: 90, Loss: 0.3811\n",
      "Epoch: 4, Index: 91, Loss: 0.5917\n",
      "Epoch: 4, Index: 92, Loss: 0.8911\n",
      "Epoch: 4, Index: 93, Loss: 6.6484\n",
      "Epoch: 4, Index: 94, Loss: 9.5787\n",
      "Epoch: 4, Index: 95, Loss: 3.2238\n",
      "Epoch: 4, Index: 96, Loss: 0.5461\n",
      "Epoch: 4, Index: 97, Loss: 4.4412\n",
      "Epoch: 4, Index: 98, Loss: 2.3996\n",
      "Epoch: 4, Index: 99, Loss: 2.9323\n",
      "Epoch: 4, Index: 100, Loss: 1.4802\n",
      "Epoch: 4, Index: 101, Loss: 0.5041\n",
      "Epoch: 4, Index: 102, Loss: 0.0300\n",
      "Epoch: 4, Index: 103, Loss: 0.6623\n",
      "Epoch: 4, Index: 104, Loss: 0.1113\n",
      "Epoch: 4, Index: 105, Loss: 0.4240\n",
      "Epoch: 4, Index: 106, Loss: 1.7676\n",
      "Epoch: 4, Index: 107, Loss: 0.0896\n",
      "Epoch: 4, Index: 108, Loss: 6.1720\n",
      "Epoch: 4, Index: 109, Loss: 1.8414\n",
      "Epoch: 4, Index: 110, Loss: 5.4563\n",
      "Epoch: 4, Index: 111, Loss: 0.1243\n",
      "Epoch: 4, Index: 112, Loss: 0.4231\n",
      "Epoch: 4, Index: 113, Loss: 0.1889\n",
      "Epoch: 4, Index: 114, Loss: 3.5244\n",
      "Epoch: 4, Index: 115, Loss: 1.1625\n",
      "Epoch: 4, Index: 116, Loss: 0.1896\n",
      "Epoch: 4, Index: 117, Loss: 2.1552\n",
      "Epoch: 4, Index: 118, Loss: 2.7874\n",
      "Epoch: 4, Index: 119, Loss: 0.7350\n",
      "Epoch: 4, Index: 120, Loss: 1.0130\n",
      "Epoch: 4, Index: 121, Loss: 3.2668\n",
      "Epoch: 4, Index: 122, Loss: 0.2508\n",
      "Epoch: 4, Index: 123, Loss: 2.1917\n",
      "Epoch: 4, Index: 124, Loss: 0.5029\n",
      "Epoch: 4, Index: 125, Loss: 0.6141\n",
      "Epoch: 4, Index: 126, Loss: 0.0424\n",
      "Epoch: 4, Index: 127, Loss: 0.9613\n",
      "Epoch: 4, Index: 128, Loss: 0.8075\n",
      "Epoch: 4, Index: 129, Loss: 3.0244\n",
      "Epoch: 4, Index: 130, Loss: 2.6767\n",
      "Epoch: 4, Index: 131, Loss: 0.9778\n",
      "Epoch: 4, Index: 132, Loss: 0.6034\n",
      "Epoch: 4, Index: 133, Loss: 4.1651\n",
      "Epoch: 4, Index: 134, Loss: 0.3205\n",
      "Epoch: 4, Index: 135, Loss: 0.2901\n",
      "Epoch: 4, Index: 136, Loss: 3.1221\n",
      "Epoch: 4, Index: 137, Loss: 0.0711\n",
      "Epoch: 4, Index: 138, Loss: 2.5576\n",
      "Epoch: 4, Index: 139, Loss: 2.0721\n",
      "Epoch: 4, Index: 140, Loss: 1.7965\n",
      "Epoch: 4, Index: 141, Loss: 0.7987\n",
      "Epoch: 4, Index: 142, Loss: 2.0099\n",
      "Epoch: 4, Index: 143, Loss: 0.5814\n",
      "Epoch: 4, Index: 144, Loss: 1.3825\n",
      "Epoch: 4, Index: 145, Loss: 2.0647\n",
      "Epoch: 4, Index: 146, Loss: 0.7285\n",
      "Epoch: 4, Index: 147, Loss: 1.4869\n",
      "Epoch: 4, Index: 148, Loss: 1.8665\n",
      "Epoch: 4, Index: 149, Loss: 1.5347\n",
      "Epoch: 4, Index: 150, Loss: 1.8752\n",
      "Epoch: 4, Index: 151, Loss: 6.6157\n",
      "Epoch: 4, Index: 152, Loss: 2.8698\n",
      "Epoch: 4, Index: 153, Loss: 3.2844\n",
      "Epoch: 4, Index: 154, Loss: 0.4844\n",
      "Epoch: 4, Index: 155, Loss: 3.0061\n",
      "Epoch: 4, Index: 156, Loss: 1.4785\n",
      "Epoch: 4, Index: 157, Loss: 2.0531\n",
      "Epoch: 4, Index: 158, Loss: 0.0768\n",
      "Epoch: 4, Index: 159, Loss: 1.0611\n",
      "Epoch: 4, Index: 160, Loss: 0.8053\n",
      "Epoch: 4, Index: 161, Loss: 1.0958\n",
      "Epoch: 4, Index: 162, Loss: 0.9134\n",
      "Epoch: 4, Index: 163, Loss: 0.0703\n",
      "Epoch: 4, Index: 164, Loss: 21.6935\n",
      "Epoch: 4, Index: 165, Loss: 2.6031\n",
      "Epoch: 4, Index: 166, Loss: 1.6138\n",
      "Epoch: 4, Index: 167, Loss: 0.1329\n",
      "Epoch: 4, Index: 168, Loss: 5.3633\n",
      "Epoch: 4, Index: 169, Loss: 0.7355\n",
      "Epoch: 4, Index: 170, Loss: 0.4779\n",
      "Epoch: 4, Index: 171, Loss: 0.4364\n",
      "Epoch: 4, Index: 172, Loss: 3.6220\n",
      "Epoch: 4, Index: 173, Loss: 2.1981\n",
      "Epoch: 4, Index: 174, Loss: 1.9233\n",
      "Epoch: 4, Index: 175, Loss: 0.0706\n",
      "Epoch: 4, Index: 176, Loss: 2.0630\n",
      "Epoch: 4, Index: 177, Loss: 0.9659\n",
      "Epoch: 4, Index: 178, Loss: 15.6704\n",
      "Epoch: 4, Index: 179, Loss: 0.1971\n",
      "Epoch: 4, Index: 180, Loss: 1.2986\n",
      "Epoch: 4, Index: 181, Loss: 6.2416\n",
      "Epoch: 4, Index: 182, Loss: 0.1610\n",
      "Epoch: 4, Index: 183, Loss: 0.3293\n",
      "Epoch: 4, Index: 184, Loss: 1.6848\n",
      "Epoch: 4, Index: 185, Loss: 0.7657\n",
      "Epoch: 4, Index: 186, Loss: 0.0738\n",
      "Epoch: 4, Index: 187, Loss: 0.4147\n",
      "Epoch: 4, Index: 188, Loss: 0.3814\n",
      "Epoch: 4, Index: 189, Loss: 2.0467\n",
      "Epoch: 4, Index: 190, Loss: 1.9522\n",
      "Epoch: 4, Index: 191, Loss: 1.4444\n",
      "Epoch: 4, Index: 192, Loss: 0.6211\n",
      "Epoch: 4, Index: 193, Loss: 5.3217\n",
      "Epoch: 4, Index: 194, Loss: 0.6026\n",
      "Epoch: 4, Index: 195, Loss: 1.2622\n",
      "Epoch: 4, Index: 196, Loss: 0.1911\n",
      "Epoch: 4, Index: 197, Loss: 0.0203\n",
      "Epoch: 4, Index: 198, Loss: 0.1477\n",
      "Epoch: 4, Index: 199, Loss: 1.0967\n",
      "Epoch: 4, Index: 200, Loss: 3.4157\n",
      "Epoch: 4, Index: 201, Loss: 1.5977\n",
      "Epoch: 4, Index: 202, Loss: 0.4137\n",
      "Epoch: 4, Index: 203, Loss: 5.0243\n",
      "Epoch: 4, Index: 204, Loss: 0.0149\n",
      "Epoch: 4, Index: 205, Loss: 0.4801\n",
      "Epoch: 4, Index: 206, Loss: 1.3635\n",
      "Epoch: 4, Index: 207, Loss: 4.1106\n",
      "Epoch: 4, Index: 208, Loss: 0.3279\n",
      "Epoch: 4, Index: 209, Loss: 0.2995\n",
      "Epoch: 4, Index: 210, Loss: 0.7510\n",
      "Epoch: 4, Index: 211, Loss: 3.2594\n",
      "Epoch: 4, Index: 212, Loss: 0.4587\n",
      "Epoch: 4, Index: 213, Loss: 1.2023\n",
      "Epoch: 4, Index: 214, Loss: 3.2258\n",
      "Epoch: 4, Index: 215, Loss: 0.0508\n",
      "Epoch: 4, Index: 216, Loss: 0.0970\n",
      "Epoch: 4, Index: 217, Loss: 0.8564\n",
      "Epoch: 4, Index: 218, Loss: 1.0799\n",
      "Epoch: 4, Index: 219, Loss: 0.6825\n",
      "Epoch: 4, Index: 220, Loss: 0.1222\n",
      "Epoch: 4, Index: 221, Loss: 1.4299\n",
      "Epoch: 4, Index: 222, Loss: 0.6301\n",
      "Epoch: 4, Index: 223, Loss: 5.5745\n",
      "Epoch: 4, Index: 224, Loss: 1.4149\n",
      "Epoch: 4, Index: 225, Loss: 0.7373\n",
      "Epoch: 4, Index: 226, Loss: 2.3633\n",
      "Epoch: 4, Index: 227, Loss: 1.3279\n",
      "Epoch: 4, Index: 228, Loss: 0.6320\n",
      "Epoch: 4, Index: 229, Loss: 1.8510\n",
      "Epoch: 4, Index: 230, Loss: 0.6031\n",
      "Epoch: 4, Index: 231, Loss: 2.5406\n",
      "Epoch: 4, Index: 232, Loss: 4.8029\n",
      "Epoch: 4, Index: 233, Loss: 2.2668\n",
      "Epoch: 4, Index: 234, Loss: 10.1763\n",
      "Epoch: 4, Index: 235, Loss: 0.0178\n",
      "Epoch: 4, Index: 236, Loss: 0.7427\n",
      "Epoch: 4, Index: 237, Loss: 2.7046\n",
      "Epoch: 4, Index: 238, Loss: 0.0372\n",
      "Epoch: 4, Index: 239, Loss: 2.3412\n",
      "Epoch: 4, Index: 240, Loss: 2.1007\n",
      "Epoch: 4, Index: 241, Loss: 0.1583\n",
      "Epoch: 4, Index: 242, Loss: 2.5988\n",
      "Epoch: 4, Index: 243, Loss: 0.7086\n",
      "Epoch: 4, Index: 244, Loss: 0.2217\n",
      "Epoch: 4, Index: 245, Loss: 6.0709\n",
      "Epoch: 4, Index: 246, Loss: 0.8534\n",
      "Epoch: 4, Index: 247, Loss: 4.6692\n",
      "Epoch: 4, Index: 248, Loss: 0.5949\n",
      "Epoch: 4, Index: 249, Loss: 2.0115\n",
      "Epoch: 4, Index: 250, Loss: 0.0960\n",
      "Epoch: 4, Index: 251, Loss: 10.7095\n",
      "Epoch: 4, Index: 252, Loss: 3.0107\n",
      "Epoch: 4, Index: 253, Loss: 1.8447\n",
      "Epoch: 4, Index: 254, Loss: 4.1316\n",
      "Epoch: 4, Index: 255, Loss: 2.5323\n",
      "Epoch: 4, Index: 256, Loss: 2.6666\n",
      "Epoch: 4, Index: 257, Loss: 1.7688\n",
      "Epoch: 4, Index: 258, Loss: 1.2089\n",
      "Epoch: 4, Index: 259, Loss: 0.6767\n",
      "Epoch: 4, Index: 260, Loss: 3.3682\n",
      "Epoch: 4, Index: 261, Loss: 1.0342\n",
      "Epoch: 4, Index: 262, Loss: 0.0733\n",
      "Epoch: 4, Index: 263, Loss: 4.5473\n",
      "Epoch: 4, Index: 264, Loss: 1.3926\n",
      "Epoch: 4, Index: 265, Loss: 0.7304\n",
      "Epoch: 4, Index: 266, Loss: 2.0866\n",
      "Epoch: 4, Index: 267, Loss: 1.1609\n",
      "Epoch: 4, Index: 268, Loss: 6.7499\n",
      "Epoch: 4, Index: 269, Loss: 1.0743\n",
      "Epoch: 4, Index: 270, Loss: 2.5080\n",
      "Epoch: 4, Index: 271, Loss: 0.5708\n",
      "Epoch: 4, Index: 272, Loss: 5.0751\n",
      "Epoch: 4, Index: 273, Loss: 0.6993\n",
      "Epoch: 4, Index: 274, Loss: 0.8514\n",
      "Epoch: 4, Index: 275, Loss: 0.7387\n",
      "Epoch: 4, Index: 276, Loss: 4.0968\n",
      "Epoch: 4, Index: 277, Loss: 2.6289\n",
      "Epoch: 4, Index: 278, Loss: 0.4743\n",
      "Epoch: 4, Index: 279, Loss: 1.9638\n",
      "Epoch: 4, Index: 280, Loss: 0.7417\n",
      "Epoch: 4, Index: 281, Loss: 0.6701\n",
      "Epoch: 4, Index: 282, Loss: 0.2597\n",
      "Epoch: 4, Index: 283, Loss: 1.3478\n",
      "Epoch: 4, Index: 284, Loss: 0.4090\n",
      "Epoch: 4, Index: 285, Loss: 0.0136\n",
      "Epoch: 4, Index: 286, Loss: 6.5970\n",
      "Epoch: 4, Index: 287, Loss: 1.0091\n",
      "Epoch: 4, Index: 288, Loss: 1.8537\n",
      "Epoch: 4, Index: 289, Loss: 1.6683\n",
      "Epoch: 4, Index: 290, Loss: 2.5578\n",
      "Epoch: 4, Index: 291, Loss: 0.2878\n",
      "Epoch: 4, Index: 292, Loss: 1.3916\n",
      "Epoch: 4, Index: 293, Loss: 2.0978\n",
      "Epoch: 4, Index: 294, Loss: 0.5971\n",
      "Epoch: 4, Index: 295, Loss: 2.4612\n",
      "Epoch: 4, Index: 296, Loss: 0.6572\n",
      "Epoch: 4, Index: 297, Loss: 3.1539\n",
      "Epoch: 4, Index: 298, Loss: 0.0822\n",
      "Epoch: 4, Index: 299, Loss: 3.7105\n",
      "Epoch: 4, Index: 300, Loss: 0.3110\n",
      "Epoch: 4, Index: 301, Loss: 0.1603\n",
      "Epoch: 4, Index: 302, Loss: 0.1380\n",
      "Epoch: 4, Index: 303, Loss: 1.3706\n",
      "Epoch: 4, Index: 304, Loss: 0.0878\n",
      "Epoch: 4, Index: 305, Loss: 1.3716\n",
      "Epoch: 4, Index: 306, Loss: 2.0821\n",
      "Epoch: 4, Index: 307, Loss: 0.6456\n",
      "Epoch: 4, Index: 308, Loss: 0.6795\n",
      "Epoch: 4, Index: 309, Loss: 0.3245\n",
      "Epoch: 4, Index: 310, Loss: 0.2636\n",
      "Epoch: 4, Index: 311, Loss: 1.5753\n",
      "Epoch: 4, Index: 312, Loss: 0.6867\n",
      "Epoch: 4, Index: 313, Loss: 0.0541\n",
      "Epoch: 4, Index: 314, Loss: 0.5826\n",
      "Epoch: 4, Index: 315, Loss: 0.7473\n",
      "Epoch: 4, Index: 316, Loss: 2.7447\n",
      "Epoch: 4, Index: 317, Loss: 0.6369\n",
      "Epoch: 4, Index: 318, Loss: 0.4530\n",
      "Epoch: 4, Index: 319, Loss: 0.9083\n",
      "Epoch: 4, Index: 320, Loss: 11.2667\n",
      "Epoch: 4, Index: 321, Loss: 0.7078\n",
      "Epoch: 4, Index: 322, Loss: 0.7000\n",
      "Epoch: 4, Index: 323, Loss: 0.0507\n",
      "Epoch: 4, Index: 324, Loss: 0.3203\n",
      "Epoch: 4, Index: 325, Loss: 2.4334\n",
      "Epoch: 4, Index: 326, Loss: 0.9901\n",
      "Epoch: 4, Index: 327, Loss: 1.9231\n",
      "Epoch: 4, Index: 328, Loss: 0.0612\n",
      "Epoch: 4, Index: 329, Loss: 4.5386\n",
      "Epoch: 4, Index: 330, Loss: 0.9111\n",
      "Epoch: 4, Index: 331, Loss: 2.7195\n",
      "Epoch: 4, Index: 332, Loss: 0.9088\n",
      "Epoch: 4, Index: 333, Loss: 1.8928\n",
      "Epoch: 4, Index: 334, Loss: 1.2371\n",
      "Epoch: 4, Index: 335, Loss: 0.4016\n",
      "Epoch: 4, Index: 336, Loss: 12.8148\n",
      "Epoch: 4, Index: 337, Loss: 3.4841\n",
      "Epoch: 4, Index: 338, Loss: 3.3892\n",
      "Epoch: 4, Index: 339, Loss: 4.3758\n",
      "Epoch: 4, Index: 340, Loss: 0.4783\n",
      "Epoch: 4, Index: 341, Loss: 0.6560\n",
      "Epoch: 4, Index: 342, Loss: 0.0944\n",
      "Epoch: 4, Index: 343, Loss: 0.2268\n",
      "Epoch: 4, Index: 344, Loss: 1.9903\n",
      "Epoch: 4, Index: 345, Loss: 1.8336\n",
      "Epoch: 4, Index: 346, Loss: 0.8835\n",
      "Epoch: 4, Index: 347, Loss: 1.2113\n",
      "Epoch: 4, Index: 348, Loss: 3.9461\n",
      "Epoch: 4, Index: 349, Loss: 3.1421\n",
      "Epoch: 4, Index: 350, Loss: 2.4569\n",
      "Epoch: 4, Index: 351, Loss: 0.3400\n",
      "Epoch: 4, Index: 352, Loss: 1.5027\n",
      "Epoch: 4, Index: 353, Loss: 0.5244\n",
      "Epoch: 4, Index: 354, Loss: 4.1943\n",
      "Epoch: 4, Index: 355, Loss: 0.2370\n",
      "Epoch: 4, Index: 356, Loss: 6.7147\n",
      "Epoch: 4, Index: 357, Loss: 0.7991\n",
      "Epoch: 4, Index: 358, Loss: 1.5044\n",
      "Epoch: 4, Index: 359, Loss: 5.2743\n",
      "Epoch: 4, Index: 360, Loss: 0.9629\n",
      "Epoch: 4, Index: 361, Loss: 4.3518\n",
      "Epoch: 4, Index: 362, Loss: 7.0415\n",
      "Epoch: 4, Index: 363, Loss: 0.4927\n",
      "Epoch: 4, Index: 364, Loss: 1.4196\n",
      "Epoch: 4, Index: 365, Loss: 1.0403\n",
      "Epoch: 4, Index: 366, Loss: 1.4386\n",
      "Epoch: 4, Index: 367, Loss: 3.8059\n",
      "Epoch: 4, Index: 368, Loss: 8.9190\n",
      "Epoch: 4, Index: 369, Loss: 4.2721\n",
      "Epoch: 4, Index: 370, Loss: 0.2657\n",
      "Epoch: 4, Index: 371, Loss: 0.2483\n",
      "Epoch: 4, Index: 372, Loss: 0.1723\n",
      "Epoch: 4, Index: 373, Loss: 2.0068\n",
      "Epoch: 4, Index: 374, Loss: 0.6851\n",
      "Epoch: 4, Index: 375, Loss: 0.3357\n",
      "Epoch: 4, Index: 376, Loss: 0.2397\n",
      "Epoch: 4, Index: 377, Loss: 0.1801\n",
      "Epoch: 4, Index: 378, Loss: 0.6993\n",
      "Epoch: 4, Index: 379, Loss: 2.0790\n",
      "Epoch: 4, Index: 380, Loss: 1.8173\n",
      "Epoch: 4, Index: 381, Loss: 1.3985\n",
      "Epoch: 4, Index: 382, Loss: 0.2512\n",
      "Epoch: 4, Index: 383, Loss: 0.7860\n",
      "Epoch: 4, Index: 384, Loss: 0.1594\n",
      "Epoch: 4, Index: 385, Loss: 0.7770\n",
      "Epoch: 4, Index: 386, Loss: 0.3290\n",
      "Epoch: 4, Index: 387, Loss: 0.0932\n",
      "Epoch: 4, Index: 388, Loss: 2.5305\n",
      "Epoch: 4, Index: 389, Loss: 2.3677\n",
      "Epoch: 4, Index: 390, Loss: 0.2524\n",
      "Epoch: 4, Index: 391, Loss: 2.9973\n",
      "Epoch: 4, Index: 392, Loss: 1.9909\n",
      "Epoch: 4, Index: 393, Loss: 1.3776\n",
      "Epoch: 4, Index: 394, Loss: 3.6901\n",
      "Epoch: 4, Index: 395, Loss: 7.1379\n",
      "Epoch: 4, Index: 396, Loss: 1.6416\n",
      "Epoch: 4, Index: 397, Loss: 0.5478\n",
      "Epoch: 4, Index: 398, Loss: 1.4810\n",
      "Epoch: 4, Index: 399, Loss: 2.1881\n",
      "Epoch: 4, Index: 400, Loss: 1.1500\n",
      "Epoch: 4, Index: 401, Loss: 4.6448\n",
      "Epoch: 4, Index: 402, Loss: 2.9884\n",
      "Epoch: 4, Index: 403, Loss: 5.3494\n",
      "Epoch: 4, Index: 404, Loss: 3.4792\n",
      "Epoch: 4, Index: 405, Loss: 0.3665\n",
      "Epoch: 4, Index: 406, Loss: 3.1031\n",
      "Epoch: 4, Index: 407, Loss: 0.0771\n",
      "Epoch: 4, Index: 408, Loss: 2.0696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260dc1f95c4244a98a6fc36d3e8661ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Index: 0, Loss: 4.5111\n",
      "Epoch: 5, Index: 1, Loss: 1.4956\n",
      "Epoch: 5, Index: 2, Loss: 5.7775\n",
      "Epoch: 5, Index: 3, Loss: 1.3008\n",
      "Epoch: 5, Index: 4, Loss: 9.1702\n",
      "Epoch: 5, Index: 5, Loss: 0.5979\n",
      "Epoch: 5, Index: 6, Loss: 4.7512\n",
      "Epoch: 5, Index: 7, Loss: 5.7540\n",
      "Epoch: 5, Index: 8, Loss: 4.9106\n",
      "Epoch: 5, Index: 9, Loss: 3.0865\n",
      "Epoch: 5, Index: 10, Loss: 0.2629\n",
      "Epoch: 5, Index: 11, Loss: 1.7141\n",
      "Epoch: 5, Index: 12, Loss: 0.5791\n",
      "Epoch: 5, Index: 13, Loss: 0.9996\n",
      "Epoch: 5, Index: 14, Loss: 0.4026\n",
      "Epoch: 5, Index: 15, Loss: 0.2199\n",
      "Epoch: 5, Index: 16, Loss: 0.5969\n",
      "Epoch: 5, Index: 17, Loss: 0.8531\n",
      "Epoch: 5, Index: 18, Loss: 0.5078\n",
      "Epoch: 5, Index: 19, Loss: 0.4578\n",
      "Epoch: 5, Index: 20, Loss: 0.4419\n",
      "Epoch: 5, Index: 21, Loss: 16.8585\n",
      "Epoch: 5, Index: 22, Loss: 0.4685\n",
      "Epoch: 5, Index: 23, Loss: 1.6969\n",
      "Epoch: 5, Index: 24, Loss: 0.2862\n",
      "Epoch: 5, Index: 25, Loss: 6.7448\n",
      "Epoch: 5, Index: 26, Loss: 0.8284\n",
      "Epoch: 5, Index: 27, Loss: 1.8576\n",
      "Epoch: 5, Index: 28, Loss: 2.2870\n",
      "Epoch: 5, Index: 29, Loss: 1.7211\n",
      "Epoch: 5, Index: 30, Loss: 1.6854\n",
      "Epoch: 5, Index: 31, Loss: 1.4812\n",
      "Epoch: 5, Index: 32, Loss: 0.7990\n",
      "Epoch: 5, Index: 33, Loss: 0.0638\n",
      "Epoch: 5, Index: 34, Loss: 0.6549\n",
      "Epoch: 5, Index: 35, Loss: 3.1116\n",
      "Epoch: 5, Index: 36, Loss: 1.7855\n",
      "Epoch: 5, Index: 37, Loss: 0.8657\n",
      "Epoch: 5, Index: 38, Loss: 0.7031\n",
      "Epoch: 5, Index: 39, Loss: 0.0690\n",
      "Epoch: 5, Index: 40, Loss: 5.4780\n",
      "Epoch: 5, Index: 41, Loss: 0.0730\n",
      "Epoch: 5, Index: 42, Loss: 1.3592\n",
      "Epoch: 5, Index: 43, Loss: 2.5651\n",
      "Epoch: 5, Index: 44, Loss: 1.4274\n",
      "Epoch: 5, Index: 45, Loss: 0.4027\n",
      "Epoch: 5, Index: 46, Loss: 3.8763\n",
      "Epoch: 5, Index: 47, Loss: 1.2214\n",
      "Epoch: 5, Index: 48, Loss: 1.6463\n",
      "Epoch: 5, Index: 49, Loss: 0.6769\n",
      "Epoch: 5, Index: 50, Loss: 2.4134\n",
      "Epoch: 5, Index: 51, Loss: 0.3360\n",
      "Epoch: 5, Index: 52, Loss: 1.6791\n",
      "Epoch: 5, Index: 53, Loss: 3.0775\n",
      "Epoch: 5, Index: 54, Loss: 0.8912\n",
      "Epoch: 5, Index: 55, Loss: 3.3920\n",
      "Epoch: 5, Index: 56, Loss: 2.4515\n",
      "Epoch: 5, Index: 57, Loss: 0.9477\n",
      "Epoch: 5, Index: 58, Loss: 0.0050\n",
      "Epoch: 5, Index: 59, Loss: 1.2816\n",
      "Epoch: 5, Index: 60, Loss: 1.6079\n",
      "Epoch: 5, Index: 61, Loss: 0.3392\n",
      "Epoch: 5, Index: 62, Loss: 0.0246\n",
      "Epoch: 5, Index: 63, Loss: 1.0189\n",
      "Epoch: 5, Index: 64, Loss: 1.5735\n",
      "Epoch: 5, Index: 65, Loss: 0.8360\n",
      "Epoch: 5, Index: 66, Loss: 0.8912\n",
      "Epoch: 5, Index: 67, Loss: 0.6430\n",
      "Epoch: 5, Index: 68, Loss: 0.2022\n",
      "Epoch: 5, Index: 69, Loss: 1.6807\n",
      "Epoch: 5, Index: 70, Loss: 3.6094\n",
      "Epoch: 5, Index: 71, Loss: 7.3001\n",
      "Epoch: 5, Index: 72, Loss: 0.0179\n",
      "Epoch: 5, Index: 73, Loss: 2.8223\n",
      "Epoch: 5, Index: 74, Loss: 2.8858\n",
      "Epoch: 5, Index: 75, Loss: 2.4110\n",
      "Epoch: 5, Index: 76, Loss: 3.7561\n",
      "Epoch: 5, Index: 77, Loss: 0.9648\n",
      "Epoch: 5, Index: 78, Loss: 3.9976\n",
      "Epoch: 5, Index: 79, Loss: 0.2880\n",
      "Epoch: 5, Index: 80, Loss: 0.9473\n",
      "Epoch: 5, Index: 81, Loss: 5.1209\n",
      "Epoch: 5, Index: 82, Loss: 3.2410\n",
      "Epoch: 5, Index: 83, Loss: 3.3086\n",
      "Epoch: 5, Index: 84, Loss: 2.2066\n",
      "Epoch: 5, Index: 85, Loss: 0.4234\n",
      "Epoch: 5, Index: 86, Loss: 3.2275\n",
      "Epoch: 5, Index: 87, Loss: 0.0513\n",
      "Epoch: 5, Index: 88, Loss: 0.3072\n",
      "Epoch: 5, Index: 89, Loss: 1.0336\n",
      "Epoch: 5, Index: 90, Loss: 4.5560\n",
      "Epoch: 5, Index: 91, Loss: 0.0007\n",
      "Epoch: 5, Index: 92, Loss: 0.4092\n",
      "Epoch: 5, Index: 93, Loss: 1.2136\n",
      "Epoch: 5, Index: 94, Loss: 2.9782\n",
      "Epoch: 5, Index: 95, Loss: 1.9931\n",
      "Epoch: 5, Index: 96, Loss: 3.6048\n",
      "Epoch: 5, Index: 97, Loss: 1.4353\n",
      "Epoch: 5, Index: 98, Loss: 0.5901\n",
      "Epoch: 5, Index: 99, Loss: 0.0492\n",
      "Epoch: 5, Index: 100, Loss: 0.8590\n",
      "Epoch: 5, Index: 101, Loss: 2.7918\n",
      "Epoch: 5, Index: 102, Loss: 2.4697\n",
      "Epoch: 5, Index: 103, Loss: 0.8850\n",
      "Epoch: 5, Index: 104, Loss: 0.8431\n",
      "Epoch: 5, Index: 105, Loss: 1.9363\n",
      "Epoch: 5, Index: 106, Loss: 0.1337\n",
      "Epoch: 5, Index: 107, Loss: 9.3804\n",
      "Epoch: 5, Index: 108, Loss: 0.0355\n",
      "Epoch: 5, Index: 109, Loss: 2.1138\n",
      "Epoch: 5, Index: 110, Loss: 0.8008\n",
      "Epoch: 5, Index: 111, Loss: 2.5525\n",
      "Epoch: 5, Index: 112, Loss: 2.6158\n",
      "Epoch: 5, Index: 113, Loss: 0.4015\n",
      "Epoch: 5, Index: 114, Loss: 1.5422\n",
      "Epoch: 5, Index: 115, Loss: 0.2049\n",
      "Epoch: 5, Index: 116, Loss: 0.5514\n",
      "Epoch: 5, Index: 117, Loss: 2.3282\n",
      "Epoch: 5, Index: 118, Loss: 1.3316\n",
      "Epoch: 5, Index: 119, Loss: 2.6958\n",
      "Epoch: 5, Index: 120, Loss: 1.6914\n",
      "Epoch: 5, Index: 121, Loss: 0.8031\n",
      "Epoch: 5, Index: 122, Loss: 1.2950\n",
      "Epoch: 5, Index: 123, Loss: 1.0050\n",
      "Epoch: 5, Index: 124, Loss: 0.3506\n",
      "Epoch: 5, Index: 125, Loss: 0.5767\n",
      "Epoch: 5, Index: 126, Loss: 0.6290\n",
      "Epoch: 5, Index: 127, Loss: 5.5763\n",
      "Epoch: 5, Index: 128, Loss: 0.7932\n",
      "Epoch: 5, Index: 129, Loss: 0.9388\n",
      "Epoch: 5, Index: 130, Loss: 0.1335\n",
      "Epoch: 5, Index: 131, Loss: 0.0382\n",
      "Epoch: 5, Index: 132, Loss: 10.0527\n",
      "Epoch: 5, Index: 133, Loss: 0.5306\n",
      "Epoch: 5, Index: 134, Loss: 3.2374\n",
      "Epoch: 5, Index: 135, Loss: 0.0673\n",
      "Epoch: 5, Index: 136, Loss: 0.1884\n",
      "Epoch: 5, Index: 137, Loss: 3.6320\n",
      "Epoch: 5, Index: 138, Loss: 2.2096\n",
      "Epoch: 5, Index: 139, Loss: 6.5083\n",
      "Epoch: 5, Index: 140, Loss: 2.1973\n",
      "Epoch: 5, Index: 141, Loss: 0.4461\n",
      "Epoch: 5, Index: 142, Loss: 1.3679\n",
      "Epoch: 5, Index: 143, Loss: 1.7148\n",
      "Epoch: 5, Index: 144, Loss: 4.1590\n",
      "Epoch: 5, Index: 145, Loss: 0.5408\n",
      "Epoch: 5, Index: 146, Loss: 1.2817\n",
      "Epoch: 5, Index: 147, Loss: 0.6832\n",
      "Epoch: 5, Index: 148, Loss: 1.0429\n",
      "Epoch: 5, Index: 149, Loss: 11.3875\n",
      "Epoch: 5, Index: 150, Loss: 3.5833\n",
      "Epoch: 5, Index: 151, Loss: 4.7532\n",
      "Epoch: 5, Index: 152, Loss: 0.1461\n",
      "Epoch: 5, Index: 153, Loss: 0.0008\n",
      "Epoch: 5, Index: 154, Loss: 1.2514\n",
      "Epoch: 5, Index: 155, Loss: 0.6435\n",
      "Epoch: 5, Index: 156, Loss: 4.5593\n",
      "Epoch: 5, Index: 157, Loss: 2.0846\n",
      "Epoch: 5, Index: 158, Loss: 1.0368\n",
      "Epoch: 5, Index: 159, Loss: 1.0782\n",
      "Epoch: 5, Index: 160, Loss: 0.4748\n",
      "Epoch: 5, Index: 161, Loss: 0.0090\n",
      "Epoch: 5, Index: 162, Loss: 0.7493\n",
      "Epoch: 5, Index: 163, Loss: 0.9776\n",
      "Epoch: 5, Index: 164, Loss: 6.5066\n",
      "Epoch: 5, Index: 165, Loss: 1.6113\n",
      "Epoch: 5, Index: 166, Loss: 17.9011\n",
      "Epoch: 5, Index: 167, Loss: 5.3217\n",
      "Epoch: 5, Index: 168, Loss: 4.4457\n",
      "Epoch: 5, Index: 169, Loss: 3.2783\n",
      "Epoch: 5, Index: 170, Loss: 0.1801\n",
      "Epoch: 5, Index: 171, Loss: 1.6772\n",
      "Epoch: 5, Index: 172, Loss: 0.6630\n",
      "Epoch: 5, Index: 173, Loss: 2.3214\n",
      "Epoch: 5, Index: 174, Loss: 5.3632\n",
      "Epoch: 5, Index: 175, Loss: 0.0690\n",
      "Epoch: 5, Index: 176, Loss: 2.7979\n",
      "Epoch: 5, Index: 177, Loss: 2.0110\n",
      "Epoch: 5, Index: 178, Loss: 0.4922\n",
      "Epoch: 5, Index: 179, Loss: 2.2072\n",
      "Epoch: 5, Index: 180, Loss: 0.0036\n",
      "Epoch: 5, Index: 181, Loss: 4.6424\n",
      "Epoch: 5, Index: 182, Loss: 0.6866\n",
      "Epoch: 5, Index: 183, Loss: 1.6305\n",
      "Epoch: 5, Index: 184, Loss: 0.3735\n",
      "Epoch: 5, Index: 185, Loss: 1.2523\n",
      "Epoch: 5, Index: 186, Loss: 2.4699\n",
      "Epoch: 5, Index: 187, Loss: 0.5074\n",
      "Epoch: 5, Index: 188, Loss: 0.7101\n",
      "Epoch: 5, Index: 189, Loss: 2.2552\n",
      "Epoch: 5, Index: 190, Loss: 0.9682\n",
      "Epoch: 5, Index: 191, Loss: 0.4914\n",
      "Epoch: 5, Index: 192, Loss: 0.3774\n",
      "Epoch: 5, Index: 193, Loss: 0.1168\n",
      "Epoch: 5, Index: 194, Loss: 6.6488\n",
      "Epoch: 5, Index: 195, Loss: 0.3052\n",
      "Epoch: 5, Index: 196, Loss: 10.2992\n",
      "Epoch: 5, Index: 197, Loss: 0.1088\n",
      "Epoch: 5, Index: 198, Loss: 1.9720\n",
      "Epoch: 5, Index: 199, Loss: 3.4076\n",
      "Epoch: 5, Index: 200, Loss: 0.6282\n",
      "Epoch: 5, Index: 201, Loss: 0.4590\n",
      "Epoch: 5, Index: 202, Loss: 0.9518\n",
      "Epoch: 5, Index: 203, Loss: 0.8260\n",
      "Epoch: 5, Index: 204, Loss: 1.3013\n",
      "Epoch: 5, Index: 205, Loss: 1.0256\n",
      "Epoch: 5, Index: 206, Loss: 1.3417\n",
      "Epoch: 5, Index: 207, Loss: 1.7452\n",
      "Epoch: 5, Index: 208, Loss: 2.0425\n",
      "Epoch: 5, Index: 209, Loss: 1.5250\n",
      "Epoch: 5, Index: 210, Loss: 1.5046\n",
      "Epoch: 5, Index: 211, Loss: 3.1089\n",
      "Epoch: 5, Index: 212, Loss: 0.0547\n",
      "Epoch: 5, Index: 213, Loss: 2.0217\n",
      "Epoch: 5, Index: 214, Loss: 2.5233\n",
      "Epoch: 5, Index: 215, Loss: 1.8928\n",
      "Epoch: 5, Index: 216, Loss: 1.8390\n",
      "Epoch: 5, Index: 217, Loss: 1.5151\n",
      "Epoch: 5, Index: 218, Loss: 1.4518\n",
      "Epoch: 5, Index: 219, Loss: 0.0966\n",
      "Epoch: 5, Index: 220, Loss: 0.5161\n",
      "Epoch: 5, Index: 221, Loss: 4.9448\n",
      "Epoch: 5, Index: 222, Loss: 0.0605\n",
      "Epoch: 5, Index: 223, Loss: 2.5170\n",
      "Epoch: 5, Index: 224, Loss: 2.7789\n",
      "Epoch: 5, Index: 225, Loss: 0.3571\n",
      "Epoch: 5, Index: 226, Loss: 2.8112\n",
      "Epoch: 5, Index: 227, Loss: 4.2300\n",
      "Epoch: 5, Index: 228, Loss: 1.2781\n",
      "Epoch: 5, Index: 229, Loss: 1.5116\n",
      "Epoch: 5, Index: 230, Loss: 3.7687\n",
      "Epoch: 5, Index: 231, Loss: 1.1979\n",
      "Epoch: 5, Index: 232, Loss: 0.4703\n",
      "Epoch: 5, Index: 233, Loss: 6.0602\n",
      "Epoch: 5, Index: 234, Loss: 0.4454\n",
      "Epoch: 5, Index: 235, Loss: 0.8560\n",
      "Epoch: 5, Index: 236, Loss: 0.4459\n",
      "Epoch: 5, Index: 237, Loss: 3.1677\n",
      "Epoch: 5, Index: 238, Loss: 0.7050\n",
      "Epoch: 5, Index: 239, Loss: 0.3400\n",
      "Epoch: 5, Index: 240, Loss: 2.2587\n",
      "Epoch: 5, Index: 241, Loss: 2.5982\n",
      "Epoch: 5, Index: 242, Loss: 0.6941\n",
      "Epoch: 5, Index: 243, Loss: 1.0626\n",
      "Epoch: 5, Index: 244, Loss: 4.7117\n",
      "Epoch: 5, Index: 245, Loss: 1.6358\n",
      "Epoch: 5, Index: 246, Loss: 0.9221\n",
      "Epoch: 5, Index: 247, Loss: 1.5045\n",
      "Epoch: 5, Index: 248, Loss: 5.0817\n",
      "Epoch: 5, Index: 249, Loss: 2.9828\n",
      "Epoch: 5, Index: 250, Loss: 2.7015\n",
      "Epoch: 5, Index: 251, Loss: 0.3489\n",
      "Epoch: 5, Index: 252, Loss: 1.9850\n",
      "Epoch: 5, Index: 253, Loss: 0.7956\n",
      "Epoch: 5, Index: 254, Loss: 1.4812\n",
      "Epoch: 5, Index: 255, Loss: 2.4447\n",
      "Epoch: 5, Index: 256, Loss: 1.0341\n",
      "Epoch: 5, Index: 257, Loss: 0.0131\n",
      "Epoch: 5, Index: 258, Loss: 1.9392\n",
      "Epoch: 5, Index: 259, Loss: 3.0756\n",
      "Epoch: 5, Index: 260, Loss: 0.0687\n",
      "Epoch: 5, Index: 261, Loss: 1.7988\n",
      "Epoch: 5, Index: 262, Loss: 0.1027\n",
      "Epoch: 5, Index: 263, Loss: 0.7462\n",
      "Epoch: 5, Index: 264, Loss: 0.7505\n",
      "Epoch: 5, Index: 265, Loss: 1.4063\n",
      "Epoch: 5, Index: 266, Loss: 0.7432\n",
      "Epoch: 5, Index: 267, Loss: 0.3264\n",
      "Epoch: 5, Index: 268, Loss: 0.3360\n",
      "Epoch: 5, Index: 269, Loss: 2.7244\n",
      "Epoch: 5, Index: 270, Loss: 1.0875\n",
      "Epoch: 5, Index: 271, Loss: 1.4824\n",
      "Epoch: 5, Index: 272, Loss: 0.1498\n",
      "Epoch: 5, Index: 273, Loss: 4.4502\n",
      "Epoch: 5, Index: 274, Loss: 0.4342\n",
      "Epoch: 5, Index: 275, Loss: 5.8234\n",
      "Epoch: 5, Index: 276, Loss: 0.0914\n",
      "Epoch: 5, Index: 277, Loss: 0.2486\n",
      "Epoch: 5, Index: 278, Loss: 0.3896\n",
      "Epoch: 5, Index: 279, Loss: 9.8566\n",
      "Epoch: 5, Index: 280, Loss: 1.5078\n",
      "Epoch: 5, Index: 281, Loss: 0.5926\n",
      "Epoch: 5, Index: 282, Loss: 2.8948\n",
      "Epoch: 5, Index: 283, Loss: 0.2183\n",
      "Epoch: 5, Index: 284, Loss: 1.3278\n",
      "Epoch: 5, Index: 285, Loss: 3.3960\n",
      "Epoch: 5, Index: 286, Loss: 2.7668\n",
      "Epoch: 5, Index: 287, Loss: 1.2481\n",
      "Epoch: 5, Index: 288, Loss: 1.4674\n",
      "Epoch: 5, Index: 289, Loss: 0.0315\n",
      "Epoch: 5, Index: 290, Loss: 1.3476\n",
      "Epoch: 5, Index: 291, Loss: 0.1111\n",
      "Epoch: 5, Index: 292, Loss: 0.4144\n",
      "Epoch: 5, Index: 293, Loss: 1.7236\n",
      "Epoch: 5, Index: 294, Loss: 5.7097\n",
      "Epoch: 5, Index: 295, Loss: 0.7169\n",
      "Epoch: 5, Index: 296, Loss: 0.4640\n",
      "Epoch: 5, Index: 297, Loss: 0.0722\n",
      "Epoch: 5, Index: 298, Loss: 7.6747\n",
      "Epoch: 5, Index: 299, Loss: 5.6231\n",
      "Epoch: 5, Index: 300, Loss: 2.4032\n",
      "Epoch: 5, Index: 301, Loss: 0.5026\n",
      "Epoch: 5, Index: 302, Loss: 0.0795\n",
      "Epoch: 5, Index: 303, Loss: 0.5157\n",
      "Epoch: 5, Index: 304, Loss: 3.2487\n",
      "Epoch: 5, Index: 305, Loss: 1.5142\n",
      "Epoch: 5, Index: 306, Loss: 7.4698\n",
      "Epoch: 5, Index: 307, Loss: 0.0049\n",
      "Epoch: 5, Index: 308, Loss: 0.0060\n",
      "Epoch: 5, Index: 309, Loss: 0.0469\n",
      "Epoch: 5, Index: 310, Loss: 1.4911\n",
      "Epoch: 5, Index: 311, Loss: 2.8351\n",
      "Epoch: 5, Index: 312, Loss: 1.7384\n",
      "Epoch: 5, Index: 313, Loss: 0.8820\n",
      "Epoch: 5, Index: 314, Loss: 1.7381\n",
      "Epoch: 5, Index: 315, Loss: 2.1197\n",
      "Epoch: 5, Index: 316, Loss: 4.4842\n",
      "Epoch: 5, Index: 317, Loss: 1.4297\n",
      "Epoch: 5, Index: 318, Loss: 3.2538\n",
      "Epoch: 5, Index: 319, Loss: 0.1122\n",
      "Epoch: 5, Index: 320, Loss: 0.8974\n",
      "Epoch: 5, Index: 321, Loss: 3.8473\n",
      "Epoch: 5, Index: 322, Loss: 2.0495\n",
      "Epoch: 5, Index: 323, Loss: 0.9661\n",
      "Epoch: 5, Index: 324, Loss: 5.1413\n",
      "Epoch: 5, Index: 325, Loss: 0.2760\n",
      "Epoch: 5, Index: 326, Loss: 0.5333\n",
      "Epoch: 5, Index: 327, Loss: 1.2413\n",
      "Epoch: 5, Index: 328, Loss: 1.4019\n",
      "Epoch: 5, Index: 329, Loss: 2.3468\n",
      "Epoch: 5, Index: 330, Loss: 0.0143\n",
      "Epoch: 5, Index: 331, Loss: 4.2349\n",
      "Epoch: 5, Index: 332, Loss: 8.0688\n",
      "Epoch: 5, Index: 333, Loss: 1.5431\n",
      "Epoch: 5, Index: 334, Loss: 5.4929\n",
      "Epoch: 5, Index: 335, Loss: 0.7395\n",
      "Epoch: 5, Index: 336, Loss: 0.8976\n",
      "Epoch: 5, Index: 337, Loss: 0.9022\n",
      "Epoch: 5, Index: 338, Loss: 0.3059\n",
      "Epoch: 5, Index: 339, Loss: 1.1315\n",
      "Epoch: 5, Index: 340, Loss: 4.0260\n",
      "Epoch: 5, Index: 341, Loss: 1.9896\n",
      "Epoch: 5, Index: 342, Loss: 0.0968\n",
      "Epoch: 5, Index: 343, Loss: 1.5318\n",
      "Epoch: 5, Index: 344, Loss: 0.6038\n",
      "Epoch: 5, Index: 345, Loss: 0.6484\n",
      "Epoch: 5, Index: 346, Loss: 2.2466\n",
      "Epoch: 5, Index: 347, Loss: 0.7340\n",
      "Epoch: 5, Index: 348, Loss: 3.2988\n",
      "Epoch: 5, Index: 349, Loss: 2.1671\n",
      "Epoch: 5, Index: 350, Loss: 3.7399\n",
      "Epoch: 5, Index: 351, Loss: 2.7703\n",
      "Epoch: 5, Index: 352, Loss: 0.3005\n",
      "Epoch: 5, Index: 353, Loss: 1.2716\n",
      "Epoch: 5, Index: 354, Loss: 2.5792\n",
      "Epoch: 5, Index: 355, Loss: 1.4655\n",
      "Epoch: 5, Index: 356, Loss: 0.3488\n",
      "Epoch: 5, Index: 357, Loss: 0.3318\n",
      "Epoch: 5, Index: 358, Loss: 1.4144\n",
      "Epoch: 5, Index: 359, Loss: 0.8512\n",
      "Epoch: 5, Index: 360, Loss: 1.1650\n",
      "Epoch: 5, Index: 361, Loss: 0.4351\n",
      "Epoch: 5, Index: 362, Loss: 2.6999\n",
      "Epoch: 5, Index: 363, Loss: 1.8361\n",
      "Epoch: 5, Index: 364, Loss: 0.5902\n",
      "Epoch: 5, Index: 365, Loss: 1.6519\n",
      "Epoch: 5, Index: 366, Loss: 1.9014\n",
      "Epoch: 5, Index: 367, Loss: 0.3419\n",
      "Epoch: 5, Index: 368, Loss: 0.3161\n",
      "Epoch: 5, Index: 369, Loss: 4.8170\n",
      "Epoch: 5, Index: 370, Loss: 0.6871\n",
      "Epoch: 5, Index: 371, Loss: 0.8136\n",
      "Epoch: 5, Index: 372, Loss: 0.0896\n",
      "Epoch: 5, Index: 373, Loss: 0.8094\n",
      "Epoch: 5, Index: 374, Loss: 0.8361\n",
      "Epoch: 5, Index: 375, Loss: 5.5584\n",
      "Epoch: 5, Index: 376, Loss: 0.6564\n",
      "Epoch: 5, Index: 377, Loss: 3.6618\n",
      "Epoch: 5, Index: 378, Loss: 0.5152\n",
      "Epoch: 5, Index: 379, Loss: 1.8884\n",
      "Epoch: 5, Index: 380, Loss: 0.3321\n",
      "Epoch: 5, Index: 381, Loss: 7.7952\n",
      "Epoch: 5, Index: 382, Loss: 0.0178\n",
      "Epoch: 5, Index: 383, Loss: 0.8151\n",
      "Epoch: 5, Index: 384, Loss: 3.9065\n",
      "Epoch: 5, Index: 385, Loss: 2.8647\n",
      "Epoch: 5, Index: 386, Loss: 2.7247\n",
      "Epoch: 5, Index: 387, Loss: 0.4871\n",
      "Epoch: 5, Index: 388, Loss: 2.2916\n",
      "Epoch: 5, Index: 389, Loss: 4.6119\n",
      "Epoch: 5, Index: 390, Loss: 0.0048\n",
      "Epoch: 5, Index: 391, Loss: 1.4477\n",
      "Epoch: 5, Index: 392, Loss: 1.6223\n",
      "Epoch: 5, Index: 393, Loss: 0.0920\n",
      "Epoch: 5, Index: 394, Loss: 2.1825\n",
      "Epoch: 5, Index: 395, Loss: 1.2133\n",
      "Epoch: 5, Index: 396, Loss: 0.5393\n",
      "Epoch: 5, Index: 397, Loss: 3.9532\n",
      "Epoch: 5, Index: 398, Loss: 2.1672\n",
      "Epoch: 5, Index: 399, Loss: 2.7012\n",
      "Epoch: 5, Index: 400, Loss: 1.7129\n",
      "Epoch: 5, Index: 401, Loss: 1.8516\n",
      "Epoch: 5, Index: 402, Loss: 3.1105\n",
      "Epoch: 5, Index: 403, Loss: 0.3479\n",
      "Epoch: 5, Index: 404, Loss: 3.9052\n",
      "Epoch: 5, Index: 405, Loss: 0.0482\n",
      "Epoch: 5, Index: 406, Loss: 3.0109\n",
      "Epoch: 5, Index: 407, Loss: 1.5012\n",
      "Epoch: 5, Index: 408, Loss: 0.8149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bf5e01ec7a415f80ed9e300819cce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Index: 0, Loss: 0.9612\n",
      "Epoch: 6, Index: 1, Loss: 2.2086\n",
      "Epoch: 6, Index: 2, Loss: 0.6767\n",
      "Epoch: 6, Index: 3, Loss: 0.2888\n",
      "Epoch: 6, Index: 4, Loss: 0.9696\n",
      "Epoch: 6, Index: 5, Loss: 1.5998\n",
      "Epoch: 6, Index: 6, Loss: 1.1051\n",
      "Epoch: 6, Index: 7, Loss: 0.4614\n",
      "Epoch: 6, Index: 8, Loss: 8.9947\n",
      "Epoch: 6, Index: 9, Loss: 0.1910\n",
      "Epoch: 6, Index: 10, Loss: 0.0362\n",
      "Epoch: 6, Index: 11, Loss: 0.4849\n",
      "Epoch: 6, Index: 12, Loss: 0.7826\n",
      "Epoch: 6, Index: 13, Loss: 1.3451\n",
      "Epoch: 6, Index: 14, Loss: 5.3301\n",
      "Epoch: 6, Index: 15, Loss: 0.1265\n",
      "Epoch: 6, Index: 16, Loss: 0.4335\n",
      "Epoch: 6, Index: 17, Loss: 0.3400\n",
      "Epoch: 6, Index: 18, Loss: 3.1159\n",
      "Epoch: 6, Index: 19, Loss: 3.0699\n",
      "Epoch: 6, Index: 20, Loss: 14.8278\n",
      "Epoch: 6, Index: 21, Loss: 0.8323\n",
      "Epoch: 6, Index: 22, Loss: 0.8633\n",
      "Epoch: 6, Index: 23, Loss: 2.4436\n",
      "Epoch: 6, Index: 24, Loss: 0.0468\n",
      "Epoch: 6, Index: 25, Loss: 0.8901\n",
      "Epoch: 6, Index: 26, Loss: 0.2775\n",
      "Epoch: 6, Index: 27, Loss: 1.2165\n",
      "Epoch: 6, Index: 28, Loss: 1.9949\n",
      "Epoch: 6, Index: 29, Loss: 2.0737\n",
      "Epoch: 6, Index: 30, Loss: 0.1269\n",
      "Epoch: 6, Index: 31, Loss: 2.0316\n",
      "Epoch: 6, Index: 32, Loss: 0.6350\n",
      "Epoch: 6, Index: 33, Loss: 0.4337\n",
      "Epoch: 6, Index: 34, Loss: 4.3591\n",
      "Epoch: 6, Index: 35, Loss: 0.2798\n",
      "Epoch: 6, Index: 36, Loss: 3.9254\n",
      "Epoch: 6, Index: 37, Loss: 0.8562\n",
      "Epoch: 6, Index: 38, Loss: 0.0512\n",
      "Epoch: 6, Index: 39, Loss: 1.5468\n",
      "Epoch: 6, Index: 40, Loss: 2.9459\n",
      "Epoch: 6, Index: 41, Loss: 0.1695\n",
      "Epoch: 6, Index: 42, Loss: 5.5109\n",
      "Epoch: 6, Index: 43, Loss: 1.2596\n",
      "Epoch: 6, Index: 44, Loss: 0.2804\n",
      "Epoch: 6, Index: 45, Loss: 4.4975\n",
      "Epoch: 6, Index: 46, Loss: 1.1196\n",
      "Epoch: 6, Index: 47, Loss: 4.4190\n",
      "Epoch: 6, Index: 48, Loss: 1.8864\n",
      "Epoch: 6, Index: 49, Loss: 2.0621\n",
      "Epoch: 6, Index: 50, Loss: 0.9590\n",
      "Epoch: 6, Index: 51, Loss: 0.3083\n",
      "Epoch: 6, Index: 52, Loss: 0.6743\n",
      "Epoch: 6, Index: 53, Loss: 0.8950\n",
      "Epoch: 6, Index: 54, Loss: 0.0444\n",
      "Epoch: 6, Index: 55, Loss: 0.4447\n",
      "Epoch: 6, Index: 56, Loss: 1.9640\n",
      "Epoch: 6, Index: 57, Loss: 3.4452\n",
      "Epoch: 6, Index: 58, Loss: 0.3206\n",
      "Epoch: 6, Index: 59, Loss: 14.8799\n",
      "Epoch: 6, Index: 60, Loss: 0.8912\n",
      "Epoch: 6, Index: 61, Loss: 0.1227\n",
      "Epoch: 6, Index: 62, Loss: 0.5521\n",
      "Epoch: 6, Index: 63, Loss: 5.4112\n",
      "Epoch: 6, Index: 64, Loss: 0.5553\n",
      "Epoch: 6, Index: 65, Loss: 1.2628\n",
      "Epoch: 6, Index: 66, Loss: 6.1449\n",
      "Epoch: 6, Index: 67, Loss: 1.8202\n",
      "Epoch: 6, Index: 68, Loss: 1.1530\n",
      "Epoch: 6, Index: 69, Loss: 2.0249\n",
      "Epoch: 6, Index: 70, Loss: 0.3662\n",
      "Epoch: 6, Index: 71, Loss: 0.2762\n",
      "Epoch: 6, Index: 72, Loss: 2.6791\n",
      "Epoch: 6, Index: 73, Loss: 2.2227\n",
      "Epoch: 6, Index: 74, Loss: 5.1128\n",
      "Epoch: 6, Index: 75, Loss: 0.6854\n",
      "Epoch: 6, Index: 76, Loss: 1.7753\n",
      "Epoch: 6, Index: 77, Loss: 4.4509\n",
      "Epoch: 6, Index: 78, Loss: 0.0629\n",
      "Epoch: 6, Index: 79, Loss: 0.2469\n",
      "Epoch: 6, Index: 80, Loss: 1.9303\n",
      "Epoch: 6, Index: 81, Loss: 3.2307\n",
      "Epoch: 6, Index: 82, Loss: 0.2200\n",
      "Epoch: 6, Index: 83, Loss: 0.0361\n",
      "Epoch: 6, Index: 84, Loss: 2.4586\n",
      "Epoch: 6, Index: 85, Loss: 0.4234\n",
      "Epoch: 6, Index: 86, Loss: 3.2977\n",
      "Epoch: 6, Index: 87, Loss: 1.4489\n",
      "Epoch: 6, Index: 88, Loss: 0.5715\n",
      "Epoch: 6, Index: 89, Loss: 0.4254\n",
      "Epoch: 6, Index: 90, Loss: 6.8660\n",
      "Epoch: 6, Index: 91, Loss: 2.2731\n",
      "Epoch: 6, Index: 92, Loss: 2.6483\n",
      "Epoch: 6, Index: 93, Loss: 2.1637\n",
      "Epoch: 6, Index: 94, Loss: 1.9477\n",
      "Epoch: 6, Index: 95, Loss: 2.0846\n",
      "Epoch: 6, Index: 96, Loss: 1.4420\n",
      "Epoch: 6, Index: 97, Loss: 2.1958\n",
      "Epoch: 6, Index: 98, Loss: 0.1127\n",
      "Epoch: 6, Index: 99, Loss: 2.3014\n",
      "Epoch: 6, Index: 100, Loss: 2.2202\n",
      "Epoch: 6, Index: 101, Loss: 0.9234\n",
      "Epoch: 6, Index: 102, Loss: 0.2266\n",
      "Epoch: 6, Index: 103, Loss: 2.9032\n",
      "Epoch: 6, Index: 104, Loss: 4.1922\n",
      "Epoch: 6, Index: 105, Loss: 0.6481\n",
      "Epoch: 6, Index: 106, Loss: 5.5171\n",
      "Epoch: 6, Index: 107, Loss: 1.8731\n",
      "Epoch: 6, Index: 108, Loss: 7.8292\n",
      "Epoch: 6, Index: 109, Loss: 1.2791\n",
      "Epoch: 6, Index: 110, Loss: 2.0386\n",
      "Epoch: 6, Index: 111, Loss: 1.8778\n",
      "Epoch: 6, Index: 112, Loss: 6.6894\n",
      "Epoch: 6, Index: 113, Loss: 0.1566\n",
      "Epoch: 6, Index: 114, Loss: 1.4692\n",
      "Epoch: 6, Index: 115, Loss: 1.4503\n",
      "Epoch: 6, Index: 116, Loss: 1.6253\n",
      "Epoch: 6, Index: 117, Loss: 0.1441\n",
      "Epoch: 6, Index: 118, Loss: 0.3699\n",
      "Epoch: 6, Index: 119, Loss: 0.1206\n",
      "Epoch: 6, Index: 120, Loss: 0.4898\n",
      "Epoch: 6, Index: 121, Loss: 1.5502\n",
      "Epoch: 6, Index: 122, Loss: 0.0430\n",
      "Epoch: 6, Index: 123, Loss: 4.0314\n",
      "Epoch: 6, Index: 124, Loss: 0.1299\n",
      "Epoch: 6, Index: 125, Loss: 1.1947\n",
      "Epoch: 6, Index: 126, Loss: 2.0021\n",
      "Epoch: 6, Index: 127, Loss: 1.6457\n",
      "Epoch: 6, Index: 128, Loss: 4.7933\n",
      "Epoch: 6, Index: 129, Loss: 4.4210\n",
      "Epoch: 6, Index: 130, Loss: 6.6065\n",
      "Epoch: 6, Index: 131, Loss: 0.6718\n",
      "Epoch: 6, Index: 132, Loss: 7.0151\n",
      "Epoch: 6, Index: 133, Loss: 0.4730\n",
      "Epoch: 6, Index: 134, Loss: 1.2959\n",
      "Epoch: 6, Index: 135, Loss: 6.1413\n",
      "Epoch: 6, Index: 136, Loss: 2.2369\n",
      "Epoch: 6, Index: 137, Loss: 0.1035\n",
      "Epoch: 6, Index: 138, Loss: 0.0092\n",
      "Epoch: 6, Index: 139, Loss: 2.7300\n",
      "Epoch: 6, Index: 140, Loss: 2.3882\n",
      "Epoch: 6, Index: 141, Loss: 0.5248\n",
      "Epoch: 6, Index: 142, Loss: 5.4014\n",
      "Epoch: 6, Index: 143, Loss: 2.6551\n",
      "Epoch: 6, Index: 144, Loss: 2.7047\n",
      "Epoch: 6, Index: 145, Loss: 1.0769\n",
      "Epoch: 6, Index: 146, Loss: 1.5504\n",
      "Epoch: 6, Index: 147, Loss: 1.7358\n",
      "Epoch: 6, Index: 148, Loss: 7.1266\n",
      "Epoch: 6, Index: 149, Loss: 0.5049\n",
      "Epoch: 6, Index: 150, Loss: 2.1679\n",
      "Epoch: 6, Index: 151, Loss: 0.8445\n",
      "Epoch: 6, Index: 152, Loss: 2.1220\n",
      "Epoch: 6, Index: 153, Loss: 0.3561\n",
      "Epoch: 6, Index: 154, Loss: 0.8522\n",
      "Epoch: 6, Index: 155, Loss: 0.0167\n",
      "Epoch: 6, Index: 156, Loss: 3.3982\n",
      "Epoch: 6, Index: 157, Loss: 0.6889\n",
      "Epoch: 6, Index: 158, Loss: 0.3340\n",
      "Epoch: 6, Index: 159, Loss: 0.5733\n",
      "Epoch: 6, Index: 160, Loss: 0.2901\n",
      "Epoch: 6, Index: 161, Loss: 4.1152\n",
      "Epoch: 6, Index: 162, Loss: 1.9293\n",
      "Epoch: 6, Index: 163, Loss: 1.3344\n",
      "Epoch: 6, Index: 164, Loss: 0.8938\n",
      "Epoch: 6, Index: 165, Loss: 0.8123\n",
      "Epoch: 6, Index: 166, Loss: 2.3664\n",
      "Epoch: 6, Index: 167, Loss: 0.9745\n",
      "Epoch: 6, Index: 168, Loss: 0.7257\n",
      "Epoch: 6, Index: 169, Loss: 3.9039\n",
      "Epoch: 6, Index: 170, Loss: 2.5948\n",
      "Epoch: 6, Index: 171, Loss: 6.1955\n",
      "Epoch: 6, Index: 172, Loss: 0.0120\n",
      "Epoch: 6, Index: 173, Loss: 5.5262\n",
      "Epoch: 6, Index: 174, Loss: 2.3102\n",
      "Epoch: 6, Index: 175, Loss: 2.0651\n",
      "Epoch: 6, Index: 176, Loss: 1.7335\n",
      "Epoch: 6, Index: 177, Loss: 1.9528\n",
      "Epoch: 6, Index: 178, Loss: 1.8788\n",
      "Epoch: 6, Index: 179, Loss: 3.8677\n",
      "Epoch: 6, Index: 180, Loss: 0.8014\n",
      "Epoch: 6, Index: 181, Loss: 0.5407\n",
      "Epoch: 6, Index: 182, Loss: 1.0964\n",
      "Epoch: 6, Index: 183, Loss: 0.1504\n",
      "Epoch: 6, Index: 184, Loss: 1.7963\n",
      "Epoch: 6, Index: 185, Loss: 0.9312\n",
      "Epoch: 6, Index: 186, Loss: 2.3981\n",
      "Epoch: 6, Index: 187, Loss: 2.7798\n",
      "Epoch: 6, Index: 188, Loss: 6.1110\n",
      "Epoch: 6, Index: 189, Loss: 0.7614\n",
      "Epoch: 6, Index: 190, Loss: 0.1068\n",
      "Epoch: 6, Index: 191, Loss: 0.9534\n",
      "Epoch: 6, Index: 192, Loss: 1.3478\n",
      "Epoch: 6, Index: 193, Loss: 0.4362\n",
      "Epoch: 6, Index: 194, Loss: 1.1312\n",
      "Epoch: 6, Index: 195, Loss: 0.0918\n",
      "Epoch: 6, Index: 196, Loss: 0.0088\n",
      "Epoch: 6, Index: 197, Loss: 0.6409\n",
      "Epoch: 6, Index: 198, Loss: 0.5957\n",
      "Epoch: 6, Index: 199, Loss: 1.6230\n",
      "Epoch: 6, Index: 200, Loss: 2.1808\n",
      "Epoch: 6, Index: 201, Loss: 0.6417\n",
      "Epoch: 6, Index: 202, Loss: 0.2025\n",
      "Epoch: 6, Index: 203, Loss: 0.6595\n",
      "Epoch: 6, Index: 204, Loss: 1.2581\n",
      "Epoch: 6, Index: 205, Loss: 1.3629\n",
      "Epoch: 6, Index: 206, Loss: 1.7395\n",
      "Epoch: 6, Index: 207, Loss: 2.2514\n",
      "Epoch: 6, Index: 208, Loss: 0.7249\n",
      "Epoch: 6, Index: 209, Loss: 5.2360\n",
      "Epoch: 6, Index: 210, Loss: 0.7054\n",
      "Epoch: 6, Index: 211, Loss: 1.8684\n",
      "Epoch: 6, Index: 212, Loss: 2.3524\n",
      "Epoch: 6, Index: 213, Loss: 1.8390\n",
      "Epoch: 6, Index: 214, Loss: 0.1326\n",
      "Epoch: 6, Index: 215, Loss: 0.0278\n",
      "Epoch: 6, Index: 216, Loss: 1.9651\n",
      "Epoch: 6, Index: 217, Loss: 9.0773\n",
      "Epoch: 6, Index: 218, Loss: 0.5828\n",
      "Epoch: 6, Index: 219, Loss: 1.8726\n",
      "Epoch: 6, Index: 220, Loss: 5.7274\n",
      "Epoch: 6, Index: 221, Loss: 2.4754\n",
      "Epoch: 6, Index: 222, Loss: 3.6613\n",
      "Epoch: 6, Index: 223, Loss: 2.0781\n",
      "Epoch: 6, Index: 224, Loss: 2.0925\n",
      "Epoch: 6, Index: 225, Loss: 1.0714\n",
      "Epoch: 6, Index: 226, Loss: 0.3477\n",
      "Epoch: 6, Index: 227, Loss: 0.0645\n",
      "Epoch: 6, Index: 228, Loss: 3.3481\n",
      "Epoch: 6, Index: 229, Loss: 2.6406\n",
      "Epoch: 6, Index: 230, Loss: 0.8887\n",
      "Epoch: 6, Index: 231, Loss: 1.2024\n",
      "Epoch: 6, Index: 232, Loss: 2.5830\n",
      "Epoch: 6, Index: 233, Loss: 0.9808\n",
      "Epoch: 6, Index: 234, Loss: 1.5920\n",
      "Epoch: 6, Index: 235, Loss: 0.0295\n",
      "Epoch: 6, Index: 236, Loss: 2.6571\n",
      "Epoch: 6, Index: 237, Loss: 0.0787\n",
      "Epoch: 6, Index: 238, Loss: 0.2841\n",
      "Epoch: 6, Index: 239, Loss: 0.8062\n",
      "Epoch: 6, Index: 240, Loss: 5.3976\n",
      "Epoch: 6, Index: 241, Loss: 9.5632\n",
      "Epoch: 6, Index: 242, Loss: 2.1922\n",
      "Epoch: 6, Index: 243, Loss: 1.7745\n",
      "Epoch: 6, Index: 244, Loss: 1.2634\n",
      "Epoch: 6, Index: 245, Loss: 0.2455\n",
      "Epoch: 6, Index: 246, Loss: 0.8950\n",
      "Epoch: 6, Index: 247, Loss: 3.1039\n",
      "Epoch: 6, Index: 248, Loss: 4.1393\n",
      "Epoch: 6, Index: 249, Loss: 0.1995\n",
      "Epoch: 6, Index: 250, Loss: 3.1423\n",
      "Epoch: 6, Index: 251, Loss: 0.4806\n",
      "Epoch: 6, Index: 252, Loss: 0.7457\n",
      "Epoch: 6, Index: 253, Loss: 2.3990\n",
      "Epoch: 6, Index: 254, Loss: 0.5116\n",
      "Epoch: 6, Index: 255, Loss: 0.7697\n",
      "Epoch: 6, Index: 256, Loss: 7.0909\n",
      "Epoch: 6, Index: 257, Loss: 0.8889\n",
      "Epoch: 6, Index: 258, Loss: 1.0780\n",
      "Epoch: 6, Index: 259, Loss: 2.2583\n",
      "Epoch: 6, Index: 260, Loss: 1.1088\n",
      "Epoch: 6, Index: 261, Loss: 0.4322\n",
      "Epoch: 6, Index: 262, Loss: 1.5691\n",
      "Epoch: 6, Index: 263, Loss: 0.5786\n",
      "Epoch: 6, Index: 264, Loss: 0.1575\n",
      "Epoch: 6, Index: 265, Loss: 2.0157\n",
      "Epoch: 6, Index: 266, Loss: 0.5410\n",
      "Epoch: 6, Index: 267, Loss: 0.8752\n",
      "Epoch: 6, Index: 268, Loss: 3.7112\n",
      "Epoch: 6, Index: 269, Loss: 0.1790\n",
      "Epoch: 6, Index: 270, Loss: 1.5078\n",
      "Epoch: 6, Index: 271, Loss: 1.3714\n",
      "Epoch: 6, Index: 272, Loss: 1.2249\n",
      "Epoch: 6, Index: 273, Loss: 0.6992\n",
      "Epoch: 6, Index: 274, Loss: 4.1166\n",
      "Epoch: 6, Index: 275, Loss: 1.4273\n",
      "Epoch: 6, Index: 276, Loss: 1.4598\n",
      "Epoch: 6, Index: 277, Loss: 0.0577\n",
      "Epoch: 6, Index: 278, Loss: 1.6587\n",
      "Epoch: 6, Index: 279, Loss: 0.8837\n",
      "Epoch: 6, Index: 280, Loss: 1.8381\n",
      "Epoch: 6, Index: 281, Loss: 0.8420\n",
      "Epoch: 6, Index: 282, Loss: 0.5503\n",
      "Epoch: 6, Index: 283, Loss: 1.9624\n",
      "Epoch: 6, Index: 284, Loss: 1.0422\n",
      "Epoch: 6, Index: 285, Loss: 7.8560\n",
      "Epoch: 6, Index: 286, Loss: 0.3253\n",
      "Epoch: 6, Index: 287, Loss: 0.3615\n",
      "Epoch: 6, Index: 288, Loss: 0.6152\n",
      "Epoch: 6, Index: 289, Loss: 0.7202\n",
      "Epoch: 6, Index: 290, Loss: 1.2397\n",
      "Epoch: 6, Index: 291, Loss: 1.1213\n",
      "Epoch: 6, Index: 292, Loss: 0.3604\n",
      "Epoch: 6, Index: 293, Loss: 1.1853\n",
      "Epoch: 6, Index: 294, Loss: 1.2720\n",
      "Epoch: 6, Index: 295, Loss: 15.7358\n",
      "Epoch: 6, Index: 296, Loss: 0.2596\n",
      "Epoch: 6, Index: 297, Loss: 4.1865\n",
      "Epoch: 6, Index: 298, Loss: 0.6565\n",
      "Epoch: 6, Index: 299, Loss: 2.7988\n",
      "Epoch: 6, Index: 300, Loss: 0.6971\n",
      "Epoch: 6, Index: 301, Loss: 0.0446\n",
      "Epoch: 6, Index: 302, Loss: 4.5970\n",
      "Epoch: 6, Index: 303, Loss: 0.9342\n",
      "Epoch: 6, Index: 304, Loss: 0.2181\n",
      "Epoch: 6, Index: 305, Loss: 2.6688\n",
      "Epoch: 6, Index: 306, Loss: 1.0809\n",
      "Epoch: 6, Index: 307, Loss: 0.0608\n",
      "Epoch: 6, Index: 308, Loss: 2.0524\n",
      "Epoch: 6, Index: 309, Loss: 0.6450\n",
      "Epoch: 6, Index: 310, Loss: 0.6202\n",
      "Epoch: 6, Index: 311, Loss: 2.1387\n",
      "Epoch: 6, Index: 312, Loss: 0.5267\n",
      "Epoch: 6, Index: 313, Loss: 0.1405\n",
      "Epoch: 6, Index: 314, Loss: 1.1675\n",
      "Epoch: 6, Index: 315, Loss: 0.1118\n",
      "Epoch: 6, Index: 316, Loss: 4.9617\n",
      "Epoch: 6, Index: 317, Loss: 2.6978\n",
      "Epoch: 6, Index: 318, Loss: 5.2925\n",
      "Epoch: 6, Index: 319, Loss: 2.1644\n",
      "Epoch: 6, Index: 320, Loss: 1.8386\n",
      "Epoch: 6, Index: 321, Loss: 1.3112\n",
      "Epoch: 6, Index: 322, Loss: 0.1984\n",
      "Epoch: 6, Index: 323, Loss: 1.2795\n",
      "Epoch: 6, Index: 324, Loss: 1.0548\n",
      "Epoch: 6, Index: 325, Loss: 1.3518\n",
      "Epoch: 6, Index: 326, Loss: 2.2682\n",
      "Epoch: 6, Index: 327, Loss: 0.1426\n",
      "Epoch: 6, Index: 328, Loss: 3.3992\n",
      "Epoch: 6, Index: 329, Loss: 1.4262\n",
      "Epoch: 6, Index: 330, Loss: 1.0978\n",
      "Epoch: 6, Index: 331, Loss: 2.8928\n",
      "Epoch: 6, Index: 332, Loss: 0.3052\n",
      "Epoch: 6, Index: 333, Loss: 0.1044\n",
      "Epoch: 6, Index: 334, Loss: 3.8461\n",
      "Epoch: 6, Index: 335, Loss: 1.6888\n",
      "Epoch: 6, Index: 336, Loss: 0.9554\n",
      "Epoch: 6, Index: 337, Loss: 1.7730\n",
      "Epoch: 6, Index: 338, Loss: 0.0752\n",
      "Epoch: 6, Index: 339, Loss: 0.0068\n",
      "Epoch: 6, Index: 340, Loss: 0.2548\n",
      "Epoch: 6, Index: 341, Loss: 0.0777\n",
      "Epoch: 6, Index: 342, Loss: 0.6479\n",
      "Epoch: 6, Index: 343, Loss: 0.9910\n",
      "Epoch: 6, Index: 344, Loss: 2.2584\n",
      "Epoch: 6, Index: 345, Loss: 0.4824\n",
      "Epoch: 6, Index: 346, Loss: 0.6711\n",
      "Epoch: 6, Index: 347, Loss: 5.8712\n",
      "Epoch: 6, Index: 348, Loss: 9.9075\n",
      "Epoch: 6, Index: 349, Loss: 0.8047\n",
      "Epoch: 6, Index: 350, Loss: 0.6622\n",
      "Epoch: 6, Index: 351, Loss: 7.8948\n",
      "Epoch: 6, Index: 352, Loss: 0.3709\n",
      "Epoch: 6, Index: 353, Loss: 5.0140\n",
      "Epoch: 6, Index: 354, Loss: 2.3273\n",
      "Epoch: 6, Index: 355, Loss: 1.0335\n",
      "Epoch: 6, Index: 356, Loss: 11.3390\n",
      "Epoch: 6, Index: 357, Loss: 1.9006\n",
      "Epoch: 6, Index: 358, Loss: 1.2285\n",
      "Epoch: 6, Index: 359, Loss: 0.0051\n",
      "Epoch: 6, Index: 360, Loss: 1.9205\n",
      "Epoch: 6, Index: 361, Loss: 3.8865\n",
      "Epoch: 6, Index: 362, Loss: 0.0391\n",
      "Epoch: 6, Index: 363, Loss: 0.9842\n",
      "Epoch: 6, Index: 364, Loss: 2.1980\n",
      "Epoch: 6, Index: 365, Loss: 0.6020\n",
      "Epoch: 6, Index: 366, Loss: 1.6807\n",
      "Epoch: 6, Index: 367, Loss: 5.8575\n",
      "Epoch: 6, Index: 368, Loss: 2.6413\n",
      "Epoch: 6, Index: 369, Loss: 0.2672\n",
      "Epoch: 6, Index: 370, Loss: 0.0903\n",
      "Epoch: 6, Index: 371, Loss: 3.5127\n",
      "Epoch: 6, Index: 372, Loss: 0.5410\n",
      "Epoch: 6, Index: 373, Loss: 0.3310\n",
      "Epoch: 6, Index: 374, Loss: 3.1082\n",
      "Epoch: 6, Index: 375, Loss: 1.5739\n",
      "Epoch: 6, Index: 376, Loss: 0.3348\n",
      "Epoch: 6, Index: 377, Loss: 2.0277\n",
      "Epoch: 6, Index: 378, Loss: 1.8059\n",
      "Epoch: 6, Index: 379, Loss: 1.0591\n",
      "Epoch: 6, Index: 380, Loss: 0.7393\n",
      "Epoch: 6, Index: 381, Loss: 2.3729\n",
      "Epoch: 6, Index: 382, Loss: 0.9657\n",
      "Epoch: 6, Index: 383, Loss: 2.7080\n",
      "Epoch: 6, Index: 384, Loss: 2.4914\n",
      "Epoch: 6, Index: 385, Loss: 1.1135\n",
      "Epoch: 6, Index: 386, Loss: 0.7072\n",
      "Epoch: 6, Index: 387, Loss: 0.2889\n",
      "Epoch: 6, Index: 388, Loss: 0.4998\n",
      "Epoch: 6, Index: 389, Loss: 0.1791\n",
      "Epoch: 6, Index: 390, Loss: 0.0387\n",
      "Epoch: 6, Index: 391, Loss: 8.6650\n",
      "Epoch: 6, Index: 392, Loss: 1.9612\n",
      "Epoch: 6, Index: 393, Loss: 0.8647\n",
      "Epoch: 6, Index: 394, Loss: 0.4187\n",
      "Epoch: 6, Index: 395, Loss: 1.3262\n",
      "Epoch: 6, Index: 396, Loss: 0.2552\n",
      "Epoch: 6, Index: 397, Loss: 9.7931\n",
      "Epoch: 6, Index: 398, Loss: 0.4705\n",
      "Epoch: 6, Index: 399, Loss: 0.5525\n",
      "Epoch: 6, Index: 400, Loss: 1.0952\n",
      "Epoch: 6, Index: 401, Loss: 6.5133\n",
      "Epoch: 6, Index: 402, Loss: 0.0787\n",
      "Epoch: 6, Index: 403, Loss: 6.2345\n",
      "Epoch: 6, Index: 404, Loss: 0.6833\n",
      "Epoch: 6, Index: 405, Loss: 1.5727\n",
      "Epoch: 6, Index: 406, Loss: 0.0663\n",
      "Epoch: 6, Index: 407, Loss: 0.6864\n",
      "Epoch: 6, Index: 408, Loss: 1.5056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a5ec9391ca4d159b1b56191c35c757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Index: 0, Loss: 0.2931\n",
      "Epoch: 7, Index: 1, Loss: 0.5687\n",
      "Epoch: 7, Index: 2, Loss: 0.2644\n",
      "Epoch: 7, Index: 3, Loss: 0.1275\n",
      "Epoch: 7, Index: 4, Loss: 3.5363\n",
      "Epoch: 7, Index: 5, Loss: 0.5730\n",
      "Epoch: 7, Index: 6, Loss: 1.4539\n",
      "Epoch: 7, Index: 7, Loss: 0.7522\n",
      "Epoch: 7, Index: 8, Loss: 1.6645\n",
      "Epoch: 7, Index: 9, Loss: 0.3230\n",
      "Epoch: 7, Index: 10, Loss: 2.5837\n",
      "Epoch: 7, Index: 11, Loss: 0.1275\n",
      "Epoch: 7, Index: 12, Loss: 0.1477\n",
      "Epoch: 7, Index: 13, Loss: 1.3330\n",
      "Epoch: 7, Index: 14, Loss: 1.4183\n",
      "Epoch: 7, Index: 15, Loss: 0.9862\n",
      "Epoch: 7, Index: 16, Loss: 18.3862\n",
      "Epoch: 7, Index: 17, Loss: 2.8555\n",
      "Epoch: 7, Index: 18, Loss: 2.4848\n",
      "Epoch: 7, Index: 19, Loss: 1.0456\n",
      "Epoch: 7, Index: 20, Loss: 0.6743\n",
      "Epoch: 7, Index: 21, Loss: 1.8225\n",
      "Epoch: 7, Index: 22, Loss: 1.6541\n",
      "Epoch: 7, Index: 23, Loss: 0.0319\n",
      "Epoch: 7, Index: 24, Loss: 7.1026\n",
      "Epoch: 7, Index: 25, Loss: 1.3024\n",
      "Epoch: 7, Index: 26, Loss: 1.2142\n",
      "Epoch: 7, Index: 27, Loss: 1.8045\n",
      "Epoch: 7, Index: 28, Loss: 0.0524\n",
      "Epoch: 7, Index: 29, Loss: 0.2434\n",
      "Epoch: 7, Index: 30, Loss: 5.5862\n",
      "Epoch: 7, Index: 31, Loss: 0.9901\n",
      "Epoch: 7, Index: 32, Loss: 0.1884\n",
      "Epoch: 7, Index: 33, Loss: 1.5130\n",
      "Epoch: 7, Index: 34, Loss: 2.0726\n",
      "Epoch: 7, Index: 35, Loss: 2.2274\n",
      "Epoch: 7, Index: 36, Loss: 4.5705\n",
      "Epoch: 7, Index: 37, Loss: 2.1056\n",
      "Epoch: 7, Index: 38, Loss: 0.9395\n",
      "Epoch: 7, Index: 39, Loss: 0.0460\n",
      "Epoch: 7, Index: 40, Loss: 0.0793\n",
      "Epoch: 7, Index: 41, Loss: 3.8860\n",
      "Epoch: 7, Index: 42, Loss: 0.5356\n",
      "Epoch: 7, Index: 43, Loss: 0.5351\n",
      "Epoch: 7, Index: 44, Loss: 0.3314\n",
      "Epoch: 7, Index: 45, Loss: 1.5923\n",
      "Epoch: 7, Index: 46, Loss: 1.7935\n",
      "Epoch: 7, Index: 47, Loss: 1.5398\n",
      "Epoch: 7, Index: 48, Loss: 4.4224\n",
      "Epoch: 7, Index: 49, Loss: 4.5453\n",
      "Epoch: 7, Index: 50, Loss: 2.5379\n",
      "Epoch: 7, Index: 51, Loss: 0.9111\n",
      "Epoch: 7, Index: 52, Loss: 3.8678\n",
      "Epoch: 7, Index: 53, Loss: 0.2483\n",
      "Epoch: 7, Index: 54, Loss: 1.1796\n",
      "Epoch: 7, Index: 55, Loss: 1.6209\n",
      "Epoch: 7, Index: 56, Loss: 2.7501\n",
      "Epoch: 7, Index: 57, Loss: 3.3020\n",
      "Epoch: 7, Index: 58, Loss: 2.6056\n",
      "Epoch: 7, Index: 59, Loss: 0.6209\n",
      "Epoch: 7, Index: 60, Loss: 3.9391\n",
      "Epoch: 7, Index: 61, Loss: 2.4492\n",
      "Epoch: 7, Index: 62, Loss: 0.3475\n",
      "Epoch: 7, Index: 63, Loss: 0.9817\n",
      "Epoch: 7, Index: 64, Loss: 5.8573\n",
      "Epoch: 7, Index: 65, Loss: 3.2532\n",
      "Epoch: 7, Index: 66, Loss: 2.6726\n",
      "Epoch: 7, Index: 67, Loss: 0.9838\n",
      "Epoch: 7, Index: 68, Loss: 2.8238\n",
      "Epoch: 7, Index: 69, Loss: 4.5493\n",
      "Epoch: 7, Index: 70, Loss: 4.4984\n",
      "Epoch: 7, Index: 71, Loss: 1.1656\n",
      "Epoch: 7, Index: 72, Loss: 1.2716\n",
      "Epoch: 7, Index: 73, Loss: 0.3277\n",
      "Epoch: 7, Index: 74, Loss: 1.9463\n",
      "Epoch: 7, Index: 75, Loss: 0.6667\n",
      "Epoch: 7, Index: 76, Loss: 0.5520\n",
      "Epoch: 7, Index: 77, Loss: 3.4310\n",
      "Epoch: 7, Index: 78, Loss: 0.6428\n",
      "Epoch: 7, Index: 79, Loss: 0.5000\n",
      "Epoch: 7, Index: 80, Loss: 1.2274\n",
      "Epoch: 7, Index: 81, Loss: 0.0892\n",
      "Epoch: 7, Index: 82, Loss: 5.2581\n",
      "Epoch: 7, Index: 83, Loss: 0.3690\n",
      "Epoch: 7, Index: 84, Loss: 1.8291\n",
      "Epoch: 7, Index: 85, Loss: 2.8268\n",
      "Epoch: 7, Index: 86, Loss: 1.5405\n",
      "Epoch: 7, Index: 87, Loss: 5.9174\n",
      "Epoch: 7, Index: 88, Loss: 0.4164\n",
      "Epoch: 7, Index: 89, Loss: 4.2076\n",
      "Epoch: 7, Index: 90, Loss: 1.1369\n",
      "Epoch: 7, Index: 91, Loss: 0.8932\n",
      "Epoch: 7, Index: 92, Loss: 0.3268\n",
      "Epoch: 7, Index: 93, Loss: 0.1735\n",
      "Epoch: 7, Index: 94, Loss: 0.6411\n",
      "Epoch: 7, Index: 95, Loss: 0.4454\n",
      "Epoch: 7, Index: 96, Loss: 0.8761\n",
      "Epoch: 7, Index: 97, Loss: 3.9495\n",
      "Epoch: 7, Index: 98, Loss: 2.2467\n",
      "Epoch: 7, Index: 99, Loss: 0.4491\n",
      "Epoch: 7, Index: 100, Loss: 0.7157\n",
      "Epoch: 7, Index: 101, Loss: 0.1960\n",
      "Epoch: 7, Index: 102, Loss: 0.0761\n",
      "Epoch: 7, Index: 103, Loss: 5.3678\n",
      "Epoch: 7, Index: 104, Loss: 0.5922\n",
      "Epoch: 7, Index: 105, Loss: 0.1797\n",
      "Epoch: 7, Index: 106, Loss: 6.2227\n",
      "Epoch: 7, Index: 107, Loss: 2.5658\n",
      "Epoch: 7, Index: 108, Loss: 0.9266\n",
      "Epoch: 7, Index: 109, Loss: 2.7293\n",
      "Epoch: 7, Index: 110, Loss: 0.3621\n",
      "Epoch: 7, Index: 111, Loss: 2.0678\n",
      "Epoch: 7, Index: 112, Loss: 0.7968\n",
      "Epoch: 7, Index: 113, Loss: 5.4127\n",
      "Epoch: 7, Index: 114, Loss: 1.0864\n",
      "Epoch: 7, Index: 115, Loss: 1.6238\n",
      "Epoch: 7, Index: 116, Loss: 3.0911\n",
      "Epoch: 7, Index: 117, Loss: 1.9066\n",
      "Epoch: 7, Index: 118, Loss: 0.4342\n",
      "Epoch: 7, Index: 119, Loss: 0.4723\n",
      "Epoch: 7, Index: 120, Loss: 2.0074\n",
      "Epoch: 7, Index: 121, Loss: 0.3686\n",
      "Epoch: 7, Index: 122, Loss: 0.1174\n",
      "Epoch: 7, Index: 123, Loss: 0.3562\n",
      "Epoch: 7, Index: 124, Loss: 3.0114\n",
      "Epoch: 7, Index: 125, Loss: 0.8627\n",
      "Epoch: 7, Index: 126, Loss: 2.7234\n",
      "Epoch: 7, Index: 127, Loss: 0.3043\n",
      "Epoch: 7, Index: 128, Loss: 1.9916\n",
      "Epoch: 7, Index: 129, Loss: 1.3300\n",
      "Epoch: 7, Index: 130, Loss: 0.3323\n",
      "Epoch: 7, Index: 131, Loss: 0.3038\n",
      "Epoch: 7, Index: 132, Loss: 4.0528\n",
      "Epoch: 7, Index: 133, Loss: 1.8471\n",
      "Epoch: 7, Index: 134, Loss: 0.1937\n",
      "Epoch: 7, Index: 135, Loss: 0.3161\n",
      "Epoch: 7, Index: 136, Loss: 1.9543\n",
      "Epoch: 7, Index: 137, Loss: 6.8100\n",
      "Epoch: 7, Index: 138, Loss: 0.8126\n",
      "Epoch: 7, Index: 139, Loss: 1.1285\n",
      "Epoch: 7, Index: 140, Loss: 1.5886\n",
      "Epoch: 7, Index: 141, Loss: 3.4995\n",
      "Epoch: 7, Index: 142, Loss: 7.9634\n",
      "Epoch: 7, Index: 143, Loss: 1.5421\n",
      "Epoch: 7, Index: 144, Loss: 0.5743\n",
      "Epoch: 7, Index: 145, Loss: 3.0976\n",
      "Epoch: 7, Index: 146, Loss: 4.4445\n",
      "Epoch: 7, Index: 147, Loss: 0.0616\n",
      "Epoch: 7, Index: 148, Loss: 2.8406\n",
      "Epoch: 7, Index: 149, Loss: 1.6242\n",
      "Epoch: 7, Index: 150, Loss: 0.9519\n",
      "Epoch: 7, Index: 151, Loss: 0.7557\n",
      "Epoch: 7, Index: 152, Loss: 1.1699\n",
      "Epoch: 7, Index: 153, Loss: 0.7592\n",
      "Epoch: 7, Index: 154, Loss: 4.1217\n",
      "Epoch: 7, Index: 155, Loss: 4.5378\n",
      "Epoch: 7, Index: 156, Loss: 3.1746\n",
      "Epoch: 7, Index: 157, Loss: 0.1369\n",
      "Epoch: 7, Index: 158, Loss: 2.3981\n",
      "Epoch: 7, Index: 159, Loss: 2.6162\n",
      "Epoch: 7, Index: 160, Loss: 4.4785\n",
      "Epoch: 7, Index: 161, Loss: 0.6611\n",
      "Epoch: 7, Index: 162, Loss: 0.9346\n",
      "Epoch: 7, Index: 163, Loss: 0.9968\n",
      "Epoch: 7, Index: 164, Loss: 1.0325\n",
      "Epoch: 7, Index: 165, Loss: 1.2542\n",
      "Epoch: 7, Index: 166, Loss: 0.8447\n",
      "Epoch: 7, Index: 167, Loss: 1.7819\n",
      "Epoch: 7, Index: 168, Loss: 4.1555\n",
      "Epoch: 7, Index: 169, Loss: 2.1241\n",
      "Epoch: 7, Index: 170, Loss: 1.6037\n",
      "Epoch: 7, Index: 171, Loss: 1.7050\n",
      "Epoch: 7, Index: 172, Loss: 1.5746\n",
      "Epoch: 7, Index: 173, Loss: 1.7468\n",
      "Epoch: 7, Index: 174, Loss: 1.2104\n",
      "Epoch: 7, Index: 175, Loss: 0.4518\n",
      "Epoch: 7, Index: 176, Loss: 1.2584\n",
      "Epoch: 7, Index: 177, Loss: 1.7285\n",
      "Epoch: 7, Index: 178, Loss: 0.3880\n",
      "Epoch: 7, Index: 179, Loss: 1.2287\n",
      "Epoch: 7, Index: 180, Loss: 3.1177\n",
      "Epoch: 7, Index: 181, Loss: 2.1962\n",
      "Epoch: 7, Index: 182, Loss: 3.8230\n",
      "Epoch: 7, Index: 183, Loss: 0.0529\n",
      "Epoch: 7, Index: 184, Loss: 0.1294\n",
      "Epoch: 7, Index: 185, Loss: 0.5015\n",
      "Epoch: 7, Index: 186, Loss: 2.1188\n",
      "Epoch: 7, Index: 187, Loss: 2.3943\n",
      "Epoch: 7, Index: 188, Loss: 1.4222\n",
      "Epoch: 7, Index: 189, Loss: 1.2991\n",
      "Epoch: 7, Index: 190, Loss: 3.1119\n",
      "Epoch: 7, Index: 191, Loss: 1.4607\n",
      "Epoch: 7, Index: 192, Loss: 1.1139\n",
      "Epoch: 7, Index: 193, Loss: 0.6086\n",
      "Epoch: 7, Index: 194, Loss: 0.3758\n",
      "Epoch: 7, Index: 195, Loss: 0.9233\n",
      "Epoch: 7, Index: 196, Loss: 0.4055\n",
      "Epoch: 7, Index: 197, Loss: 1.2822\n",
      "Epoch: 7, Index: 198, Loss: 0.1614\n",
      "Epoch: 7, Index: 199, Loss: 1.6672\n",
      "Epoch: 7, Index: 200, Loss: 1.3126\n",
      "Epoch: 7, Index: 201, Loss: 3.3508\n",
      "Epoch: 7, Index: 202, Loss: 1.7245\n",
      "Epoch: 7, Index: 203, Loss: 0.4290\n",
      "Epoch: 7, Index: 204, Loss: 5.4221\n",
      "Epoch: 7, Index: 205, Loss: 1.3843\n",
      "Epoch: 7, Index: 206, Loss: 1.3904\n",
      "Epoch: 7, Index: 207, Loss: 0.5208\n",
      "Epoch: 7, Index: 208, Loss: 0.2731\n",
      "Epoch: 7, Index: 209, Loss: 0.7614\n",
      "Epoch: 7, Index: 210, Loss: 1.2249\n",
      "Epoch: 7, Index: 211, Loss: 0.3174\n",
      "Epoch: 7, Index: 212, Loss: 1.7828\n",
      "Epoch: 7, Index: 213, Loss: 6.1850\n",
      "Epoch: 7, Index: 214, Loss: 16.5105\n",
      "Epoch: 7, Index: 215, Loss: 1.9200\n",
      "Epoch: 7, Index: 216, Loss: 2.9376\n",
      "Epoch: 7, Index: 217, Loss: 0.5334\n",
      "Epoch: 7, Index: 218, Loss: 1.9221\n",
      "Epoch: 7, Index: 219, Loss: 3.8895\n",
      "Epoch: 7, Index: 220, Loss: 2.4868\n",
      "Epoch: 7, Index: 221, Loss: 1.2626\n",
      "Epoch: 7, Index: 222, Loss: 0.2775\n",
      "Epoch: 7, Index: 223, Loss: 2.9669\n",
      "Epoch: 7, Index: 224, Loss: 0.0057\n",
      "Epoch: 7, Index: 225, Loss: 0.7558\n",
      "Epoch: 7, Index: 226, Loss: 2.3510\n",
      "Epoch: 7, Index: 227, Loss: 0.1967\n",
      "Epoch: 7, Index: 228, Loss: 0.5127\n",
      "Epoch: 7, Index: 229, Loss: 0.1311\n",
      "Epoch: 7, Index: 230, Loss: 17.2532\n",
      "Epoch: 7, Index: 231, Loss: 1.6089\n",
      "Epoch: 7, Index: 232, Loss: 0.6466\n",
      "Epoch: 7, Index: 233, Loss: 2.8454\n",
      "Epoch: 7, Index: 234, Loss: 0.7950\n",
      "Epoch: 7, Index: 235, Loss: 1.4481\n",
      "Epoch: 7, Index: 236, Loss: 0.0999\n",
      "Epoch: 7, Index: 237, Loss: 0.2684\n",
      "Epoch: 7, Index: 238, Loss: 1.6820\n",
      "Epoch: 7, Index: 239, Loss: 1.1853\n",
      "Epoch: 7, Index: 240, Loss: 2.2828\n",
      "Epoch: 7, Index: 241, Loss: 1.3847\n",
      "Epoch: 7, Index: 242, Loss: 16.2312\n",
      "Epoch: 7, Index: 243, Loss: 1.1974\n",
      "Epoch: 7, Index: 244, Loss: 5.4358\n",
      "Epoch: 7, Index: 245, Loss: 2.1000\n",
      "Epoch: 7, Index: 246, Loss: 0.2250\n",
      "Epoch: 7, Index: 247, Loss: 0.8564\n",
      "Epoch: 7, Index: 248, Loss: 1.5005\n",
      "Epoch: 7, Index: 249, Loss: 3.3125\n",
      "Epoch: 7, Index: 250, Loss: 2.1514\n",
      "Epoch: 7, Index: 251, Loss: 1.5004\n",
      "Epoch: 7, Index: 252, Loss: 1.3940\n",
      "Epoch: 7, Index: 253, Loss: 1.2599\n",
      "Epoch: 7, Index: 254, Loss: 0.7010\n",
      "Epoch: 7, Index: 255, Loss: 3.0345\n",
      "Epoch: 7, Index: 256, Loss: 0.1031\n",
      "Epoch: 7, Index: 257, Loss: 1.7699\n",
      "Epoch: 7, Index: 258, Loss: 0.6574\n",
      "Epoch: 7, Index: 259, Loss: 0.5704\n",
      "Epoch: 7, Index: 260, Loss: 0.8164\n",
      "Epoch: 7, Index: 261, Loss: 0.3486\n",
      "Epoch: 7, Index: 262, Loss: 2.7184\n",
      "Epoch: 7, Index: 263, Loss: 0.4087\n",
      "Epoch: 7, Index: 264, Loss: 0.7658\n",
      "Epoch: 7, Index: 265, Loss: 6.6076\n",
      "Epoch: 7, Index: 266, Loss: 2.9789\n",
      "Epoch: 7, Index: 267, Loss: 0.4030\n",
      "Epoch: 7, Index: 268, Loss: 1.3179\n",
      "Epoch: 7, Index: 269, Loss: 1.4045\n",
      "Epoch: 7, Index: 270, Loss: 0.2130\n",
      "Epoch: 7, Index: 271, Loss: 5.4569\n",
      "Epoch: 7, Index: 272, Loss: 2.1989\n",
      "Epoch: 7, Index: 273, Loss: 0.3266\n",
      "Epoch: 7, Index: 274, Loss: 1.2975\n",
      "Epoch: 7, Index: 275, Loss: 3.3408\n",
      "Epoch: 7, Index: 276, Loss: 1.5012\n",
      "Epoch: 7, Index: 277, Loss: 3.0779\n",
      "Epoch: 7, Index: 278, Loss: 0.2049\n",
      "Epoch: 7, Index: 279, Loss: 1.8698\n",
      "Epoch: 7, Index: 280, Loss: 0.0385\n",
      "Epoch: 7, Index: 281, Loss: 3.3511\n",
      "Epoch: 7, Index: 282, Loss: 1.8280\n",
      "Epoch: 7, Index: 283, Loss: 3.3297\n",
      "Epoch: 7, Index: 284, Loss: 1.9407\n",
      "Epoch: 7, Index: 285, Loss: 0.7952\n",
      "Epoch: 7, Index: 286, Loss: 0.2120\n",
      "Epoch: 7, Index: 287, Loss: 1.0563\n",
      "Epoch: 7, Index: 288, Loss: 0.0063\n",
      "Epoch: 7, Index: 289, Loss: 3.4894\n",
      "Epoch: 7, Index: 290, Loss: 1.1684\n",
      "Epoch: 7, Index: 291, Loss: 0.0744\n",
      "Epoch: 7, Index: 292, Loss: 3.1293\n",
      "Epoch: 7, Index: 293, Loss: 0.9762\n",
      "Epoch: 7, Index: 294, Loss: 0.4569\n",
      "Epoch: 7, Index: 295, Loss: 0.5490\n",
      "Epoch: 7, Index: 296, Loss: 0.1764\n",
      "Epoch: 7, Index: 297, Loss: 0.3407\n",
      "Epoch: 7, Index: 298, Loss: 6.0163\n",
      "Epoch: 7, Index: 299, Loss: 1.9820\n",
      "Epoch: 7, Index: 300, Loss: 1.6988\n",
      "Epoch: 7, Index: 301, Loss: 1.0334\n",
      "Epoch: 7, Index: 302, Loss: 1.1074\n",
      "Epoch: 7, Index: 303, Loss: 1.1816\n",
      "Epoch: 7, Index: 304, Loss: 2.1168\n",
      "Epoch: 7, Index: 305, Loss: 2.3669\n",
      "Epoch: 7, Index: 306, Loss: 11.2240\n",
      "Epoch: 7, Index: 307, Loss: 2.6873\n",
      "Epoch: 7, Index: 308, Loss: 0.6190\n",
      "Epoch: 7, Index: 309, Loss: 0.0603\n",
      "Epoch: 7, Index: 310, Loss: 4.8210\n",
      "Epoch: 7, Index: 311, Loss: 0.8887\n",
      "Epoch: 7, Index: 312, Loss: 2.2451\n",
      "Epoch: 7, Index: 313, Loss: 1.8129\n",
      "Epoch: 7, Index: 314, Loss: 0.0095\n",
      "Epoch: 7, Index: 315, Loss: 0.9672\n",
      "Epoch: 7, Index: 316, Loss: 0.0321\n",
      "Epoch: 7, Index: 317, Loss: 0.2904\n",
      "Epoch: 7, Index: 318, Loss: 9.1345\n",
      "Epoch: 7, Index: 319, Loss: 2.5544\n",
      "Epoch: 7, Index: 320, Loss: 0.4051\n",
      "Epoch: 7, Index: 321, Loss: 4.6794\n",
      "Epoch: 7, Index: 322, Loss: 1.5678\n",
      "Epoch: 7, Index: 323, Loss: 1.6182\n",
      "Epoch: 7, Index: 324, Loss: 1.7749\n",
      "Epoch: 7, Index: 325, Loss: 2.2924\n",
      "Epoch: 7, Index: 326, Loss: 0.2013\n",
      "Epoch: 7, Index: 327, Loss: 0.1296\n",
      "Epoch: 7, Index: 328, Loss: 0.3834\n",
      "Epoch: 7, Index: 329, Loss: 1.9484\n",
      "Epoch: 7, Index: 330, Loss: 2.2636\n",
      "Epoch: 7, Index: 331, Loss: 0.0321\n",
      "Epoch: 7, Index: 332, Loss: 1.2759\n",
      "Epoch: 7, Index: 333, Loss: 0.6646\n",
      "Epoch: 7, Index: 334, Loss: 1.5373\n",
      "Epoch: 7, Index: 335, Loss: 1.4906\n",
      "Epoch: 7, Index: 336, Loss: 1.0414\n",
      "Epoch: 7, Index: 337, Loss: 0.6152\n",
      "Epoch: 7, Index: 338, Loss: 2.6773\n",
      "Epoch: 7, Index: 339, Loss: 2.4740\n",
      "Epoch: 7, Index: 340, Loss: 1.5757\n",
      "Epoch: 7, Index: 341, Loss: 8.0833\n",
      "Epoch: 7, Index: 342, Loss: 1.1890\n",
      "Epoch: 7, Index: 343, Loss: 5.4255\n",
      "Epoch: 7, Index: 344, Loss: 3.9277\n",
      "Epoch: 7, Index: 345, Loss: 1.1324\n",
      "Epoch: 7, Index: 346, Loss: 1.3092\n",
      "Epoch: 7, Index: 347, Loss: 2.8157\n",
      "Epoch: 7, Index: 348, Loss: 0.7181\n",
      "Epoch: 7, Index: 349, Loss: 0.0199\n",
      "Epoch: 7, Index: 350, Loss: 0.7592\n",
      "Epoch: 7, Index: 351, Loss: 0.0606\n",
      "Epoch: 7, Index: 352, Loss: 0.0180\n",
      "Epoch: 7, Index: 353, Loss: 3.1822\n",
      "Epoch: 7, Index: 354, Loss: 0.4191\n",
      "Epoch: 7, Index: 355, Loss: 0.2359\n",
      "Epoch: 7, Index: 356, Loss: 0.3852\n",
      "Epoch: 7, Index: 357, Loss: 1.7577\n",
      "Epoch: 7, Index: 358, Loss: 5.1975\n",
      "Epoch: 7, Index: 359, Loss: 1.7898\n",
      "Epoch: 7, Index: 360, Loss: 2.5972\n",
      "Epoch: 7, Index: 361, Loss: 2.4757\n",
      "Epoch: 7, Index: 362, Loss: 6.8440\n",
      "Epoch: 7, Index: 363, Loss: 2.7889\n",
      "Epoch: 7, Index: 364, Loss: 0.2986\n",
      "Epoch: 7, Index: 365, Loss: 3.3051\n",
      "Epoch: 7, Index: 366, Loss: 0.4058\n",
      "Epoch: 7, Index: 367, Loss: 1.9273\n",
      "Epoch: 7, Index: 368, Loss: 0.0304\n",
      "Epoch: 7, Index: 369, Loss: 2.0435\n",
      "Epoch: 7, Index: 370, Loss: 3.3541\n",
      "Epoch: 7, Index: 371, Loss: 3.1955\n",
      "Epoch: 7, Index: 372, Loss: 2.2181\n",
      "Epoch: 7, Index: 373, Loss: 0.1649\n",
      "Epoch: 7, Index: 374, Loss: 0.7501\n",
      "Epoch: 7, Index: 375, Loss: 0.8398\n",
      "Epoch: 7, Index: 376, Loss: 0.7980\n",
      "Epoch: 7, Index: 377, Loss: 0.9216\n",
      "Epoch: 7, Index: 378, Loss: 0.5400\n",
      "Epoch: 7, Index: 379, Loss: 1.4677\n",
      "Epoch: 7, Index: 380, Loss: 2.5183\n",
      "Epoch: 7, Index: 381, Loss: 1.0103\n",
      "Epoch: 7, Index: 382, Loss: 1.5803\n",
      "Epoch: 7, Index: 383, Loss: 3.5817\n",
      "Epoch: 7, Index: 384, Loss: 1.2116\n",
      "Epoch: 7, Index: 385, Loss: 0.5079\n",
      "Epoch: 7, Index: 386, Loss: 0.8904\n",
      "Epoch: 7, Index: 387, Loss: 0.4765\n",
      "Epoch: 7, Index: 388, Loss: 0.4226\n",
      "Epoch: 7, Index: 389, Loss: 0.4132\n",
      "Epoch: 7, Index: 390, Loss: 0.9277\n",
      "Epoch: 7, Index: 391, Loss: 1.1650\n",
      "Epoch: 7, Index: 392, Loss: 3.9316\n",
      "Epoch: 7, Index: 393, Loss: 3.8716\n",
      "Epoch: 7, Index: 394, Loss: 0.1331\n",
      "Epoch: 7, Index: 395, Loss: 0.7290\n",
      "Epoch: 7, Index: 396, Loss: 1.9814\n",
      "Epoch: 7, Index: 397, Loss: 2.3932\n",
      "Epoch: 7, Index: 398, Loss: 1.5851\n",
      "Epoch: 7, Index: 399, Loss: 1.7747\n",
      "Epoch: 7, Index: 400, Loss: 1.4168\n",
      "Epoch: 7, Index: 401, Loss: 3.0161\n",
      "Epoch: 7, Index: 402, Loss: 0.9718\n",
      "Epoch: 7, Index: 403, Loss: 0.6219\n",
      "Epoch: 7, Index: 404, Loss: 0.2130\n",
      "Epoch: 7, Index: 405, Loss: 2.3013\n",
      "Epoch: 7, Index: 406, Loss: 1.6224\n",
      "Epoch: 7, Index: 407, Loss: 0.4869\n",
      "Epoch: 7, Index: 408, Loss: 1.2487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6bb31043ca4b7d968a78131f4c49d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Index: 0, Loss: 1.9883\n",
      "Epoch: 8, Index: 1, Loss: 1.7956\n",
      "Epoch: 8, Index: 2, Loss: 1.8442\n",
      "Epoch: 8, Index: 3, Loss: 0.5669\n",
      "Epoch: 8, Index: 4, Loss: 0.0653\n",
      "Epoch: 8, Index: 5, Loss: 4.7455\n",
      "Epoch: 8, Index: 6, Loss: 1.2557\n",
      "Epoch: 8, Index: 7, Loss: 2.6531\n",
      "Epoch: 8, Index: 8, Loss: 0.3313\n",
      "Epoch: 8, Index: 9, Loss: 1.1678\n",
      "Epoch: 8, Index: 10, Loss: 0.6150\n",
      "Epoch: 8, Index: 11, Loss: 4.6056\n",
      "Epoch: 8, Index: 12, Loss: 0.5826\n",
      "Epoch: 8, Index: 13, Loss: 0.4353\n",
      "Epoch: 8, Index: 14, Loss: 2.3251\n",
      "Epoch: 8, Index: 15, Loss: 1.7895\n",
      "Epoch: 8, Index: 16, Loss: 3.7952\n",
      "Epoch: 8, Index: 17, Loss: 1.7330\n",
      "Epoch: 8, Index: 18, Loss: 0.2299\n",
      "Epoch: 8, Index: 19, Loss: 6.5630\n",
      "Epoch: 8, Index: 20, Loss: 0.9971\n",
      "Epoch: 8, Index: 21, Loss: 0.1594\n",
      "Epoch: 8, Index: 22, Loss: 2.3027\n",
      "Epoch: 8, Index: 23, Loss: 0.6917\n",
      "Epoch: 8, Index: 24, Loss: 1.4351\n",
      "Epoch: 8, Index: 25, Loss: 3.3799\n",
      "Epoch: 8, Index: 26, Loss: 0.0369\n",
      "Epoch: 8, Index: 27, Loss: 1.3978\n",
      "Epoch: 8, Index: 28, Loss: 1.3700\n",
      "Epoch: 8, Index: 29, Loss: 0.3499\n",
      "Epoch: 8, Index: 30, Loss: 3.4582\n",
      "Epoch: 8, Index: 31, Loss: 0.2828\n",
      "Epoch: 8, Index: 32, Loss: 4.5303\n",
      "Epoch: 8, Index: 33, Loss: 0.1251\n",
      "Epoch: 8, Index: 34, Loss: 1.9482\n",
      "Epoch: 8, Index: 35, Loss: 0.3137\n",
      "Epoch: 8, Index: 36, Loss: 0.0072\n",
      "Epoch: 8, Index: 37, Loss: 2.0588\n",
      "Epoch: 8, Index: 38, Loss: 0.4256\n",
      "Epoch: 8, Index: 39, Loss: 0.0405\n",
      "Epoch: 8, Index: 40, Loss: 0.1389\n",
      "Epoch: 8, Index: 41, Loss: 0.6574\n",
      "Epoch: 8, Index: 42, Loss: 2.5671\n",
      "Epoch: 8, Index: 43, Loss: 2.0945\n",
      "Epoch: 8, Index: 44, Loss: 0.3441\n",
      "Epoch: 8, Index: 45, Loss: 0.6779\n",
      "Epoch: 8, Index: 46, Loss: 0.3229\n",
      "Epoch: 8, Index: 47, Loss: 0.8728\n",
      "Epoch: 8, Index: 48, Loss: 2.9358\n",
      "Epoch: 8, Index: 49, Loss: 0.7154\n",
      "Epoch: 8, Index: 50, Loss: 0.2352\n",
      "Epoch: 8, Index: 51, Loss: 2.4725\n",
      "Epoch: 8, Index: 52, Loss: 2.3744\n",
      "Epoch: 8, Index: 53, Loss: 0.7689\n",
      "Epoch: 8, Index: 54, Loss: 3.7437\n",
      "Epoch: 8, Index: 55, Loss: 0.0074\n",
      "Epoch: 8, Index: 56, Loss: 0.1874\n",
      "Epoch: 8, Index: 57, Loss: 0.4661\n",
      "Epoch: 8, Index: 58, Loss: 0.3189\n",
      "Epoch: 8, Index: 59, Loss: 1.6641\n",
      "Epoch: 8, Index: 60, Loss: 0.9739\n",
      "Epoch: 8, Index: 61, Loss: 0.4218\n",
      "Epoch: 8, Index: 62, Loss: 2.5497\n",
      "Epoch: 8, Index: 63, Loss: 0.0521\n",
      "Epoch: 8, Index: 64, Loss: 0.1057\n",
      "Epoch: 8, Index: 65, Loss: 2.6057\n",
      "Epoch: 8, Index: 66, Loss: 1.4885\n",
      "Epoch: 8, Index: 67, Loss: 2.3209\n",
      "Epoch: 8, Index: 68, Loss: 0.4790\n",
      "Epoch: 8, Index: 69, Loss: 0.9139\n",
      "Epoch: 8, Index: 70, Loss: 2.5230\n",
      "Epoch: 8, Index: 71, Loss: 0.0065\n",
      "Epoch: 8, Index: 72, Loss: 0.0488\n",
      "Epoch: 8, Index: 73, Loss: 0.3197\n",
      "Epoch: 8, Index: 74, Loss: 2.3205\n",
      "Epoch: 8, Index: 75, Loss: 1.8556\n",
      "Epoch: 8, Index: 76, Loss: 0.8601\n",
      "Epoch: 8, Index: 77, Loss: 0.4305\n",
      "Epoch: 8, Index: 78, Loss: 0.5488\n",
      "Epoch: 8, Index: 79, Loss: 1.5078\n",
      "Epoch: 8, Index: 80, Loss: 0.6984\n",
      "Epoch: 8, Index: 81, Loss: 1.5104\n",
      "Epoch: 8, Index: 82, Loss: 3.7864\n",
      "Epoch: 8, Index: 83, Loss: 3.1496\n",
      "Epoch: 8, Index: 84, Loss: 3.3455\n",
      "Epoch: 8, Index: 85, Loss: 2.7842\n",
      "Epoch: 8, Index: 86, Loss: 2.7903\n",
      "Epoch: 8, Index: 87, Loss: 1.3649\n",
      "Epoch: 8, Index: 88, Loss: 2.0021\n",
      "Epoch: 8, Index: 89, Loss: 3.5676\n",
      "Epoch: 8, Index: 90, Loss: 0.6271\n",
      "Epoch: 8, Index: 91, Loss: 0.4108\n",
      "Epoch: 8, Index: 92, Loss: 3.7360\n",
      "Epoch: 8, Index: 93, Loss: 4.5402\n",
      "Epoch: 8, Index: 94, Loss: 1.2536\n",
      "Epoch: 8, Index: 95, Loss: 2.2952\n",
      "Epoch: 8, Index: 96, Loss: 1.8698\n",
      "Epoch: 8, Index: 97, Loss: 0.8421\n",
      "Epoch: 8, Index: 98, Loss: 3.6048\n",
      "Epoch: 8, Index: 99, Loss: 4.3281\n",
      "Epoch: 8, Index: 100, Loss: 2.9919\n",
      "Epoch: 8, Index: 101, Loss: 2.7691\n",
      "Epoch: 8, Index: 102, Loss: 0.5649\n",
      "Epoch: 8, Index: 103, Loss: 2.2099\n",
      "Epoch: 8, Index: 104, Loss: 1.7336\n",
      "Epoch: 8, Index: 105, Loss: 0.3198\n",
      "Epoch: 8, Index: 106, Loss: 0.2740\n",
      "Epoch: 8, Index: 107, Loss: 1.3713\n",
      "Epoch: 8, Index: 108, Loss: 0.7257\n",
      "Epoch: 8, Index: 109, Loss: 0.2904\n",
      "Epoch: 8, Index: 110, Loss: 0.5555\n",
      "Epoch: 8, Index: 111, Loss: 2.2075\n",
      "Epoch: 8, Index: 112, Loss: 0.1498\n",
      "Epoch: 8, Index: 113, Loss: 0.2782\n",
      "Epoch: 8, Index: 114, Loss: 0.9460\n",
      "Epoch: 8, Index: 115, Loss: 0.4767\n",
      "Epoch: 8, Index: 116, Loss: 1.8489\n",
      "Epoch: 8, Index: 117, Loss: 2.3476\n",
      "Epoch: 8, Index: 118, Loss: 0.4740\n",
      "Epoch: 8, Index: 119, Loss: 1.9556\n",
      "Epoch: 8, Index: 120, Loss: 2.8977\n",
      "Epoch: 8, Index: 121, Loss: 0.1530\n",
      "Epoch: 8, Index: 122, Loss: 4.1616\n",
      "Epoch: 8, Index: 123, Loss: 0.6528\n",
      "Epoch: 8, Index: 124, Loss: 1.8206\n",
      "Epoch: 8, Index: 125, Loss: 0.7054\n",
      "Epoch: 8, Index: 126, Loss: 2.4139\n",
      "Epoch: 8, Index: 127, Loss: 1.4955\n",
      "Epoch: 8, Index: 128, Loss: 1.2225\n",
      "Epoch: 8, Index: 129, Loss: 0.4276\n",
      "Epoch: 8, Index: 130, Loss: 1.6808\n",
      "Epoch: 8, Index: 131, Loss: 0.4721\n",
      "Epoch: 8, Index: 132, Loss: 0.0197\n",
      "Epoch: 8, Index: 133, Loss: 0.6603\n",
      "Epoch: 8, Index: 134, Loss: 1.0252\n",
      "Epoch: 8, Index: 135, Loss: 0.1413\n",
      "Epoch: 8, Index: 136, Loss: 4.0114\n",
      "Epoch: 8, Index: 137, Loss: 1.4211\n",
      "Epoch: 8, Index: 138, Loss: 1.6118\n",
      "Epoch: 8, Index: 139, Loss: 0.7280\n",
      "Epoch: 8, Index: 140, Loss: 0.1413\n",
      "Epoch: 8, Index: 141, Loss: 2.2318\n",
      "Epoch: 8, Index: 142, Loss: 1.2609\n",
      "Epoch: 8, Index: 143, Loss: 1.3039\n",
      "Epoch: 8, Index: 144, Loss: 0.9731\n",
      "Epoch: 8, Index: 145, Loss: 0.7088\n",
      "Epoch: 8, Index: 146, Loss: 1.0920\n",
      "Epoch: 8, Index: 147, Loss: 0.2476\n",
      "Epoch: 8, Index: 148, Loss: 2.5328\n",
      "Epoch: 8, Index: 149, Loss: 3.1872\n",
      "Epoch: 8, Index: 150, Loss: 8.2180\n",
      "Epoch: 8, Index: 151, Loss: 1.8513\n",
      "Epoch: 8, Index: 152, Loss: 2.0696\n",
      "Epoch: 8, Index: 153, Loss: 3.3619\n",
      "Epoch: 8, Index: 154, Loss: 3.1175\n",
      "Epoch: 8, Index: 155, Loss: 0.6652\n",
      "Epoch: 8, Index: 156, Loss: 20.7414\n",
      "Epoch: 8, Index: 157, Loss: 2.0752\n",
      "Epoch: 8, Index: 158, Loss: 5.8182\n",
      "Epoch: 8, Index: 159, Loss: 2.1903\n",
      "Epoch: 8, Index: 160, Loss: 2.5343\n",
      "Epoch: 8, Index: 161, Loss: 0.8858\n",
      "Epoch: 8, Index: 162, Loss: 4.3231\n",
      "Epoch: 8, Index: 163, Loss: 1.1054\n",
      "Epoch: 8, Index: 164, Loss: 4.4140\n",
      "Epoch: 8, Index: 165, Loss: 0.8878\n",
      "Epoch: 8, Index: 166, Loss: 0.9641\n",
      "Epoch: 8, Index: 167, Loss: 0.7864\n",
      "Epoch: 8, Index: 168, Loss: 1.3477\n",
      "Epoch: 8, Index: 169, Loss: 0.0097\n",
      "Epoch: 8, Index: 170, Loss: 3.0666\n",
      "Epoch: 8, Index: 171, Loss: 2.7993\n",
      "Epoch: 8, Index: 172, Loss: 0.9483\n",
      "Epoch: 8, Index: 173, Loss: 2.4615\n",
      "Epoch: 8, Index: 174, Loss: 3.2283\n",
      "Epoch: 8, Index: 175, Loss: 0.1373\n",
      "Epoch: 8, Index: 176, Loss: 0.4114\n",
      "Epoch: 8, Index: 177, Loss: 0.0424\n",
      "Epoch: 8, Index: 178, Loss: 1.7848\n",
      "Epoch: 8, Index: 179, Loss: 0.3573\n",
      "Epoch: 8, Index: 180, Loss: 1.8136\n",
      "Epoch: 8, Index: 181, Loss: 3.1356\n",
      "Epoch: 8, Index: 182, Loss: 1.7645\n",
      "Epoch: 8, Index: 183, Loss: 0.0679\n",
      "Epoch: 8, Index: 184, Loss: 2.5865\n",
      "Epoch: 8, Index: 185, Loss: 0.0239\n",
      "Epoch: 8, Index: 186, Loss: 0.0115\n",
      "Epoch: 8, Index: 187, Loss: 1.6434\n",
      "Epoch: 8, Index: 188, Loss: 2.0810\n",
      "Epoch: 8, Index: 189, Loss: 1.6448\n",
      "Epoch: 8, Index: 190, Loss: 10.7162\n",
      "Epoch: 8, Index: 191, Loss: 0.3254\n",
      "Epoch: 8, Index: 192, Loss: 5.6033\n",
      "Epoch: 8, Index: 193, Loss: 0.0314\n",
      "Epoch: 8, Index: 194, Loss: 3.3966\n",
      "Epoch: 8, Index: 195, Loss: 3.0225\n",
      "Epoch: 8, Index: 196, Loss: 2.7578\n",
      "Epoch: 8, Index: 197, Loss: 1.1699\n",
      "Epoch: 8, Index: 198, Loss: 1.2289\n",
      "Epoch: 8, Index: 199, Loss: 0.1229\n",
      "Epoch: 8, Index: 200, Loss: 0.7601\n",
      "Epoch: 8, Index: 201, Loss: 0.0103\n",
      "Epoch: 8, Index: 202, Loss: 1.5750\n",
      "Epoch: 8, Index: 203, Loss: 2.7225\n",
      "Epoch: 8, Index: 204, Loss: 0.2188\n",
      "Epoch: 8, Index: 205, Loss: 6.0054\n",
      "Epoch: 8, Index: 206, Loss: 0.5665\n",
      "Epoch: 8, Index: 207, Loss: 0.4168\n",
      "Epoch: 8, Index: 208, Loss: 0.0267\n",
      "Epoch: 8, Index: 209, Loss: 0.2239\n",
      "Epoch: 8, Index: 210, Loss: 1.5333\n",
      "Epoch: 8, Index: 211, Loss: 0.5744\n",
      "Epoch: 8, Index: 212, Loss: 2.0176\n",
      "Epoch: 8, Index: 213, Loss: 5.8057\n",
      "Epoch: 8, Index: 214, Loss: 1.2454\n",
      "Epoch: 8, Index: 215, Loss: 2.1881\n",
      "Epoch: 8, Index: 216, Loss: 2.3372\n",
      "Epoch: 8, Index: 217, Loss: 1.0075\n",
      "Epoch: 8, Index: 218, Loss: 2.7271\n",
      "Epoch: 8, Index: 219, Loss: 1.0563\n",
      "Epoch: 8, Index: 220, Loss: 6.9287\n",
      "Epoch: 8, Index: 221, Loss: 1.0618\n",
      "Epoch: 8, Index: 222, Loss: 0.0295\n",
      "Epoch: 8, Index: 223, Loss: 0.6350\n",
      "Epoch: 8, Index: 224, Loss: 1.0393\n",
      "Epoch: 8, Index: 225, Loss: 0.7565\n",
      "Epoch: 8, Index: 226, Loss: 1.1862\n",
      "Epoch: 8, Index: 227, Loss: 0.8268\n",
      "Epoch: 8, Index: 228, Loss: 0.6043\n",
      "Epoch: 8, Index: 229, Loss: 3.5859\n",
      "Epoch: 8, Index: 230, Loss: 0.3986\n",
      "Epoch: 8, Index: 231, Loss: 1.4062\n",
      "Epoch: 8, Index: 232, Loss: 3.8346\n",
      "Epoch: 8, Index: 233, Loss: 1.2307\n",
      "Epoch: 8, Index: 234, Loss: 1.1741\n",
      "Epoch: 8, Index: 235, Loss: 0.1968\n",
      "Epoch: 8, Index: 236, Loss: 0.6075\n",
      "Epoch: 8, Index: 237, Loss: 3.4505\n",
      "Epoch: 8, Index: 238, Loss: 1.7998\n",
      "Epoch: 8, Index: 239, Loss: 3.0573\n",
      "Epoch: 8, Index: 240, Loss: 1.4624\n",
      "Epoch: 8, Index: 241, Loss: 0.0431\n",
      "Epoch: 8, Index: 242, Loss: 0.5268\n",
      "Epoch: 8, Index: 243, Loss: 2.3040\n",
      "Epoch: 8, Index: 244, Loss: 1.6665\n",
      "Epoch: 8, Index: 245, Loss: 1.2614\n",
      "Epoch: 8, Index: 246, Loss: 1.2475\n",
      "Epoch: 8, Index: 247, Loss: 0.6158\n",
      "Epoch: 8, Index: 248, Loss: 3.2304\n",
      "Epoch: 8, Index: 249, Loss: 1.5559\n",
      "Epoch: 8, Index: 250, Loss: 1.2100\n",
      "Epoch: 8, Index: 251, Loss: 4.5263\n",
      "Epoch: 8, Index: 252, Loss: 0.2174\n",
      "Epoch: 8, Index: 253, Loss: 3.4688\n",
      "Epoch: 8, Index: 254, Loss: 5.4950\n",
      "Epoch: 8, Index: 255, Loss: 0.9023\n",
      "Epoch: 8, Index: 256, Loss: 1.1092\n",
      "Epoch: 8, Index: 257, Loss: 2.2272\n",
      "Epoch: 8, Index: 258, Loss: 1.0585\n",
      "Epoch: 8, Index: 259, Loss: 0.5310\n",
      "Epoch: 8, Index: 260, Loss: 5.2622\n",
      "Epoch: 8, Index: 261, Loss: 1.5635\n",
      "Epoch: 8, Index: 262, Loss: 1.3586\n",
      "Epoch: 8, Index: 263, Loss: 2.2241\n",
      "Epoch: 8, Index: 264, Loss: 0.1537\n",
      "Epoch: 8, Index: 265, Loss: 1.2970\n",
      "Epoch: 8, Index: 266, Loss: 3.8613\n",
      "Epoch: 8, Index: 267, Loss: 0.0640\n",
      "Epoch: 8, Index: 268, Loss: 8.2563\n",
      "Epoch: 8, Index: 269, Loss: 1.6341\n",
      "Epoch: 8, Index: 270, Loss: 0.2719\n",
      "Epoch: 8, Index: 271, Loss: 1.1361\n",
      "Epoch: 8, Index: 272, Loss: 0.9984\n",
      "Epoch: 8, Index: 273, Loss: 1.3181\n",
      "Epoch: 8, Index: 274, Loss: 0.9730\n",
      "Epoch: 8, Index: 275, Loss: 0.5272\n",
      "Epoch: 8, Index: 276, Loss: 0.7342\n",
      "Epoch: 8, Index: 277, Loss: 0.4555\n",
      "Epoch: 8, Index: 278, Loss: 1.4420\n",
      "Epoch: 8, Index: 279, Loss: 1.5280\n",
      "Epoch: 8, Index: 280, Loss: 4.9215\n",
      "Epoch: 8, Index: 281, Loss: 4.0964\n",
      "Epoch: 8, Index: 282, Loss: 1.7831\n",
      "Epoch: 8, Index: 283, Loss: 0.8817\n",
      "Epoch: 8, Index: 284, Loss: 0.8278\n",
      "Epoch: 8, Index: 285, Loss: 1.4169\n",
      "Epoch: 8, Index: 286, Loss: 2.2390\n",
      "Epoch: 8, Index: 287, Loss: 2.9334\n",
      "Epoch: 8, Index: 288, Loss: 0.0675\n",
      "Epoch: 8, Index: 289, Loss: 0.1474\n",
      "Epoch: 8, Index: 290, Loss: 0.2848\n",
      "Epoch: 8, Index: 291, Loss: 0.2623\n",
      "Epoch: 8, Index: 292, Loss: 1.0840\n",
      "Epoch: 8, Index: 293, Loss: 1.7249\n",
      "Epoch: 8, Index: 294, Loss: 1.1096\n",
      "Epoch: 8, Index: 295, Loss: 3.3573\n",
      "Epoch: 8, Index: 296, Loss: 0.6497\n",
      "Epoch: 8, Index: 297, Loss: 1.0064\n",
      "Epoch: 8, Index: 298, Loss: 1.6857\n",
      "Epoch: 8, Index: 299, Loss: 0.8895\n",
      "Epoch: 8, Index: 300, Loss: 1.0276\n",
      "Epoch: 8, Index: 301, Loss: 3.0768\n",
      "Epoch: 8, Index: 302, Loss: 0.4814\n",
      "Epoch: 8, Index: 303, Loss: 0.5605\n",
      "Epoch: 8, Index: 304, Loss: 0.0088\n",
      "Epoch: 8, Index: 305, Loss: 3.1403\n",
      "Epoch: 8, Index: 306, Loss: 7.7046\n",
      "Epoch: 8, Index: 307, Loss: 0.5729\n",
      "Epoch: 8, Index: 308, Loss: 1.5701\n",
      "Epoch: 8, Index: 309, Loss: 1.5155\n",
      "Epoch: 8, Index: 310, Loss: 1.0574\n",
      "Epoch: 8, Index: 311, Loss: 1.3853\n",
      "Epoch: 8, Index: 312, Loss: 4.1584\n",
      "Epoch: 8, Index: 313, Loss: 1.4705\n",
      "Epoch: 8, Index: 314, Loss: 1.2239\n",
      "Epoch: 8, Index: 315, Loss: 3.0202\n",
      "Epoch: 8, Index: 316, Loss: 0.5051\n",
      "Epoch: 8, Index: 317, Loss: 2.6601\n",
      "Epoch: 8, Index: 318, Loss: 1.1636\n",
      "Epoch: 8, Index: 319, Loss: 0.4449\n",
      "Epoch: 8, Index: 320, Loss: 0.3137\n",
      "Epoch: 8, Index: 321, Loss: 0.6397\n",
      "Epoch: 8, Index: 322, Loss: 2.4496\n",
      "Epoch: 8, Index: 323, Loss: 1.1923\n",
      "Epoch: 8, Index: 324, Loss: 0.8396\n",
      "Epoch: 8, Index: 325, Loss: 1.9691\n",
      "Epoch: 8, Index: 326, Loss: 0.2083\n",
      "Epoch: 8, Index: 327, Loss: 3.2819\n",
      "Epoch: 8, Index: 328, Loss: 3.2858\n",
      "Epoch: 8, Index: 329, Loss: 0.4676\n",
      "Epoch: 8, Index: 330, Loss: 2.6964\n",
      "Epoch: 8, Index: 331, Loss: 0.0505\n",
      "Epoch: 8, Index: 332, Loss: 1.1467\n",
      "Epoch: 8, Index: 333, Loss: 0.4223\n",
      "Epoch: 8, Index: 334, Loss: 4.1112\n",
      "Epoch: 8, Index: 335, Loss: 1.7179\n",
      "Epoch: 8, Index: 336, Loss: 2.0412\n",
      "Epoch: 8, Index: 337, Loss: 0.3732\n",
      "Epoch: 8, Index: 338, Loss: 4.3402\n",
      "Epoch: 8, Index: 339, Loss: 0.2195\n",
      "Epoch: 8, Index: 340, Loss: 2.2833\n",
      "Epoch: 8, Index: 341, Loss: 2.5477\n",
      "Epoch: 8, Index: 342, Loss: 1.6759\n",
      "Epoch: 8, Index: 343, Loss: 2.2719\n",
      "Epoch: 8, Index: 344, Loss: 1.0436\n",
      "Epoch: 8, Index: 345, Loss: 0.2333\n",
      "Epoch: 8, Index: 346, Loss: 0.8221\n",
      "Epoch: 8, Index: 347, Loss: 0.2335\n",
      "Epoch: 8, Index: 348, Loss: 0.9511\n",
      "Epoch: 8, Index: 349, Loss: 1.0300\n",
      "Epoch: 8, Index: 350, Loss: 0.9979\n",
      "Epoch: 8, Index: 351, Loss: 8.1712\n",
      "Epoch: 8, Index: 352, Loss: 0.7348\n",
      "Epoch: 8, Index: 353, Loss: 0.9928\n",
      "Epoch: 8, Index: 354, Loss: 3.4271\n",
      "Epoch: 8, Index: 355, Loss: 1.7177\n",
      "Epoch: 8, Index: 356, Loss: 0.8299\n",
      "Epoch: 8, Index: 357, Loss: 1.4771\n",
      "Epoch: 8, Index: 358, Loss: 4.0248\n",
      "Epoch: 8, Index: 359, Loss: 0.0274\n",
      "Epoch: 8, Index: 360, Loss: 2.1902\n",
      "Epoch: 8, Index: 361, Loss: 0.7941\n",
      "Epoch: 8, Index: 362, Loss: 0.3792\n",
      "Epoch: 8, Index: 363, Loss: 1.7723\n",
      "Epoch: 8, Index: 364, Loss: 11.0624\n",
      "Epoch: 8, Index: 365, Loss: 0.1004\n",
      "Epoch: 8, Index: 366, Loss: 1.2374\n",
      "Epoch: 8, Index: 367, Loss: 0.6902\n",
      "Epoch: 8, Index: 368, Loss: 0.7920\n",
      "Epoch: 8, Index: 369, Loss: 0.0149\n",
      "Epoch: 8, Index: 370, Loss: 0.6924\n",
      "Epoch: 8, Index: 371, Loss: 0.0763\n",
      "Epoch: 8, Index: 372, Loss: 0.0232\n",
      "Epoch: 8, Index: 373, Loss: 13.8551\n",
      "Epoch: 8, Index: 374, Loss: 1.8616\n",
      "Epoch: 8, Index: 375, Loss: 0.8968\n",
      "Epoch: 8, Index: 376, Loss: 4.4215\n",
      "Epoch: 8, Index: 377, Loss: 1.8597\n",
      "Epoch: 8, Index: 378, Loss: 2.7961\n",
      "Epoch: 8, Index: 379, Loss: 1.9371\n",
      "Epoch: 8, Index: 380, Loss: 0.1094\n",
      "Epoch: 8, Index: 381, Loss: 6.7185\n",
      "Epoch: 8, Index: 382, Loss: 1.4193\n",
      "Epoch: 8, Index: 383, Loss: 0.9313\n",
      "Epoch: 8, Index: 384, Loss: 1.0389\n",
      "Epoch: 8, Index: 385, Loss: 2.3313\n",
      "Epoch: 8, Index: 386, Loss: 1.7576\n",
      "Epoch: 8, Index: 387, Loss: 0.2004\n",
      "Epoch: 8, Index: 388, Loss: 5.4077\n",
      "Epoch: 8, Index: 389, Loss: 0.3233\n",
      "Epoch: 8, Index: 390, Loss: 3.4638\n",
      "Epoch: 8, Index: 391, Loss: 0.8499\n",
      "Epoch: 8, Index: 392, Loss: 3.5506\n",
      "Epoch: 8, Index: 393, Loss: 1.4681\n",
      "Epoch: 8, Index: 394, Loss: 3.0250\n",
      "Epoch: 8, Index: 395, Loss: 0.0566\n",
      "Epoch: 8, Index: 396, Loss: 0.6466\n",
      "Epoch: 8, Index: 397, Loss: 2.3248\n",
      "Epoch: 8, Index: 398, Loss: 3.0098\n",
      "Epoch: 8, Index: 399, Loss: 2.5757\n",
      "Epoch: 8, Index: 400, Loss: 0.8141\n",
      "Epoch: 8, Index: 401, Loss: 2.0981\n",
      "Epoch: 8, Index: 402, Loss: 2.2249\n",
      "Epoch: 8, Index: 403, Loss: 2.8977\n",
      "Epoch: 8, Index: 404, Loss: 1.1647\n",
      "Epoch: 8, Index: 405, Loss: 1.0664\n",
      "Epoch: 8, Index: 406, Loss: 1.7500\n",
      "Epoch: 8, Index: 407, Loss: 1.7503\n",
      "Epoch: 8, Index: 408, Loss: 3.9020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb61d678ea947fdbdedb2b24a30a102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Index: 0, Loss: 0.4118\n",
      "Epoch: 9, Index: 1, Loss: 3.7283\n",
      "Epoch: 9, Index: 2, Loss: 0.1318\n",
      "Epoch: 9, Index: 3, Loss: 1.1330\n",
      "Epoch: 9, Index: 4, Loss: 13.0914\n",
      "Epoch: 9, Index: 5, Loss: 4.4561\n",
      "Epoch: 9, Index: 6, Loss: 1.1973\n",
      "Epoch: 9, Index: 7, Loss: 1.7973\n",
      "Epoch: 9, Index: 8, Loss: 1.7375\n",
      "Epoch: 9, Index: 9, Loss: 1.8809\n",
      "Epoch: 9, Index: 10, Loss: 0.4000\n",
      "Epoch: 9, Index: 11, Loss: 0.1105\n",
      "Epoch: 9, Index: 12, Loss: 0.4743\n",
      "Epoch: 9, Index: 13, Loss: 2.2126\n",
      "Epoch: 9, Index: 14, Loss: 0.0876\n",
      "Epoch: 9, Index: 15, Loss: 8.0636\n",
      "Epoch: 9, Index: 16, Loss: 1.2952\n",
      "Epoch: 9, Index: 17, Loss: 8.7725\n",
      "Epoch: 9, Index: 18, Loss: 0.8590\n",
      "Epoch: 9, Index: 19, Loss: 0.0252\n",
      "Epoch: 9, Index: 20, Loss: 2.9428\n",
      "Epoch: 9, Index: 21, Loss: 5.0565\n",
      "Epoch: 9, Index: 22, Loss: 0.4749\n",
      "Epoch: 9, Index: 23, Loss: 0.3028\n",
      "Epoch: 9, Index: 24, Loss: 0.5347\n",
      "Epoch: 9, Index: 25, Loss: 0.1154\n",
      "Epoch: 9, Index: 26, Loss: 1.9947\n",
      "Epoch: 9, Index: 27, Loss: 0.3420\n",
      "Epoch: 9, Index: 28, Loss: 0.3097\n",
      "Epoch: 9, Index: 29, Loss: 2.4865\n",
      "Epoch: 9, Index: 30, Loss: 1.6111\n",
      "Epoch: 9, Index: 31, Loss: 2.9274\n",
      "Epoch: 9, Index: 32, Loss: 4.3205\n",
      "Epoch: 9, Index: 33, Loss: 3.8075\n",
      "Epoch: 9, Index: 34, Loss: 2.9116\n",
      "Epoch: 9, Index: 35, Loss: 1.3060\n",
      "Epoch: 9, Index: 36, Loss: 1.9363\n",
      "Epoch: 9, Index: 37, Loss: 1.1624\n",
      "Epoch: 9, Index: 38, Loss: 0.8628\n",
      "Epoch: 9, Index: 39, Loss: 1.0798\n",
      "Epoch: 9, Index: 40, Loss: 0.9572\n",
      "Epoch: 9, Index: 41, Loss: 1.2874\n",
      "Epoch: 9, Index: 42, Loss: 1.8787\n",
      "Epoch: 9, Index: 43, Loss: 3.1776\n",
      "Epoch: 9, Index: 44, Loss: 2.6813\n",
      "Epoch: 9, Index: 45, Loss: 1.7587\n",
      "Epoch: 9, Index: 46, Loss: 3.4317\n",
      "Epoch: 9, Index: 47, Loss: 1.9797\n",
      "Epoch: 9, Index: 48, Loss: 3.6221\n",
      "Epoch: 9, Index: 49, Loss: 1.2886\n",
      "Epoch: 9, Index: 50, Loss: 0.2812\n",
      "Epoch: 9, Index: 51, Loss: 4.9866\n",
      "Epoch: 9, Index: 52, Loss: 0.9384\n",
      "Epoch: 9, Index: 53, Loss: 0.4912\n",
      "Epoch: 9, Index: 54, Loss: 2.7052\n",
      "Epoch: 9, Index: 55, Loss: 1.3998\n",
      "Epoch: 9, Index: 56, Loss: 4.2606\n",
      "Epoch: 9, Index: 57, Loss: 0.0346\n",
      "Epoch: 9, Index: 58, Loss: 3.9437\n",
      "Epoch: 9, Index: 59, Loss: 1.2735\n",
      "Epoch: 9, Index: 60, Loss: 0.0378\n",
      "Epoch: 9, Index: 61, Loss: 1.2090\n",
      "Epoch: 9, Index: 62, Loss: 0.0086\n",
      "Epoch: 9, Index: 63, Loss: 0.7830\n",
      "Epoch: 9, Index: 64, Loss: 0.1436\n",
      "Epoch: 9, Index: 65, Loss: 1.9828\n",
      "Epoch: 9, Index: 66, Loss: 0.2001\n",
      "Epoch: 9, Index: 67, Loss: 1.0271\n",
      "Epoch: 9, Index: 68, Loss: 1.8706\n",
      "Epoch: 9, Index: 69, Loss: 0.3602\n",
      "Epoch: 9, Index: 70, Loss: 1.3023\n",
      "Epoch: 9, Index: 71, Loss: 5.4748\n",
      "Epoch: 9, Index: 72, Loss: 1.3449\n",
      "Epoch: 9, Index: 73, Loss: 0.3862\n",
      "Epoch: 9, Index: 74, Loss: 1.4850\n",
      "Epoch: 9, Index: 75, Loss: 2.1227\n",
      "Epoch: 9, Index: 76, Loss: 0.8203\n",
      "Epoch: 9, Index: 77, Loss: 1.6798\n",
      "Epoch: 9, Index: 78, Loss: 0.0385\n",
      "Epoch: 9, Index: 79, Loss: 7.9230\n",
      "Epoch: 9, Index: 80, Loss: 0.0680\n",
      "Epoch: 9, Index: 81, Loss: 0.6779\n",
      "Epoch: 9, Index: 82, Loss: 0.0646\n",
      "Epoch: 9, Index: 83, Loss: 2.2441\n",
      "Epoch: 9, Index: 84, Loss: 1.4881\n",
      "Epoch: 9, Index: 85, Loss: 0.2763\n",
      "Epoch: 9, Index: 86, Loss: 0.3306\n",
      "Epoch: 9, Index: 87, Loss: 1.1770\n",
      "Epoch: 9, Index: 88, Loss: 1.4708\n",
      "Epoch: 9, Index: 89, Loss: 2.2967\n",
      "Epoch: 9, Index: 90, Loss: 0.4075\n",
      "Epoch: 9, Index: 91, Loss: 2.2563\n",
      "Epoch: 9, Index: 92, Loss: 0.8464\n",
      "Epoch: 9, Index: 93, Loss: 4.2803\n",
      "Epoch: 9, Index: 94, Loss: 3.4175\n",
      "Epoch: 9, Index: 95, Loss: 2.1263\n",
      "Epoch: 9, Index: 96, Loss: 3.0421\n",
      "Epoch: 9, Index: 97, Loss: 4.7734\n",
      "Epoch: 9, Index: 98, Loss: 0.6626\n",
      "Epoch: 9, Index: 99, Loss: 1.2955\n",
      "Epoch: 9, Index: 100, Loss: 2.9704\n",
      "Epoch: 9, Index: 101, Loss: 0.3325\n",
      "Epoch: 9, Index: 102, Loss: 1.6629\n",
      "Epoch: 9, Index: 103, Loss: 0.3519\n",
      "Epoch: 9, Index: 104, Loss: 1.6684\n",
      "Epoch: 9, Index: 105, Loss: 1.4400\n",
      "Epoch: 9, Index: 106, Loss: 3.0454\n",
      "Epoch: 9, Index: 107, Loss: 2.7184\n",
      "Epoch: 9, Index: 108, Loss: 1.1440\n",
      "Epoch: 9, Index: 109, Loss: 2.5000\n",
      "Epoch: 9, Index: 110, Loss: 0.2161\n",
      "Epoch: 9, Index: 111, Loss: 2.2580\n",
      "Epoch: 9, Index: 112, Loss: 8.2869\n",
      "Epoch: 9, Index: 113, Loss: 2.8234\n",
      "Epoch: 9, Index: 114, Loss: 0.3934\n",
      "Epoch: 9, Index: 115, Loss: 0.9424\n",
      "Epoch: 9, Index: 116, Loss: 1.6571\n",
      "Epoch: 9, Index: 117, Loss: 3.3847\n",
      "Epoch: 9, Index: 118, Loss: 0.7736\n",
      "Epoch: 9, Index: 119, Loss: 0.8152\n",
      "Epoch: 9, Index: 120, Loss: 0.3622\n",
      "Epoch: 9, Index: 121, Loss: 1.4015\n",
      "Epoch: 9, Index: 122, Loss: 2.1052\n",
      "Epoch: 9, Index: 123, Loss: 2.3234\n",
      "Epoch: 9, Index: 124, Loss: 0.2360\n",
      "Epoch: 9, Index: 125, Loss: 3.0065\n",
      "Epoch: 9, Index: 126, Loss: 0.1781\n",
      "Epoch: 9, Index: 127, Loss: 2.3090\n",
      "Epoch: 9, Index: 128, Loss: 2.9888\n",
      "Epoch: 9, Index: 129, Loss: 0.7310\n",
      "Epoch: 9, Index: 130, Loss: 1.6249\n",
      "Epoch: 9, Index: 131, Loss: 1.9481\n",
      "Epoch: 9, Index: 132, Loss: 3.1828\n",
      "Epoch: 9, Index: 133, Loss: 1.6403\n",
      "Epoch: 9, Index: 134, Loss: 0.3090\n",
      "Epoch: 9, Index: 135, Loss: 1.9419\n",
      "Epoch: 9, Index: 136, Loss: 8.4615\n",
      "Epoch: 9, Index: 137, Loss: 1.0990\n",
      "Epoch: 9, Index: 138, Loss: 5.2192\n",
      "Epoch: 9, Index: 139, Loss: 1.3933\n",
      "Epoch: 9, Index: 140, Loss: 0.3883\n",
      "Epoch: 9, Index: 141, Loss: 0.5649\n",
      "Epoch: 9, Index: 142, Loss: 0.0352\n",
      "Epoch: 9, Index: 143, Loss: 1.8610\n",
      "Epoch: 9, Index: 144, Loss: 2.0298\n",
      "Epoch: 9, Index: 145, Loss: 0.0380\n",
      "Epoch: 9, Index: 146, Loss: 0.7754\n",
      "Epoch: 9, Index: 147, Loss: 1.1597\n",
      "Epoch: 9, Index: 148, Loss: 0.1094\n",
      "Epoch: 9, Index: 149, Loss: 3.0805\n",
      "Epoch: 9, Index: 150, Loss: 0.6123\n",
      "Epoch: 9, Index: 151, Loss: 3.5676\n",
      "Epoch: 9, Index: 152, Loss: 2.0618\n",
      "Epoch: 9, Index: 153, Loss: 1.1888\n",
      "Epoch: 9, Index: 154, Loss: 2.5043\n",
      "Epoch: 9, Index: 155, Loss: 6.1773\n",
      "Epoch: 9, Index: 156, Loss: 0.7366\n",
      "Epoch: 9, Index: 157, Loss: 3.7029\n",
      "Epoch: 9, Index: 158, Loss: 0.1298\n",
      "Epoch: 9, Index: 159, Loss: 1.3742\n",
      "Epoch: 9, Index: 160, Loss: 0.9908\n",
      "Epoch: 9, Index: 161, Loss: 0.5703\n",
      "Epoch: 9, Index: 162, Loss: 1.2366\n",
      "Epoch: 9, Index: 163, Loss: 1.2696\n",
      "Epoch: 9, Index: 164, Loss: 2.1995\n",
      "Epoch: 9, Index: 165, Loss: 1.6208\n",
      "Epoch: 9, Index: 166, Loss: 3.8680\n",
      "Epoch: 9, Index: 167, Loss: 0.0978\n",
      "Epoch: 9, Index: 168, Loss: 3.0304\n",
      "Epoch: 9, Index: 169, Loss: 0.7081\n",
      "Epoch: 9, Index: 170, Loss: 3.9043\n",
      "Epoch: 9, Index: 171, Loss: 2.5526\n",
      "Epoch: 9, Index: 172, Loss: 3.1961\n",
      "Epoch: 9, Index: 173, Loss: 1.7963\n",
      "Epoch: 9, Index: 174, Loss: 0.4225\n",
      "Epoch: 9, Index: 175, Loss: 0.4895\n",
      "Epoch: 9, Index: 176, Loss: 1.3035\n",
      "Epoch: 9, Index: 177, Loss: 0.6690\n",
      "Epoch: 9, Index: 178, Loss: 0.9934\n",
      "Epoch: 9, Index: 179, Loss: 0.2549\n",
      "Epoch: 9, Index: 180, Loss: 2.5940\n",
      "Epoch: 9, Index: 181, Loss: 14.2359\n",
      "Epoch: 9, Index: 182, Loss: 0.6700\n",
      "Epoch: 9, Index: 183, Loss: 0.0048\n",
      "Epoch: 9, Index: 184, Loss: 0.0388\n",
      "Epoch: 9, Index: 185, Loss: 3.8468\n",
      "Epoch: 9, Index: 186, Loss: 1.9963\n",
      "Epoch: 9, Index: 187, Loss: 0.1544\n",
      "Epoch: 9, Index: 188, Loss: 2.0925\n",
      "Epoch: 9, Index: 189, Loss: 2.0280\n",
      "Epoch: 9, Index: 190, Loss: 7.0986\n",
      "Epoch: 9, Index: 191, Loss: 2.0227\n",
      "Epoch: 9, Index: 192, Loss: 0.5148\n",
      "Epoch: 9, Index: 193, Loss: 2.9943\n",
      "Epoch: 9, Index: 194, Loss: 0.2630\n",
      "Epoch: 9, Index: 195, Loss: 0.8010\n",
      "Epoch: 9, Index: 196, Loss: 1.7699\n",
      "Epoch: 9, Index: 197, Loss: 0.1617\n",
      "Epoch: 9, Index: 198, Loss: 3.5657\n",
      "Epoch: 9, Index: 199, Loss: 2.4134\n",
      "Epoch: 9, Index: 200, Loss: 1.2807\n",
      "Epoch: 9, Index: 201, Loss: 1.5855\n",
      "Epoch: 9, Index: 202, Loss: 1.4405\n",
      "Epoch: 9, Index: 203, Loss: 3.4876\n",
      "Epoch: 9, Index: 204, Loss: 0.8049\n",
      "Epoch: 9, Index: 205, Loss: 0.1028\n",
      "Epoch: 9, Index: 206, Loss: 2.4427\n",
      "Epoch: 9, Index: 207, Loss: 0.5856\n",
      "Epoch: 9, Index: 208, Loss: 4.7877\n",
      "Epoch: 9, Index: 209, Loss: 0.7311\n",
      "Epoch: 9, Index: 210, Loss: 0.1144\n",
      "Epoch: 9, Index: 211, Loss: 1.2799\n",
      "Epoch: 9, Index: 212, Loss: 2.4575\n",
      "Epoch: 9, Index: 213, Loss: 2.1932\n",
      "Epoch: 9, Index: 214, Loss: 2.2691\n",
      "Epoch: 9, Index: 215, Loss: 0.9250\n",
      "Epoch: 9, Index: 216, Loss: 1.5691\n",
      "Epoch: 9, Index: 217, Loss: 1.3869\n",
      "Epoch: 9, Index: 218, Loss: 1.1852\n",
      "Epoch: 9, Index: 219, Loss: 2.9583\n",
      "Epoch: 9, Index: 220, Loss: 1.6543\n",
      "Epoch: 9, Index: 221, Loss: 0.5538\n",
      "Epoch: 9, Index: 222, Loss: 0.3485\n",
      "Epoch: 9, Index: 223, Loss: 0.3949\n",
      "Epoch: 9, Index: 224, Loss: 0.6444\n",
      "Epoch: 9, Index: 225, Loss: 1.5972\n",
      "Epoch: 9, Index: 226, Loss: 2.6098\n",
      "Epoch: 9, Index: 227, Loss: 1.3622\n",
      "Epoch: 9, Index: 228, Loss: 2.1010\n",
      "Epoch: 9, Index: 229, Loss: 1.3137\n",
      "Epoch: 9, Index: 230, Loss: 0.9459\n",
      "Epoch: 9, Index: 231, Loss: 2.3724\n",
      "Epoch: 9, Index: 232, Loss: 3.8718\n",
      "Epoch: 9, Index: 233, Loss: 0.4172\n",
      "Epoch: 9, Index: 234, Loss: 0.8139\n",
      "Epoch: 9, Index: 235, Loss: 1.8400\n",
      "Epoch: 9, Index: 236, Loss: 1.6651\n",
      "Epoch: 9, Index: 237, Loss: 0.9702\n",
      "Epoch: 9, Index: 238, Loss: 0.0427\n",
      "Epoch: 9, Index: 239, Loss: 0.4163\n",
      "Epoch: 9, Index: 240, Loss: 4.6725\n",
      "Epoch: 9, Index: 241, Loss: 0.1307\n",
      "Epoch: 9, Index: 242, Loss: 0.9185\n",
      "Epoch: 9, Index: 243, Loss: 0.3459\n",
      "Epoch: 9, Index: 244, Loss: 3.3239\n",
      "Epoch: 9, Index: 245, Loss: 0.5970\n",
      "Epoch: 9, Index: 246, Loss: 0.1026\n",
      "Epoch: 9, Index: 247, Loss: 0.9914\n",
      "Epoch: 9, Index: 248, Loss: 0.5722\n",
      "Epoch: 9, Index: 249, Loss: 0.1685\n",
      "Epoch: 9, Index: 250, Loss: 0.2548\n",
      "Epoch: 9, Index: 251, Loss: 0.7285\n",
      "Epoch: 9, Index: 252, Loss: 0.7578\n",
      "Epoch: 9, Index: 253, Loss: 0.3700\n",
      "Epoch: 9, Index: 254, Loss: 5.7995\n",
      "Epoch: 9, Index: 255, Loss: 1.8200\n",
      "Epoch: 9, Index: 256, Loss: 2.0770\n",
      "Epoch: 9, Index: 257, Loss: 0.5476\n",
      "Epoch: 9, Index: 258, Loss: 2.6371\n",
      "Epoch: 9, Index: 259, Loss: 3.1071\n",
      "Epoch: 9, Index: 260, Loss: 0.3315\n",
      "Epoch: 9, Index: 261, Loss: 2.4639\n",
      "Epoch: 9, Index: 262, Loss: 0.1076\n",
      "Epoch: 9, Index: 263, Loss: 5.9806\n",
      "Epoch: 9, Index: 264, Loss: 0.6296\n",
      "Epoch: 9, Index: 265, Loss: 2.0546\n",
      "Epoch: 9, Index: 266, Loss: 1.4084\n",
      "Epoch: 9, Index: 267, Loss: 7.7198\n",
      "Epoch: 9, Index: 268, Loss: 1.7461\n",
      "Epoch: 9, Index: 269, Loss: 0.4298\n",
      "Epoch: 9, Index: 270, Loss: 0.4705\n",
      "Epoch: 9, Index: 271, Loss: 1.0906\n",
      "Epoch: 9, Index: 272, Loss: 3.5838\n",
      "Epoch: 9, Index: 273, Loss: 3.1653\n",
      "Epoch: 9, Index: 274, Loss: 1.2214\n",
      "Epoch: 9, Index: 275, Loss: 0.3937\n",
      "Epoch: 9, Index: 276, Loss: 0.0206\n",
      "Epoch: 9, Index: 277, Loss: 11.0383\n",
      "Epoch: 9, Index: 278, Loss: 0.0022\n",
      "Epoch: 9, Index: 279, Loss: 0.3890\n",
      "Epoch: 9, Index: 280, Loss: 0.4682\n",
      "Epoch: 9, Index: 281, Loss: 1.7368\n",
      "Epoch: 9, Index: 282, Loss: 3.0226\n",
      "Epoch: 9, Index: 283, Loss: 1.5939\n",
      "Epoch: 9, Index: 284, Loss: 2.8916\n",
      "Epoch: 9, Index: 285, Loss: 1.0455\n",
      "Epoch: 9, Index: 286, Loss: 0.9402\n",
      "Epoch: 9, Index: 287, Loss: 0.5572\n",
      "Epoch: 9, Index: 288, Loss: 1.8574\n",
      "Epoch: 9, Index: 289, Loss: 0.2567\n",
      "Epoch: 9, Index: 290, Loss: 0.0764\n",
      "Epoch: 9, Index: 291, Loss: 2.1577\n",
      "Epoch: 9, Index: 292, Loss: 1.4787\n",
      "Epoch: 9, Index: 293, Loss: 0.0245\n",
      "Epoch: 9, Index: 294, Loss: 0.2188\n",
      "Epoch: 9, Index: 295, Loss: 0.4719\n",
      "Epoch: 9, Index: 296, Loss: 1.5846\n",
      "Epoch: 9, Index: 297, Loss: 10.0057\n",
      "Epoch: 9, Index: 298, Loss: 0.0794\n",
      "Epoch: 9, Index: 299, Loss: 1.1942\n",
      "Epoch: 9, Index: 300, Loss: 3.5271\n",
      "Epoch: 9, Index: 301, Loss: 2.1141\n",
      "Epoch: 9, Index: 302, Loss: 14.7574\n",
      "Epoch: 9, Index: 303, Loss: 1.7575\n",
      "Epoch: 9, Index: 304, Loss: 0.1069\n",
      "Epoch: 9, Index: 305, Loss: 1.1152\n",
      "Epoch: 9, Index: 306, Loss: 2.0206\n",
      "Epoch: 9, Index: 307, Loss: 3.3282\n",
      "Epoch: 9, Index: 308, Loss: 7.8585\n",
      "Epoch: 9, Index: 309, Loss: 3.0113\n",
      "Epoch: 9, Index: 310, Loss: 1.9276\n",
      "Epoch: 9, Index: 311, Loss: 0.5846\n",
      "Epoch: 9, Index: 312, Loss: 0.0145\n",
      "Epoch: 9, Index: 313, Loss: 0.6401\n",
      "Epoch: 9, Index: 314, Loss: 1.0354\n",
      "Epoch: 9, Index: 315, Loss: 2.1889\n",
      "Epoch: 9, Index: 316, Loss: 0.5631\n",
      "Epoch: 9, Index: 317, Loss: 0.4168\n",
      "Epoch: 9, Index: 318, Loss: 2.3604\n",
      "Epoch: 9, Index: 319, Loss: 13.6344\n",
      "Epoch: 9, Index: 320, Loss: 0.4764\n",
      "Epoch: 9, Index: 321, Loss: 1.5552\n",
      "Epoch: 9, Index: 322, Loss: 0.7791\n",
      "Epoch: 9, Index: 323, Loss: 0.9579\n",
      "Epoch: 9, Index: 324, Loss: 2.1445\n",
      "Epoch: 9, Index: 325, Loss: 1.3522\n",
      "Epoch: 9, Index: 326, Loss: 0.4150\n",
      "Epoch: 9, Index: 327, Loss: 1.8236\n",
      "Epoch: 9, Index: 328, Loss: 2.3102\n",
      "Epoch: 9, Index: 329, Loss: 3.7464\n",
      "Epoch: 9, Index: 330, Loss: 2.7596\n",
      "Epoch: 9, Index: 331, Loss: 0.5553\n",
      "Epoch: 9, Index: 332, Loss: 0.2884\n",
      "Epoch: 9, Index: 333, Loss: 4.9074\n",
      "Epoch: 9, Index: 334, Loss: 1.8974\n",
      "Epoch: 9, Index: 335, Loss: 1.7154\n",
      "Epoch: 9, Index: 336, Loss: 2.1338\n",
      "Epoch: 9, Index: 337, Loss: 2.2434\n",
      "Epoch: 9, Index: 338, Loss: 0.4772\n",
      "Epoch: 9, Index: 339, Loss: 0.4034\n",
      "Epoch: 9, Index: 340, Loss: 1.8928\n",
      "Epoch: 9, Index: 341, Loss: 1.5005\n",
      "Epoch: 9, Index: 342, Loss: 0.1926\n",
      "Epoch: 9, Index: 343, Loss: 0.5823\n",
      "Epoch: 9, Index: 344, Loss: 0.4527\n",
      "Epoch: 9, Index: 345, Loss: 0.7193\n",
      "Epoch: 9, Index: 346, Loss: 0.3952\n",
      "Epoch: 9, Index: 347, Loss: 0.6114\n",
      "Epoch: 9, Index: 348, Loss: 1.7300\n",
      "Epoch: 9, Index: 349, Loss: 1.4434\n",
      "Epoch: 9, Index: 350, Loss: 5.0648\n",
      "Epoch: 9, Index: 351, Loss: 0.0689\n",
      "Epoch: 9, Index: 352, Loss: 0.1267\n",
      "Epoch: 9, Index: 353, Loss: 3.3855\n",
      "Epoch: 9, Index: 354, Loss: 0.6840\n",
      "Epoch: 9, Index: 355, Loss: 0.2060\n",
      "Epoch: 9, Index: 356, Loss: 3.3601\n",
      "Epoch: 9, Index: 357, Loss: 0.3766\n",
      "Epoch: 9, Index: 358, Loss: 0.8060\n",
      "Epoch: 9, Index: 359, Loss: 5.0928\n",
      "Epoch: 9, Index: 360, Loss: 0.2831\n",
      "Epoch: 9, Index: 361, Loss: 0.7421\n",
      "Epoch: 9, Index: 362, Loss: 0.0779\n",
      "Epoch: 9, Index: 363, Loss: 0.5160\n",
      "Epoch: 9, Index: 364, Loss: 2.0970\n",
      "Epoch: 9, Index: 365, Loss: 0.4715\n",
      "Epoch: 9, Index: 366, Loss: 0.0101\n",
      "Epoch: 9, Index: 367, Loss: 0.4944\n",
      "Epoch: 9, Index: 368, Loss: 3.2477\n",
      "Epoch: 9, Index: 369, Loss: 3.5942\n",
      "Epoch: 9, Index: 370, Loss: 0.3491\n",
      "Epoch: 9, Index: 371, Loss: 1.9909\n",
      "Epoch: 9, Index: 372, Loss: 0.7454\n",
      "Epoch: 9, Index: 373, Loss: 0.4895\n",
      "Epoch: 9, Index: 374, Loss: 2.4508\n",
      "Epoch: 9, Index: 375, Loss: 1.3777\n",
      "Epoch: 9, Index: 376, Loss: 1.6823\n",
      "Epoch: 9, Index: 377, Loss: 1.5541\n",
      "Epoch: 9, Index: 378, Loss: 3.2819\n",
      "Epoch: 9, Index: 379, Loss: 8.6589\n",
      "Epoch: 9, Index: 380, Loss: 0.8031\n",
      "Epoch: 9, Index: 381, Loss: 1.2326\n",
      "Epoch: 9, Index: 382, Loss: 0.0483\n",
      "Epoch: 9, Index: 383, Loss: 5.1274\n",
      "Epoch: 9, Index: 384, Loss: 1.3064\n",
      "Epoch: 9, Index: 385, Loss: 3.7480\n",
      "Epoch: 9, Index: 386, Loss: 0.0271\n",
      "Epoch: 9, Index: 387, Loss: 0.2279\n",
      "Epoch: 9, Index: 388, Loss: 0.9841\n",
      "Epoch: 9, Index: 389, Loss: 3.1544\n",
      "Epoch: 9, Index: 390, Loss: 0.1715\n",
      "Epoch: 9, Index: 391, Loss: 0.8055\n",
      "Epoch: 9, Index: 392, Loss: 1.7054\n",
      "Epoch: 9, Index: 393, Loss: 5.7289\n",
      "Epoch: 9, Index: 394, Loss: 1.0109\n",
      "Epoch: 9, Index: 395, Loss: 4.7247\n",
      "Epoch: 9, Index: 396, Loss: 1.4807\n",
      "Epoch: 9, Index: 397, Loss: 0.7723\n",
      "Epoch: 9, Index: 398, Loss: 0.4591\n",
      "Epoch: 9, Index: 399, Loss: 0.2846\n",
      "Epoch: 9, Index: 400, Loss: 0.1409\n",
      "Epoch: 9, Index: 401, Loss: 1.2165\n",
      "Epoch: 9, Index: 402, Loss: 3.2996\n",
      "Epoch: 9, Index: 403, Loss: 0.5270\n",
      "Epoch: 9, Index: 404, Loss: 0.0122\n",
      "Epoch: 9, Index: 405, Loss: 1.8361\n",
      "Epoch: 9, Index: 406, Loss: 5.0123\n",
      "Epoch: 9, Index: 407, Loss: 1.2187\n",
      "Epoch: 9, Index: 408, Loss: 0.6939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a168f23c0b4b48639938c5d2fa10bb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Index: 0, Loss: 4.6757\n",
      "Epoch: 10, Index: 1, Loss: 0.7854\n",
      "Epoch: 10, Index: 2, Loss: 1.7072\n",
      "Epoch: 10, Index: 3, Loss: 0.1802\n",
      "Epoch: 10, Index: 4, Loss: 1.1890\n",
      "Epoch: 10, Index: 5, Loss: 0.9257\n",
      "Epoch: 10, Index: 6, Loss: 0.3417\n",
      "Epoch: 10, Index: 7, Loss: 0.6226\n",
      "Epoch: 10, Index: 8, Loss: 1.4821\n",
      "Epoch: 10, Index: 9, Loss: 2.9389\n",
      "Epoch: 10, Index: 10, Loss: 2.1737\n",
      "Epoch: 10, Index: 11, Loss: 10.0425\n",
      "Epoch: 10, Index: 12, Loss: 3.3620\n",
      "Epoch: 10, Index: 13, Loss: 0.2198\n",
      "Epoch: 10, Index: 14, Loss: 0.4557\n",
      "Epoch: 10, Index: 15, Loss: 0.3746\n",
      "Epoch: 10, Index: 16, Loss: 0.5170\n",
      "Epoch: 10, Index: 17, Loss: 1.0418\n",
      "Epoch: 10, Index: 18, Loss: 0.7113\n",
      "Epoch: 10, Index: 19, Loss: 0.1232\n",
      "Epoch: 10, Index: 20, Loss: 3.0564\n",
      "Epoch: 10, Index: 21, Loss: 2.5149\n",
      "Epoch: 10, Index: 22, Loss: 5.0118\n",
      "Epoch: 10, Index: 23, Loss: 0.2793\n",
      "Epoch: 10, Index: 24, Loss: 0.4056\n",
      "Epoch: 10, Index: 25, Loss: 2.4700\n",
      "Epoch: 10, Index: 26, Loss: 0.6630\n",
      "Epoch: 10, Index: 27, Loss: 0.7611\n",
      "Epoch: 10, Index: 28, Loss: 1.6480\n",
      "Epoch: 10, Index: 29, Loss: 1.6475\n",
      "Epoch: 10, Index: 30, Loss: 0.4326\n",
      "Epoch: 10, Index: 31, Loss: 1.0561\n",
      "Epoch: 10, Index: 32, Loss: 1.0086\n",
      "Epoch: 10, Index: 33, Loss: 1.9448\n",
      "Epoch: 10, Index: 34, Loss: 6.7578\n",
      "Epoch: 10, Index: 35, Loss: 1.1563\n",
      "Epoch: 10, Index: 36, Loss: 2.0349\n",
      "Epoch: 10, Index: 37, Loss: 0.3405\n",
      "Epoch: 10, Index: 38, Loss: 0.3339\n",
      "Epoch: 10, Index: 39, Loss: 1.2008\n",
      "Epoch: 10, Index: 40, Loss: 0.1995\n",
      "Epoch: 10, Index: 41, Loss: 2.5807\n",
      "Epoch: 10, Index: 42, Loss: 1.4478\n",
      "Epoch: 10, Index: 43, Loss: 1.4479\n",
      "Epoch: 10, Index: 44, Loss: 1.3489\n",
      "Epoch: 10, Index: 45, Loss: 5.0483\n",
      "Epoch: 10, Index: 46, Loss: 0.1144\n",
      "Epoch: 10, Index: 47, Loss: 0.1190\n",
      "Epoch: 10, Index: 48, Loss: 1.1280\n",
      "Epoch: 10, Index: 49, Loss: 1.1190\n",
      "Epoch: 10, Index: 50, Loss: 0.6938\n",
      "Epoch: 10, Index: 51, Loss: 0.2548\n",
      "Epoch: 10, Index: 52, Loss: 0.2137\n",
      "Epoch: 10, Index: 53, Loss: 0.0074\n",
      "Epoch: 10, Index: 54, Loss: 1.4500\n",
      "Epoch: 10, Index: 55, Loss: 0.3628\n",
      "Epoch: 10, Index: 56, Loss: 5.7769\n",
      "Epoch: 10, Index: 57, Loss: 0.0079\n",
      "Epoch: 10, Index: 58, Loss: 1.1944\n",
      "Epoch: 10, Index: 59, Loss: 1.2942\n",
      "Epoch: 10, Index: 60, Loss: 1.0333\n",
      "Epoch: 10, Index: 61, Loss: 1.8896\n",
      "Epoch: 10, Index: 62, Loss: 1.4978\n",
      "Epoch: 10, Index: 63, Loss: 3.2926\n",
      "Epoch: 10, Index: 64, Loss: 0.9167\n",
      "Epoch: 10, Index: 65, Loss: 0.0854\n",
      "Epoch: 10, Index: 66, Loss: 5.5012\n",
      "Epoch: 10, Index: 67, Loss: 2.1539\n",
      "Epoch: 10, Index: 68, Loss: 0.0012\n",
      "Epoch: 10, Index: 69, Loss: 3.6907\n",
      "Epoch: 10, Index: 70, Loss: 1.8917\n",
      "Epoch: 10, Index: 71, Loss: 0.2409\n",
      "Epoch: 10, Index: 72, Loss: 1.7242\n",
      "Epoch: 10, Index: 73, Loss: 6.3723\n",
      "Epoch: 10, Index: 74, Loss: 3.2021\n",
      "Epoch: 10, Index: 75, Loss: 6.9069\n",
      "Epoch: 10, Index: 76, Loss: 2.2614\n",
      "Epoch: 10, Index: 77, Loss: 0.3519\n",
      "Epoch: 10, Index: 78, Loss: 0.5923\n",
      "Epoch: 10, Index: 79, Loss: 0.0948\n",
      "Epoch: 10, Index: 80, Loss: 0.5335\n",
      "Epoch: 10, Index: 81, Loss: 0.2223\n",
      "Epoch: 10, Index: 82, Loss: 3.9855\n",
      "Epoch: 10, Index: 83, Loss: 0.2410\n",
      "Epoch: 10, Index: 84, Loss: 3.1943\n",
      "Epoch: 10, Index: 85, Loss: 2.3490\n",
      "Epoch: 10, Index: 86, Loss: 2.8275\n",
      "Epoch: 10, Index: 87, Loss: 1.6897\n",
      "Epoch: 10, Index: 88, Loss: 3.8696\n",
      "Epoch: 10, Index: 89, Loss: 3.3285\n",
      "Epoch: 10, Index: 90, Loss: 0.7438\n",
      "Epoch: 10, Index: 91, Loss: 0.7135\n",
      "Epoch: 10, Index: 92, Loss: 1.0999\n",
      "Epoch: 10, Index: 93, Loss: 0.8204\n",
      "Epoch: 10, Index: 94, Loss: 0.0804\n",
      "Epoch: 10, Index: 95, Loss: 1.0030\n",
      "Epoch: 10, Index: 96, Loss: 2.9791\n",
      "Epoch: 10, Index: 97, Loss: 0.4635\n",
      "Epoch: 10, Index: 98, Loss: 0.0651\n",
      "Epoch: 10, Index: 99, Loss: 0.8747\n",
      "Epoch: 10, Index: 100, Loss: 0.0776\n",
      "Epoch: 10, Index: 101, Loss: 0.7713\n",
      "Epoch: 10, Index: 102, Loss: 1.6043\n",
      "Epoch: 10, Index: 103, Loss: 0.1229\n",
      "Epoch: 10, Index: 104, Loss: 1.3086\n",
      "Epoch: 10, Index: 105, Loss: 3.1420\n",
      "Epoch: 10, Index: 106, Loss: 1.5936\n",
      "Epoch: 10, Index: 107, Loss: 3.5416\n",
      "Epoch: 10, Index: 108, Loss: 0.5697\n",
      "Epoch: 10, Index: 109, Loss: 2.5543\n",
      "Epoch: 10, Index: 110, Loss: 2.6533\n",
      "Epoch: 10, Index: 111, Loss: 1.5768\n",
      "Epoch: 10, Index: 112, Loss: 1.6878\n",
      "Epoch: 10, Index: 113, Loss: 0.4254\n",
      "Epoch: 10, Index: 114, Loss: 4.4586\n",
      "Epoch: 10, Index: 115, Loss: 5.2485\n",
      "Epoch: 10, Index: 116, Loss: 4.1762\n",
      "Epoch: 10, Index: 117, Loss: 4.2705\n",
      "Epoch: 10, Index: 118, Loss: 2.0213\n",
      "Epoch: 10, Index: 119, Loss: 1.2477\n",
      "Epoch: 10, Index: 120, Loss: 3.3937\n",
      "Epoch: 10, Index: 121, Loss: 1.0250\n",
      "Epoch: 10, Index: 122, Loss: 18.1098\n",
      "Epoch: 10, Index: 123, Loss: 0.9494\n",
      "Epoch: 10, Index: 124, Loss: 4.9047\n",
      "Epoch: 10, Index: 125, Loss: 0.4945\n",
      "Epoch: 10, Index: 126, Loss: 1.8632\n",
      "Epoch: 10, Index: 127, Loss: 0.5589\n",
      "Epoch: 10, Index: 128, Loss: 0.0092\n",
      "Epoch: 10, Index: 129, Loss: 1.0043\n",
      "Epoch: 10, Index: 130, Loss: 1.3176\n",
      "Epoch: 10, Index: 131, Loss: 0.1454\n",
      "Epoch: 10, Index: 132, Loss: 2.2588\n",
      "Epoch: 10, Index: 133, Loss: 1.4779\n",
      "Epoch: 10, Index: 134, Loss: 3.0948\n",
      "Epoch: 10, Index: 135, Loss: 1.2621\n",
      "Epoch: 10, Index: 136, Loss: 2.3043\n",
      "Epoch: 10, Index: 137, Loss: 1.6938\n",
      "Epoch: 10, Index: 138, Loss: 2.1947\n",
      "Epoch: 10, Index: 139, Loss: 3.3410\n",
      "Epoch: 10, Index: 140, Loss: 2.0769\n",
      "Epoch: 10, Index: 141, Loss: 1.7800\n",
      "Epoch: 10, Index: 142, Loss: 2.1207\n",
      "Epoch: 10, Index: 143, Loss: 0.6708\n",
      "Epoch: 10, Index: 144, Loss: 5.8382\n",
      "Epoch: 10, Index: 145, Loss: 0.0223\n",
      "Epoch: 10, Index: 146, Loss: 2.3449\n",
      "Epoch: 10, Index: 147, Loss: 2.8829\n",
      "Epoch: 10, Index: 148, Loss: 1.3256\n",
      "Epoch: 10, Index: 149, Loss: 2.2339\n",
      "Epoch: 10, Index: 150, Loss: 1.5415\n",
      "Epoch: 10, Index: 151, Loss: 2.2994\n",
      "Epoch: 10, Index: 152, Loss: 5.6445\n",
      "Epoch: 10, Index: 153, Loss: 0.0939\n",
      "Epoch: 10, Index: 154, Loss: 0.3964\n",
      "Epoch: 10, Index: 155, Loss: 2.7037\n",
      "Epoch: 10, Index: 156, Loss: 0.2900\n",
      "Epoch: 10, Index: 157, Loss: 1.0432\n",
      "Epoch: 10, Index: 158, Loss: 1.6125\n",
      "Epoch: 10, Index: 159, Loss: 0.9307\n",
      "Epoch: 10, Index: 160, Loss: 0.2641\n",
      "Epoch: 10, Index: 161, Loss: 0.5570\n",
      "Epoch: 10, Index: 162, Loss: 0.8053\n",
      "Epoch: 10, Index: 163, Loss: 1.1605\n",
      "Epoch: 10, Index: 164, Loss: 0.0770\n",
      "Epoch: 10, Index: 165, Loss: 0.5407\n",
      "Epoch: 10, Index: 166, Loss: 1.4320\n",
      "Epoch: 10, Index: 167, Loss: 1.5039\n",
      "Epoch: 10, Index: 168, Loss: 0.2369\n",
      "Epoch: 10, Index: 169, Loss: 0.0890\n",
      "Epoch: 10, Index: 170, Loss: 1.4064\n",
      "Epoch: 10, Index: 171, Loss: 2.1558\n",
      "Epoch: 10, Index: 172, Loss: 1.8557\n",
      "Epoch: 10, Index: 173, Loss: 0.1453\n",
      "Epoch: 10, Index: 174, Loss: 0.8478\n",
      "Epoch: 10, Index: 175, Loss: 0.2666\n",
      "Epoch: 10, Index: 176, Loss: 0.2460\n",
      "Epoch: 10, Index: 177, Loss: 1.8868\n",
      "Epoch: 10, Index: 178, Loss: 2.0794\n",
      "Epoch: 10, Index: 179, Loss: 1.8791\n",
      "Epoch: 10, Index: 180, Loss: 0.2209\n",
      "Epoch: 10, Index: 181, Loss: 0.1518\n",
      "Epoch: 10, Index: 182, Loss: 3.1174\n",
      "Epoch: 10, Index: 183, Loss: 3.0381\n",
      "Epoch: 10, Index: 184, Loss: 0.0830\n",
      "Epoch: 10, Index: 185, Loss: 0.5120\n",
      "Epoch: 10, Index: 186, Loss: 5.7416\n",
      "Epoch: 10, Index: 187, Loss: 1.0638\n",
      "Epoch: 10, Index: 188, Loss: 2.2097\n",
      "Epoch: 10, Index: 189, Loss: 3.1209\n",
      "Epoch: 10, Index: 190, Loss: 1.0668\n",
      "Epoch: 10, Index: 191, Loss: 0.1500\n",
      "Epoch: 10, Index: 192, Loss: 2.0635\n",
      "Epoch: 10, Index: 193, Loss: 1.9557\n",
      "Epoch: 10, Index: 194, Loss: 6.3517\n",
      "Epoch: 10, Index: 195, Loss: 0.5339\n",
      "Epoch: 10, Index: 196, Loss: 1.0116\n",
      "Epoch: 10, Index: 197, Loss: 3.3660\n",
      "Epoch: 10, Index: 198, Loss: 0.8792\n",
      "Epoch: 10, Index: 199, Loss: 1.2506\n",
      "Epoch: 10, Index: 200, Loss: 0.1322\n",
      "Epoch: 10, Index: 201, Loss: 0.1550\n",
      "Epoch: 10, Index: 202, Loss: 1.2975\n",
      "Epoch: 10, Index: 203, Loss: 1.2815\n",
      "Epoch: 10, Index: 204, Loss: 5.4665\n",
      "Epoch: 10, Index: 205, Loss: 2.2724\n",
      "Epoch: 10, Index: 206, Loss: 0.2175\n",
      "Epoch: 10, Index: 207, Loss: 1.5387\n",
      "Epoch: 10, Index: 208, Loss: 0.3839\n",
      "Epoch: 10, Index: 209, Loss: 0.0336\n",
      "Epoch: 10, Index: 210, Loss: 0.5359\n",
      "Epoch: 10, Index: 211, Loss: 1.5465\n",
      "Epoch: 10, Index: 212, Loss: 6.4983\n",
      "Epoch: 10, Index: 213, Loss: 1.3598\n",
      "Epoch: 10, Index: 214, Loss: 0.1144\n",
      "Epoch: 10, Index: 215, Loss: 1.6988\n",
      "Epoch: 10, Index: 216, Loss: 1.1486\n",
      "Epoch: 10, Index: 217, Loss: 0.2101\n",
      "Epoch: 10, Index: 218, Loss: 7.1461\n",
      "Epoch: 10, Index: 219, Loss: 1.0929\n",
      "Epoch: 10, Index: 220, Loss: 0.1694\n",
      "Epoch: 10, Index: 221, Loss: 2.2077\n",
      "Epoch: 10, Index: 222, Loss: 3.1124\n",
      "Epoch: 10, Index: 223, Loss: 1.9247\n",
      "Epoch: 10, Index: 224, Loss: 0.5552\n",
      "Epoch: 10, Index: 225, Loss: 1.3599\n",
      "Epoch: 10, Index: 226, Loss: 2.3225\n",
      "Epoch: 10, Index: 227, Loss: 1.7236\n",
      "Epoch: 10, Index: 228, Loss: 2.6567\n",
      "Epoch: 10, Index: 229, Loss: 0.3138\n",
      "Epoch: 10, Index: 230, Loss: 2.4949\n",
      "Epoch: 10, Index: 231, Loss: 0.6503\n",
      "Epoch: 10, Index: 232, Loss: 1.9033\n",
      "Epoch: 10, Index: 233, Loss: 4.4213\n",
      "Epoch: 10, Index: 234, Loss: 0.9319\n",
      "Epoch: 10, Index: 235, Loss: 2.2939\n",
      "Epoch: 10, Index: 236, Loss: 1.2936\n",
      "Epoch: 10, Index: 237, Loss: 3.3270\n",
      "Epoch: 10, Index: 238, Loss: 2.8237\n",
      "Epoch: 10, Index: 239, Loss: 2.3819\n",
      "Epoch: 10, Index: 240, Loss: 2.1545\n",
      "Epoch: 10, Index: 241, Loss: 1.2261\n",
      "Epoch: 10, Index: 242, Loss: 0.0205\n",
      "Epoch: 10, Index: 243, Loss: 0.6360\n",
      "Epoch: 10, Index: 244, Loss: 1.1209\n",
      "Epoch: 10, Index: 245, Loss: 6.5614\n",
      "Epoch: 10, Index: 246, Loss: 0.6624\n",
      "Epoch: 10, Index: 247, Loss: 0.1500\n",
      "Epoch: 10, Index: 248, Loss: 2.3841\n",
      "Epoch: 10, Index: 249, Loss: 3.6179\n",
      "Epoch: 10, Index: 250, Loss: 0.1401\n",
      "Epoch: 10, Index: 251, Loss: 6.0325\n",
      "Epoch: 10, Index: 252, Loss: 0.1856\n",
      "Epoch: 10, Index: 253, Loss: 4.2141\n",
      "Epoch: 10, Index: 254, Loss: 1.3116\n",
      "Epoch: 10, Index: 255, Loss: 1.5678\n",
      "Epoch: 10, Index: 256, Loss: 0.3944\n",
      "Epoch: 10, Index: 257, Loss: 2.3889\n",
      "Epoch: 10, Index: 258, Loss: 4.5464\n",
      "Epoch: 10, Index: 259, Loss: 0.8513\n",
      "Epoch: 10, Index: 260, Loss: 14.4341\n",
      "Epoch: 10, Index: 261, Loss: 1.1994\n",
      "Epoch: 10, Index: 262, Loss: 2.7357\n",
      "Epoch: 10, Index: 263, Loss: 1.8888\n",
      "Epoch: 10, Index: 264, Loss: 1.0407\n",
      "Epoch: 10, Index: 265, Loss: 0.8299\n",
      "Epoch: 10, Index: 266, Loss: 0.1125\n",
      "Epoch: 10, Index: 267, Loss: 1.7877\n",
      "Epoch: 10, Index: 268, Loss: 1.0984\n",
      "Epoch: 10, Index: 269, Loss: 0.1217\n",
      "Epoch: 10, Index: 270, Loss: 0.1596\n",
      "Epoch: 10, Index: 271, Loss: 0.1361\n",
      "Epoch: 10, Index: 272, Loss: 1.4331\n",
      "Epoch: 10, Index: 273, Loss: 0.9971\n",
      "Epoch: 10, Index: 274, Loss: 3.2064\n",
      "Epoch: 10, Index: 275, Loss: 1.7109\n",
      "Epoch: 10, Index: 276, Loss: 0.0169\n",
      "Epoch: 10, Index: 277, Loss: 7.5502\n",
      "Epoch: 10, Index: 278, Loss: 7.2423\n",
      "Epoch: 10, Index: 279, Loss: 1.4427\n",
      "Epoch: 10, Index: 280, Loss: 0.1314\n",
      "Epoch: 10, Index: 281, Loss: 1.0100\n",
      "Epoch: 10, Index: 282, Loss: 1.7974\n",
      "Epoch: 10, Index: 283, Loss: 2.8073\n",
      "Epoch: 10, Index: 284, Loss: 0.0164\n",
      "Epoch: 10, Index: 285, Loss: 1.8716\n",
      "Epoch: 10, Index: 286, Loss: 4.1855\n",
      "Epoch: 10, Index: 287, Loss: 0.3170\n",
      "Epoch: 10, Index: 288, Loss: 1.2508\n",
      "Epoch: 10, Index: 289, Loss: 1.0100\n",
      "Epoch: 10, Index: 290, Loss: 1.1335\n",
      "Epoch: 10, Index: 291, Loss: 3.2825\n",
      "Epoch: 10, Index: 292, Loss: 0.0822\n",
      "Epoch: 10, Index: 293, Loss: 0.2285\n",
      "Epoch: 10, Index: 294, Loss: 0.0078\n",
      "Epoch: 10, Index: 295, Loss: 2.2422\n",
      "Epoch: 10, Index: 296, Loss: 0.1387\n",
      "Epoch: 10, Index: 297, Loss: 2.4702\n",
      "Epoch: 10, Index: 298, Loss: 0.1640\n",
      "Epoch: 10, Index: 299, Loss: 1.4940\n",
      "Epoch: 10, Index: 300, Loss: 1.8056\n",
      "Epoch: 10, Index: 301, Loss: 1.6548\n",
      "Epoch: 10, Index: 302, Loss: 0.1676\n",
      "Epoch: 10, Index: 303, Loss: 1.2202\n",
      "Epoch: 10, Index: 304, Loss: 7.3038\n",
      "Epoch: 10, Index: 305, Loss: 0.3415\n",
      "Epoch: 10, Index: 306, Loss: 3.9829\n",
      "Epoch: 10, Index: 307, Loss: 0.9253\n",
      "Epoch: 10, Index: 308, Loss: 0.3651\n",
      "Epoch: 10, Index: 309, Loss: 1.0914\n",
      "Epoch: 10, Index: 310, Loss: 0.2940\n",
      "Epoch: 10, Index: 311, Loss: 0.3429\n",
      "Epoch: 10, Index: 312, Loss: 2.2527\n",
      "Epoch: 10, Index: 313, Loss: 2.5443\n",
      "Epoch: 10, Index: 314, Loss: 3.3437\n",
      "Epoch: 10, Index: 315, Loss: 2.0385\n",
      "Epoch: 10, Index: 316, Loss: 0.4729\n",
      "Epoch: 10, Index: 317, Loss: 0.5416\n",
      "Epoch: 10, Index: 318, Loss: 0.1376\n",
      "Epoch: 10, Index: 319, Loss: 0.7321\n",
      "Epoch: 10, Index: 320, Loss: 3.4571\n",
      "Epoch: 10, Index: 321, Loss: 1.5617\n",
      "Epoch: 10, Index: 322, Loss: 1.7881\n",
      "Epoch: 10, Index: 323, Loss: 0.0015\n",
      "Epoch: 10, Index: 324, Loss: 0.8219\n",
      "Epoch: 10, Index: 325, Loss: 0.8782\n",
      "Epoch: 10, Index: 326, Loss: 5.0514\n",
      "Epoch: 10, Index: 327, Loss: 0.5350\n",
      "Epoch: 10, Index: 328, Loss: 0.7117\n",
      "Epoch: 10, Index: 329, Loss: 0.5784\n",
      "Epoch: 10, Index: 330, Loss: 1.0799\n",
      "Epoch: 10, Index: 331, Loss: 3.3049\n",
      "Epoch: 10, Index: 332, Loss: 0.4020\n",
      "Epoch: 10, Index: 333, Loss: 0.2798\n",
      "Epoch: 10, Index: 334, Loss: 2.4663\n",
      "Epoch: 10, Index: 335, Loss: 4.9852\n",
      "Epoch: 10, Index: 336, Loss: 1.8662\n",
      "Epoch: 10, Index: 337, Loss: 2.9342\n",
      "Epoch: 10, Index: 338, Loss: 0.1881\n",
      "Epoch: 10, Index: 339, Loss: 0.0843\n",
      "Epoch: 10, Index: 340, Loss: 1.9061\n",
      "Epoch: 10, Index: 341, Loss: 2.9640\n",
      "Epoch: 10, Index: 342, Loss: 1.2289\n",
      "Epoch: 10, Index: 343, Loss: 2.1455\n",
      "Epoch: 10, Index: 344, Loss: 2.8459\n",
      "Epoch: 10, Index: 345, Loss: 3.5163\n",
      "Epoch: 10, Index: 346, Loss: 0.8721\n",
      "Epoch: 10, Index: 347, Loss: 0.8789\n",
      "Epoch: 10, Index: 348, Loss: 3.1942\n",
      "Epoch: 10, Index: 349, Loss: 2.2025\n",
      "Epoch: 10, Index: 350, Loss: 3.2778\n",
      "Epoch: 10, Index: 351, Loss: 1.6825\n",
      "Epoch: 10, Index: 352, Loss: 0.6468\n",
      "Epoch: 10, Index: 353, Loss: 0.2776\n",
      "Epoch: 10, Index: 354, Loss: 3.6846\n",
      "Epoch: 10, Index: 355, Loss: 1.3570\n",
      "Epoch: 10, Index: 356, Loss: 3.2927\n",
      "Epoch: 10, Index: 357, Loss: 1.2771\n",
      "Epoch: 10, Index: 358, Loss: 0.5085\n",
      "Epoch: 10, Index: 359, Loss: 11.7145\n",
      "Epoch: 10, Index: 360, Loss: 0.8651\n",
      "Epoch: 10, Index: 361, Loss: 0.4368\n",
      "Epoch: 10, Index: 362, Loss: 0.2426\n",
      "Epoch: 10, Index: 363, Loss: 2.7697\n",
      "Epoch: 10, Index: 364, Loss: 1.1259\n",
      "Epoch: 10, Index: 365, Loss: 1.7775\n",
      "Epoch: 10, Index: 366, Loss: 2.6498\n",
      "Epoch: 10, Index: 367, Loss: 0.8171\n",
      "Epoch: 10, Index: 368, Loss: 0.8035\n",
      "Epoch: 10, Index: 369, Loss: 0.1743\n",
      "Epoch: 10, Index: 370, Loss: 1.7975\n",
      "Epoch: 10, Index: 371, Loss: 5.4774\n",
      "Epoch: 10, Index: 372, Loss: 0.1235\n",
      "Epoch: 10, Index: 373, Loss: 0.9741\n",
      "Epoch: 10, Index: 374, Loss: 0.9018\n",
      "Epoch: 10, Index: 375, Loss: 6.4360\n",
      "Epoch: 10, Index: 376, Loss: 4.0856\n",
      "Epoch: 10, Index: 377, Loss: 0.6302\n",
      "Epoch: 10, Index: 378, Loss: 0.3854\n",
      "Epoch: 10, Index: 379, Loss: 2.2855\n",
      "Epoch: 10, Index: 380, Loss: 0.1761\n",
      "Epoch: 10, Index: 381, Loss: 0.3766\n",
      "Epoch: 10, Index: 382, Loss: 0.6914\n",
      "Epoch: 10, Index: 383, Loss: 2.9767\n",
      "Epoch: 10, Index: 384, Loss: 0.6653\n",
      "Epoch: 10, Index: 385, Loss: 0.2181\n",
      "Epoch: 10, Index: 386, Loss: 1.8742\n",
      "Epoch: 10, Index: 387, Loss: 0.8411\n",
      "Epoch: 10, Index: 388, Loss: 2.2682\n",
      "Epoch: 10, Index: 389, Loss: 0.7006\n",
      "Epoch: 10, Index: 390, Loss: 1.6364\n",
      "Epoch: 10, Index: 391, Loss: 0.5727\n",
      "Epoch: 10, Index: 392, Loss: 5.1599\n",
      "Epoch: 10, Index: 393, Loss: 4.5117\n",
      "Epoch: 10, Index: 394, Loss: 1.4793\n",
      "Epoch: 10, Index: 395, Loss: 1.6400\n",
      "Epoch: 10, Index: 396, Loss: 0.3423\n",
      "Epoch: 10, Index: 397, Loss: 0.1736\n",
      "Epoch: 10, Index: 398, Loss: 1.9185\n",
      "Epoch: 10, Index: 399, Loss: 1.2132\n",
      "Epoch: 10, Index: 400, Loss: 0.3353\n",
      "Epoch: 10, Index: 401, Loss: 2.5870\n",
      "Epoch: 10, Index: 402, Loss: 0.3401\n",
      "Epoch: 10, Index: 403, Loss: 0.2861\n",
      "Epoch: 10, Index: 404, Loss: 1.6119\n",
      "Epoch: 10, Index: 405, Loss: 1.2066\n",
      "Epoch: 10, Index: 406, Loss: 2.0206\n",
      "Epoch: 10, Index: 407, Loss: 1.9095\n",
      "Epoch: 10, Index: 408, Loss: 0.9503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ba562126ef4302834284d8faa130d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Index: 0, Loss: 2.9826\n",
      "Epoch: 11, Index: 1, Loss: 1.2314\n",
      "Epoch: 11, Index: 2, Loss: 0.8461\n",
      "Epoch: 11, Index: 3, Loss: 4.9877\n",
      "Epoch: 11, Index: 4, Loss: 3.3497\n",
      "Epoch: 11, Index: 5, Loss: 0.1599\n",
      "Epoch: 11, Index: 6, Loss: 0.0597\n",
      "Epoch: 11, Index: 7, Loss: 1.2629\n",
      "Epoch: 11, Index: 8, Loss: 1.9927\n",
      "Epoch: 11, Index: 9, Loss: 2.3265\n",
      "Epoch: 11, Index: 10, Loss: 4.9495\n",
      "Epoch: 11, Index: 11, Loss: 0.5086\n",
      "Epoch: 11, Index: 12, Loss: 2.3764\n",
      "Epoch: 11, Index: 13, Loss: 2.9746\n",
      "Epoch: 11, Index: 14, Loss: 0.4560\n",
      "Epoch: 11, Index: 15, Loss: 0.9288\n",
      "Epoch: 11, Index: 16, Loss: 16.6098\n",
      "Epoch: 11, Index: 17, Loss: 1.2834\n",
      "Epoch: 11, Index: 18, Loss: 3.9123\n",
      "Epoch: 11, Index: 19, Loss: 2.2543\n",
      "Epoch: 11, Index: 20, Loss: 1.7032\n",
      "Epoch: 11, Index: 21, Loss: 0.1631\n",
      "Epoch: 11, Index: 22, Loss: 0.1988\n",
      "Epoch: 11, Index: 23, Loss: 5.0561\n",
      "Epoch: 11, Index: 24, Loss: 0.9583\n",
      "Epoch: 11, Index: 25, Loss: 2.9252\n",
      "Epoch: 11, Index: 26, Loss: 2.1485\n",
      "Epoch: 11, Index: 27, Loss: 0.3227\n",
      "Epoch: 11, Index: 28, Loss: 0.0032\n",
      "Epoch: 11, Index: 29, Loss: 2.3532\n",
      "Epoch: 11, Index: 30, Loss: 0.1634\n",
      "Epoch: 11, Index: 31, Loss: 0.4383\n",
      "Epoch: 11, Index: 32, Loss: 0.1975\n",
      "Epoch: 11, Index: 33, Loss: 0.0444\n",
      "Epoch: 11, Index: 34, Loss: 1.6098\n",
      "Epoch: 11, Index: 35, Loss: 1.9033\n",
      "Epoch: 11, Index: 36, Loss: 3.7683\n",
      "Epoch: 11, Index: 37, Loss: 0.6604\n",
      "Epoch: 11, Index: 38, Loss: 2.7067\n",
      "Epoch: 11, Index: 39, Loss: 1.0533\n",
      "Epoch: 11, Index: 40, Loss: 0.2156\n",
      "Epoch: 11, Index: 41, Loss: 0.6230\n",
      "Epoch: 11, Index: 42, Loss: 0.9905\n",
      "Epoch: 11, Index: 43, Loss: 1.0256\n",
      "Epoch: 11, Index: 44, Loss: 3.9874\n",
      "Epoch: 11, Index: 45, Loss: 0.3765\n",
      "Epoch: 11, Index: 46, Loss: 1.7247\n",
      "Epoch: 11, Index: 47, Loss: 1.3966\n",
      "Epoch: 11, Index: 48, Loss: 0.6979\n",
      "Epoch: 11, Index: 49, Loss: 0.5686\n",
      "Epoch: 11, Index: 50, Loss: 1.3526\n",
      "Epoch: 11, Index: 51, Loss: 0.1486\n",
      "Epoch: 11, Index: 52, Loss: 2.6698\n",
      "Epoch: 11, Index: 53, Loss: 1.1282\n",
      "Epoch: 11, Index: 54, Loss: 1.6457\n",
      "Epoch: 11, Index: 55, Loss: 3.1395\n",
      "Epoch: 11, Index: 56, Loss: 0.1660\n",
      "Epoch: 11, Index: 57, Loss: 3.7302\n",
      "Epoch: 11, Index: 58, Loss: 0.8464\n",
      "Epoch: 11, Index: 59, Loss: 2.1058\n",
      "Epoch: 11, Index: 60, Loss: 0.4948\n",
      "Epoch: 11, Index: 61, Loss: 2.0193\n",
      "Epoch: 11, Index: 62, Loss: 0.3284\n",
      "Epoch: 11, Index: 63, Loss: 0.6641\n",
      "Epoch: 11, Index: 64, Loss: 0.8121\n",
      "Epoch: 11, Index: 65, Loss: 0.7965\n",
      "Epoch: 11, Index: 66, Loss: 1.0332\n",
      "Epoch: 11, Index: 67, Loss: 0.0185\n",
      "Epoch: 11, Index: 68, Loss: 2.0679\n",
      "Epoch: 11, Index: 69, Loss: 3.2307\n",
      "Epoch: 11, Index: 70, Loss: 0.6446\n",
      "Epoch: 11, Index: 71, Loss: 0.5769\n",
      "Epoch: 11, Index: 72, Loss: 0.8533\n",
      "Epoch: 11, Index: 73, Loss: 3.3479\n",
      "Epoch: 11, Index: 74, Loss: 0.2317\n",
      "Epoch: 11, Index: 75, Loss: 1.3616\n",
      "Epoch: 11, Index: 76, Loss: 0.1103\n",
      "Epoch: 11, Index: 77, Loss: 5.5830\n",
      "Epoch: 11, Index: 78, Loss: 0.3237\n",
      "Epoch: 11, Index: 79, Loss: 3.2495\n",
      "Epoch: 11, Index: 80, Loss: 0.8494\n",
      "Epoch: 11, Index: 81, Loss: 0.6091\n",
      "Epoch: 11, Index: 82, Loss: 0.2175\n",
      "Epoch: 11, Index: 83, Loss: 1.0352\n",
      "Epoch: 11, Index: 84, Loss: 3.6035\n",
      "Epoch: 11, Index: 85, Loss: 2.1081\n",
      "Epoch: 11, Index: 86, Loss: 2.5750\n",
      "Epoch: 11, Index: 87, Loss: 0.2117\n",
      "Epoch: 11, Index: 88, Loss: 1.8652\n",
      "Epoch: 11, Index: 89, Loss: 0.0929\n",
      "Epoch: 11, Index: 90, Loss: 1.3618\n",
      "Epoch: 11, Index: 91, Loss: 0.3878\n",
      "Epoch: 11, Index: 92, Loss: 3.1357\n",
      "Epoch: 11, Index: 93, Loss: 0.5052\n",
      "Epoch: 11, Index: 94, Loss: 1.2898\n",
      "Epoch: 11, Index: 95, Loss: 0.6336\n",
      "Epoch: 11, Index: 96, Loss: 0.0542\n",
      "Epoch: 11, Index: 97, Loss: 1.9788\n",
      "Epoch: 11, Index: 98, Loss: 0.4743\n",
      "Epoch: 11, Index: 99, Loss: 1.5953\n",
      "Epoch: 11, Index: 100, Loss: 3.2829\n",
      "Epoch: 11, Index: 101, Loss: 1.8986\n",
      "Epoch: 11, Index: 102, Loss: 0.4809\n",
      "Epoch: 11, Index: 103, Loss: 1.2261\n",
      "Epoch: 11, Index: 104, Loss: 0.0031\n",
      "Epoch: 11, Index: 105, Loss: 1.5576\n",
      "Epoch: 11, Index: 106, Loss: 0.2605\n",
      "Epoch: 11, Index: 107, Loss: 9.5637\n",
      "Epoch: 11, Index: 108, Loss: 1.2264\n",
      "Epoch: 11, Index: 109, Loss: 3.5864\n",
      "Epoch: 11, Index: 110, Loss: 1.1842\n",
      "Epoch: 11, Index: 111, Loss: 1.6635\n",
      "Epoch: 11, Index: 112, Loss: 0.5207\n",
      "Epoch: 11, Index: 113, Loss: 0.5473\n",
      "Epoch: 11, Index: 114, Loss: 1.1517\n",
      "Epoch: 11, Index: 115, Loss: 2.1105\n",
      "Epoch: 11, Index: 116, Loss: 1.0268\n",
      "Epoch: 11, Index: 117, Loss: 0.7872\n",
      "Epoch: 11, Index: 118, Loss: 0.5741\n",
      "Epoch: 11, Index: 119, Loss: 1.6343\n",
      "Epoch: 11, Index: 120, Loss: 1.1249\n",
      "Epoch: 11, Index: 121, Loss: 1.2644\n",
      "Epoch: 11, Index: 122, Loss: 0.9258\n",
      "Epoch: 11, Index: 123, Loss: 1.7420\n",
      "Epoch: 11, Index: 124, Loss: 0.1932\n",
      "Epoch: 11, Index: 125, Loss: 1.0854\n",
      "Epoch: 11, Index: 126, Loss: 1.0960\n",
      "Epoch: 11, Index: 127, Loss: 0.9440\n",
      "Epoch: 11, Index: 128, Loss: 0.2130\n",
      "Epoch: 11, Index: 129, Loss: 3.1521\n",
      "Epoch: 11, Index: 130, Loss: 2.8715\n",
      "Epoch: 11, Index: 131, Loss: 1.1087\n",
      "Epoch: 11, Index: 132, Loss: 2.2502\n",
      "Epoch: 11, Index: 133, Loss: 2.4697\n",
      "Epoch: 11, Index: 134, Loss: 1.6026\n",
      "Epoch: 11, Index: 135, Loss: 1.3226\n",
      "Epoch: 11, Index: 136, Loss: 0.0391\n",
      "Epoch: 11, Index: 137, Loss: 0.4426\n",
      "Epoch: 11, Index: 138, Loss: 2.8036\n",
      "Epoch: 11, Index: 139, Loss: 0.9696\n",
      "Epoch: 11, Index: 140, Loss: 4.7913\n",
      "Epoch: 11, Index: 141, Loss: 0.6985\n",
      "Epoch: 11, Index: 142, Loss: 0.8557\n",
      "Epoch: 11, Index: 143, Loss: 1.2517\n",
      "Epoch: 11, Index: 144, Loss: 0.1950\n",
      "Epoch: 11, Index: 145, Loss: 1.4034\n",
      "Epoch: 11, Index: 146, Loss: 0.7365\n",
      "Epoch: 11, Index: 147, Loss: 0.6071\n",
      "Epoch: 11, Index: 148, Loss: 1.6444\n",
      "Epoch: 11, Index: 149, Loss: 0.7255\n",
      "Epoch: 11, Index: 150, Loss: 0.4908\n",
      "Epoch: 11, Index: 151, Loss: 0.3306\n",
      "Epoch: 11, Index: 152, Loss: 6.8383\n",
      "Epoch: 11, Index: 153, Loss: 1.8533\n",
      "Epoch: 11, Index: 154, Loss: 0.4306\n",
      "Epoch: 11, Index: 155, Loss: 0.5116\n",
      "Epoch: 11, Index: 156, Loss: 0.4052\n",
      "Epoch: 11, Index: 157, Loss: 0.0007\n",
      "Epoch: 11, Index: 158, Loss: 2.8477\n",
      "Epoch: 11, Index: 159, Loss: 0.2439\n",
      "Epoch: 11, Index: 160, Loss: 1.3614\n",
      "Epoch: 11, Index: 161, Loss: 1.2856\n",
      "Epoch: 11, Index: 162, Loss: 0.9083\n",
      "Epoch: 11, Index: 163, Loss: 2.8793\n",
      "Epoch: 11, Index: 164, Loss: 0.5019\n",
      "Epoch: 11, Index: 165, Loss: 2.0300\n",
      "Epoch: 11, Index: 166, Loss: 2.7228\n",
      "Epoch: 11, Index: 167, Loss: 3.5902\n",
      "Epoch: 11, Index: 168, Loss: 7.6143\n",
      "Epoch: 11, Index: 169, Loss: 2.3886\n",
      "Epoch: 11, Index: 170, Loss: 3.4719\n",
      "Epoch: 11, Index: 171, Loss: 1.8502\n",
      "Epoch: 11, Index: 172, Loss: 0.2408\n",
      "Epoch: 11, Index: 173, Loss: 1.4221\n",
      "Epoch: 11, Index: 174, Loss: 0.7891\n",
      "Epoch: 11, Index: 175, Loss: 0.3159\n",
      "Epoch: 11, Index: 176, Loss: 5.0343\n",
      "Epoch: 11, Index: 177, Loss: 1.4038\n",
      "Epoch: 11, Index: 178, Loss: 0.4107\n",
      "Epoch: 11, Index: 179, Loss: 0.0520\n",
      "Epoch: 11, Index: 180, Loss: 2.1665\n",
      "Epoch: 11, Index: 181, Loss: 3.4214\n",
      "Epoch: 11, Index: 182, Loss: 3.4394\n",
      "Epoch: 11, Index: 183, Loss: 0.0277\n",
      "Epoch: 11, Index: 184, Loss: 1.1360\n",
      "Epoch: 11, Index: 185, Loss: 2.0128\n",
      "Epoch: 11, Index: 186, Loss: 0.5898\n",
      "Epoch: 11, Index: 187, Loss: 1.7965\n",
      "Epoch: 11, Index: 188, Loss: 0.8175\n",
      "Epoch: 11, Index: 189, Loss: 2.4551\n",
      "Epoch: 11, Index: 190, Loss: 0.9790\n",
      "Epoch: 11, Index: 191, Loss: 3.7197\n",
      "Epoch: 11, Index: 192, Loss: 0.3452\n",
      "Epoch: 11, Index: 193, Loss: 0.9636\n",
      "Epoch: 11, Index: 194, Loss: 2.2234\n",
      "Epoch: 11, Index: 195, Loss: 2.7999\n",
      "Epoch: 11, Index: 196, Loss: 0.0226\n",
      "Epoch: 11, Index: 197, Loss: 0.0324\n",
      "Epoch: 11, Index: 198, Loss: 2.8870\n",
      "Epoch: 11, Index: 199, Loss: 0.3034\n",
      "Epoch: 11, Index: 200, Loss: 2.7407\n",
      "Epoch: 11, Index: 201, Loss: 0.5011\n",
      "Epoch: 11, Index: 202, Loss: 2.4889\n",
      "Epoch: 11, Index: 203, Loss: 0.5520\n",
      "Epoch: 11, Index: 204, Loss: 2.0115\n",
      "Epoch: 11, Index: 205, Loss: 0.3720\n",
      "Epoch: 11, Index: 206, Loss: 1.2789\n",
      "Epoch: 11, Index: 207, Loss: 2.2840\n",
      "Epoch: 11, Index: 208, Loss: 4.9804\n",
      "Epoch: 11, Index: 209, Loss: 2.0445\n",
      "Epoch: 11, Index: 210, Loss: 2.7556\n",
      "Epoch: 11, Index: 211, Loss: 6.5364\n",
      "Epoch: 11, Index: 212, Loss: 5.6783\n",
      "Epoch: 11, Index: 213, Loss: 0.1061\n",
      "Epoch: 11, Index: 214, Loss: 4.4330\n",
      "Epoch: 11, Index: 215, Loss: 0.5259\n",
      "Epoch: 11, Index: 216, Loss: 2.0659\n",
      "Epoch: 11, Index: 217, Loss: 0.1922\n",
      "Epoch: 11, Index: 218, Loss: 0.1243\n",
      "Epoch: 11, Index: 219, Loss: 3.8167\n",
      "Epoch: 11, Index: 220, Loss: 0.3755\n",
      "Epoch: 11, Index: 221, Loss: 0.2414\n",
      "Epoch: 11, Index: 222, Loss: 3.1226\n",
      "Epoch: 11, Index: 223, Loss: 0.6757\n",
      "Epoch: 11, Index: 224, Loss: 1.5761\n",
      "Epoch: 11, Index: 225, Loss: 0.6562\n",
      "Epoch: 11, Index: 226, Loss: 0.0562\n",
      "Epoch: 11, Index: 227, Loss: 1.4086\n",
      "Epoch: 11, Index: 228, Loss: 2.7632\n",
      "Epoch: 11, Index: 229, Loss: 1.0130\n",
      "Epoch: 11, Index: 230, Loss: 0.6192\n",
      "Epoch: 11, Index: 231, Loss: 0.8645\n",
      "Epoch: 11, Index: 232, Loss: 2.0281\n",
      "Epoch: 11, Index: 233, Loss: 0.1135\n",
      "Epoch: 11, Index: 234, Loss: 0.5833\n",
      "Epoch: 11, Index: 235, Loss: 0.4559\n",
      "Epoch: 11, Index: 236, Loss: 0.0515\n",
      "Epoch: 11, Index: 237, Loss: 2.6086\n",
      "Epoch: 11, Index: 238, Loss: 1.3898\n",
      "Epoch: 11, Index: 239, Loss: 1.4425\n",
      "Epoch: 11, Index: 240, Loss: 2.6477\n",
      "Epoch: 11, Index: 241, Loss: 0.9522\n",
      "Epoch: 11, Index: 242, Loss: 0.6113\n",
      "Epoch: 11, Index: 243, Loss: 9.7707\n",
      "Epoch: 11, Index: 244, Loss: 0.4490\n",
      "Epoch: 11, Index: 245, Loss: 8.0663\n",
      "Epoch: 11, Index: 246, Loss: 6.7419\n",
      "Epoch: 11, Index: 247, Loss: 0.7823\n",
      "Epoch: 11, Index: 248, Loss: 0.7418\n",
      "Epoch: 11, Index: 249, Loss: 0.7530\n",
      "Epoch: 11, Index: 250, Loss: 0.3910\n",
      "Epoch: 11, Index: 251, Loss: 0.3830\n",
      "Epoch: 11, Index: 252, Loss: 2.3220\n",
      "Epoch: 11, Index: 253, Loss: 4.3803\n",
      "Epoch: 11, Index: 254, Loss: 0.3203\n",
      "Epoch: 11, Index: 255, Loss: 0.5351\n",
      "Epoch: 11, Index: 256, Loss: 5.2706\n",
      "Epoch: 11, Index: 257, Loss: 3.7677\n",
      "Epoch: 11, Index: 258, Loss: 3.0449\n",
      "Epoch: 11, Index: 259, Loss: 0.9942\n",
      "Epoch: 11, Index: 260, Loss: 0.0962\n",
      "Epoch: 11, Index: 261, Loss: 0.8559\n",
      "Epoch: 11, Index: 262, Loss: 0.8411\n",
      "Epoch: 11, Index: 263, Loss: 0.8902\n",
      "Epoch: 11, Index: 264, Loss: 0.1657\n",
      "Epoch: 11, Index: 265, Loss: 0.6466\n",
      "Epoch: 11, Index: 266, Loss: 0.0717\n",
      "Epoch: 11, Index: 267, Loss: 1.1788\n",
      "Epoch: 11, Index: 268, Loss: 6.6705\n",
      "Epoch: 11, Index: 269, Loss: 0.8205\n",
      "Epoch: 11, Index: 270, Loss: 0.2461\n",
      "Epoch: 11, Index: 271, Loss: 0.7692\n",
      "Epoch: 11, Index: 272, Loss: 0.5375\n",
      "Epoch: 11, Index: 273, Loss: 1.8466\n",
      "Epoch: 11, Index: 274, Loss: 0.1378\n",
      "Epoch: 11, Index: 275, Loss: 3.7871\n",
      "Epoch: 11, Index: 276, Loss: 0.4489\n",
      "Epoch: 11, Index: 277, Loss: 6.8852\n",
      "Epoch: 11, Index: 278, Loss: 0.3532\n",
      "Epoch: 11, Index: 279, Loss: 1.6792\n",
      "Epoch: 11, Index: 280, Loss: 1.6941\n",
      "Epoch: 11, Index: 281, Loss: 0.6476\n",
      "Epoch: 11, Index: 282, Loss: 1.4192\n",
      "Epoch: 11, Index: 283, Loss: 0.9821\n",
      "Epoch: 11, Index: 284, Loss: 1.9271\n",
      "Epoch: 11, Index: 285, Loss: 1.2300\n",
      "Epoch: 11, Index: 286, Loss: 0.6179\n",
      "Epoch: 11, Index: 287, Loss: 1.4350\n",
      "Epoch: 11, Index: 288, Loss: 0.5841\n",
      "Epoch: 11, Index: 289, Loss: 0.5631\n",
      "Epoch: 11, Index: 290, Loss: 2.1412\n",
      "Epoch: 11, Index: 291, Loss: 0.6938\n",
      "Epoch: 11, Index: 292, Loss: 0.4056\n",
      "Epoch: 11, Index: 293, Loss: 0.2388\n",
      "Epoch: 11, Index: 294, Loss: 1.9547\n",
      "Epoch: 11, Index: 295, Loss: 1.1412\n",
      "Epoch: 11, Index: 296, Loss: 1.9512\n",
      "Epoch: 11, Index: 297, Loss: 3.7314\n",
      "Epoch: 11, Index: 298, Loss: 1.2341\n",
      "Epoch: 11, Index: 299, Loss: 0.3251\n",
      "Epoch: 11, Index: 300, Loss: 2.1115\n",
      "Epoch: 11, Index: 301, Loss: 3.3051\n",
      "Epoch: 11, Index: 302, Loss: 0.4824\n",
      "Epoch: 11, Index: 303, Loss: 0.0774\n",
      "Epoch: 11, Index: 304, Loss: 0.9511\n",
      "Epoch: 11, Index: 305, Loss: 0.0055\n",
      "Epoch: 11, Index: 306, Loss: 0.1050\n",
      "Epoch: 11, Index: 307, Loss: 3.7662\n",
      "Epoch: 11, Index: 308, Loss: 0.0108\n",
      "Epoch: 11, Index: 309, Loss: 3.1803\n",
      "Epoch: 11, Index: 310, Loss: 4.4794\n",
      "Epoch: 11, Index: 311, Loss: 0.8176\n",
      "Epoch: 11, Index: 312, Loss: 0.7588\n",
      "Epoch: 11, Index: 313, Loss: 4.2984\n",
      "Epoch: 11, Index: 314, Loss: 4.8018\n",
      "Epoch: 11, Index: 315, Loss: 1.4427\n",
      "Epoch: 11, Index: 316, Loss: 2.2646\n",
      "Epoch: 11, Index: 317, Loss: 2.2702\n",
      "Epoch: 11, Index: 318, Loss: 0.5584\n",
      "Epoch: 11, Index: 319, Loss: 0.8498\n",
      "Epoch: 11, Index: 320, Loss: 0.1847\n",
      "Epoch: 11, Index: 321, Loss: 7.0880\n",
      "Epoch: 11, Index: 322, Loss: 0.8904\n",
      "Epoch: 11, Index: 323, Loss: 1.1963\n",
      "Epoch: 11, Index: 324, Loss: 0.7260\n",
      "Epoch: 11, Index: 325, Loss: 5.0708\n",
      "Epoch: 11, Index: 326, Loss: 1.6065\n",
      "Epoch: 11, Index: 327, Loss: 2.4106\n",
      "Epoch: 11, Index: 328, Loss: 2.9222\n",
      "Epoch: 11, Index: 329, Loss: 3.0196\n",
      "Epoch: 11, Index: 330, Loss: 0.8596\n",
      "Epoch: 11, Index: 331, Loss: 2.8020\n",
      "Epoch: 11, Index: 332, Loss: 0.9317\n",
      "Epoch: 11, Index: 333, Loss: 1.3588\n",
      "Epoch: 11, Index: 334, Loss: 8.0424\n",
      "Epoch: 11, Index: 335, Loss: 3.0343\n",
      "Epoch: 11, Index: 336, Loss: 1.5227\n",
      "Epoch: 11, Index: 337, Loss: 0.4339\n",
      "Epoch: 11, Index: 338, Loss: 4.1390\n",
      "Epoch: 11, Index: 339, Loss: 0.3287\n",
      "Epoch: 11, Index: 340, Loss: 4.7352\n",
      "Epoch: 11, Index: 341, Loss: 5.4956\n",
      "Epoch: 11, Index: 342, Loss: 0.0393\n",
      "Epoch: 11, Index: 343, Loss: 0.0535\n",
      "Epoch: 11, Index: 344, Loss: 0.0169\n",
      "Epoch: 11, Index: 345, Loss: 1.1811\n",
      "Epoch: 11, Index: 346, Loss: 1.1370\n",
      "Epoch: 11, Index: 347, Loss: 4.0903\n",
      "Epoch: 11, Index: 348, Loss: 0.0431\n",
      "Epoch: 11, Index: 349, Loss: 4.3154\n",
      "Epoch: 11, Index: 350, Loss: 6.0297\n",
      "Epoch: 11, Index: 351, Loss: 14.2360\n",
      "Epoch: 11, Index: 352, Loss: 1.1176\n",
      "Epoch: 11, Index: 353, Loss: 1.8979\n",
      "Epoch: 11, Index: 354, Loss: 1.3194\n",
      "Epoch: 11, Index: 355, Loss: 1.7550\n",
      "Epoch: 11, Index: 356, Loss: 0.4205\n",
      "Epoch: 11, Index: 357, Loss: 0.2177\n",
      "Epoch: 11, Index: 358, Loss: 0.7569\n",
      "Epoch: 11, Index: 359, Loss: 0.9557\n",
      "Epoch: 11, Index: 360, Loss: 0.7209\n",
      "Epoch: 11, Index: 361, Loss: 2.9356\n",
      "Epoch: 11, Index: 362, Loss: 2.9321\n",
      "Epoch: 11, Index: 363, Loss: 0.8239\n",
      "Epoch: 11, Index: 364, Loss: 0.6562\n",
      "Epoch: 11, Index: 365, Loss: 2.8928\n",
      "Epoch: 11, Index: 366, Loss: 1.7781\n",
      "Epoch: 11, Index: 367, Loss: 1.3202\n",
      "Epoch: 11, Index: 368, Loss: 0.4104\n",
      "Epoch: 11, Index: 369, Loss: 0.5405\n",
      "Epoch: 11, Index: 370, Loss: 4.9001\n",
      "Epoch: 11, Index: 371, Loss: 0.7061\n",
      "Epoch: 11, Index: 372, Loss: 0.8758\n",
      "Epoch: 11, Index: 373, Loss: 1.0334\n",
      "Epoch: 11, Index: 374, Loss: 0.3054\n",
      "Epoch: 11, Index: 375, Loss: 0.3953\n",
      "Epoch: 11, Index: 376, Loss: 2.5132\n",
      "Epoch: 11, Index: 377, Loss: 3.8936\n",
      "Epoch: 11, Index: 378, Loss: 1.0361\n",
      "Epoch: 11, Index: 379, Loss: 0.9886\n",
      "Epoch: 11, Index: 380, Loss: 0.4414\n",
      "Epoch: 11, Index: 381, Loss: 2.3462\n",
      "Epoch: 11, Index: 382, Loss: 3.6867\n",
      "Epoch: 11, Index: 383, Loss: 2.2953\n",
      "Epoch: 11, Index: 384, Loss: 0.5825\n",
      "Epoch: 11, Index: 385, Loss: 3.8861\n",
      "Epoch: 11, Index: 386, Loss: 1.5224\n",
      "Epoch: 11, Index: 387, Loss: 1.5482\n",
      "Epoch: 11, Index: 388, Loss: 1.2115\n",
      "Epoch: 11, Index: 389, Loss: 0.8077\n",
      "Epoch: 11, Index: 390, Loss: 0.1599\n",
      "Epoch: 11, Index: 391, Loss: 1.7350\n",
      "Epoch: 11, Index: 392, Loss: 1.4328\n",
      "Epoch: 11, Index: 393, Loss: 6.0468\n",
      "Epoch: 11, Index: 394, Loss: 1.7740\n",
      "Epoch: 11, Index: 395, Loss: 8.2153\n",
      "Epoch: 11, Index: 396, Loss: 1.1621\n",
      "Epoch: 11, Index: 397, Loss: 1.7800\n",
      "Epoch: 11, Index: 398, Loss: 5.6839\n",
      "Epoch: 11, Index: 399, Loss: 0.8032\n",
      "Epoch: 11, Index: 400, Loss: 3.4104\n",
      "Epoch: 11, Index: 401, Loss: 5.1762\n",
      "Epoch: 11, Index: 402, Loss: 1.4735\n",
      "Epoch: 11, Index: 403, Loss: 0.2776\n",
      "Epoch: 11, Index: 404, Loss: 0.7162\n",
      "Epoch: 11, Index: 405, Loss: 1.5380\n",
      "Epoch: 11, Index: 406, Loss: 3.0621\n",
      "Epoch: 11, Index: 407, Loss: 0.8619\n",
      "Epoch: 11, Index: 408, Loss: 0.0504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ef6345018e4ffa80695ed2901a8a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Index: 0, Loss: 0.4151\n",
      "Epoch: 12, Index: 1, Loss: 0.3828\n",
      "Epoch: 12, Index: 2, Loss: 7.2620\n",
      "Epoch: 12, Index: 3, Loss: 0.2050\n",
      "Epoch: 12, Index: 4, Loss: 1.4098\n",
      "Epoch: 12, Index: 5, Loss: 1.1507\n",
      "Epoch: 12, Index: 6, Loss: 3.1071\n",
      "Epoch: 12, Index: 7, Loss: 1.7592\n",
      "Epoch: 12, Index: 8, Loss: 5.4119\n",
      "Epoch: 12, Index: 9, Loss: 4.0003\n",
      "Epoch: 12, Index: 10, Loss: 2.6041\n",
      "Epoch: 12, Index: 11, Loss: 1.6734\n",
      "Epoch: 12, Index: 12, Loss: 2.7339\n",
      "Epoch: 12, Index: 13, Loss: 1.8706\n",
      "Epoch: 12, Index: 14, Loss: 0.9634\n",
      "Epoch: 12, Index: 15, Loss: 1.5005\n",
      "Epoch: 12, Index: 16, Loss: 0.1983\n",
      "Epoch: 12, Index: 17, Loss: 0.6532\n",
      "Epoch: 12, Index: 18, Loss: 3.3226\n",
      "Epoch: 12, Index: 19, Loss: 0.4877\n",
      "Epoch: 12, Index: 20, Loss: 3.5158\n",
      "Epoch: 12, Index: 21, Loss: 2.0767\n",
      "Epoch: 12, Index: 22, Loss: 1.4998\n",
      "Epoch: 12, Index: 23, Loss: 3.5125\n",
      "Epoch: 12, Index: 24, Loss: 2.9160\n",
      "Epoch: 12, Index: 25, Loss: 1.7249\n",
      "Epoch: 12, Index: 26, Loss: 0.1359\n",
      "Epoch: 12, Index: 27, Loss: 0.1607\n",
      "Epoch: 12, Index: 28, Loss: 0.6927\n",
      "Epoch: 12, Index: 29, Loss: 1.3822\n",
      "Epoch: 12, Index: 30, Loss: 1.8987\n",
      "Epoch: 12, Index: 31, Loss: 1.0219\n",
      "Epoch: 12, Index: 32, Loss: 2.8287\n",
      "Epoch: 12, Index: 33, Loss: 0.0697\n",
      "Epoch: 12, Index: 34, Loss: 1.1243\n",
      "Epoch: 12, Index: 35, Loss: 9.9165\n",
      "Epoch: 12, Index: 36, Loss: 0.7956\n",
      "Epoch: 12, Index: 37, Loss: 0.9465\n",
      "Epoch: 12, Index: 38, Loss: 0.8807\n",
      "Epoch: 12, Index: 39, Loss: 0.8931\n",
      "Epoch: 12, Index: 40, Loss: 0.1259\n",
      "Epoch: 12, Index: 41, Loss: 0.8204\n",
      "Epoch: 12, Index: 42, Loss: 3.1164\n",
      "Epoch: 12, Index: 43, Loss: 0.2096\n",
      "Epoch: 12, Index: 44, Loss: 0.0984\n",
      "Epoch: 12, Index: 45, Loss: 0.4255\n",
      "Epoch: 12, Index: 46, Loss: 1.5010\n",
      "Epoch: 12, Index: 47, Loss: 4.3693\n",
      "Epoch: 12, Index: 48, Loss: 2.9756\n",
      "Epoch: 12, Index: 49, Loss: 0.3113\n",
      "Epoch: 12, Index: 50, Loss: 0.4117\n",
      "Epoch: 12, Index: 51, Loss: 3.4870\n",
      "Epoch: 12, Index: 52, Loss: 0.4281\n",
      "Epoch: 12, Index: 53, Loss: 0.4084\n",
      "Epoch: 12, Index: 54, Loss: 4.4501\n",
      "Epoch: 12, Index: 55, Loss: 0.6989\n",
      "Epoch: 12, Index: 56, Loss: 2.1485\n",
      "Epoch: 12, Index: 57, Loss: 1.4171\n",
      "Epoch: 12, Index: 58, Loss: 1.0293\n",
      "Epoch: 12, Index: 59, Loss: 1.0538\n",
      "Epoch: 12, Index: 60, Loss: 0.1331\n",
      "Epoch: 12, Index: 61, Loss: 1.4437\n",
      "Epoch: 12, Index: 62, Loss: 1.3574\n",
      "Epoch: 12, Index: 63, Loss: 1.2350\n",
      "Epoch: 12, Index: 64, Loss: 0.2312\n",
      "Epoch: 12, Index: 65, Loss: 0.3284\n",
      "Epoch: 12, Index: 66, Loss: 0.0842\n",
      "Epoch: 12, Index: 67, Loss: 7.4012\n",
      "Epoch: 12, Index: 68, Loss: 3.3058\n",
      "Epoch: 12, Index: 69, Loss: 0.8515\n",
      "Epoch: 12, Index: 70, Loss: 0.0047\n",
      "Epoch: 12, Index: 71, Loss: 0.1020\n",
      "Epoch: 12, Index: 72, Loss: 0.8648\n",
      "Epoch: 12, Index: 73, Loss: 1.0593\n",
      "Epoch: 12, Index: 74, Loss: 0.2814\n",
      "Epoch: 12, Index: 75, Loss: 4.3398\n",
      "Epoch: 12, Index: 76, Loss: 1.4534\n",
      "Epoch: 12, Index: 77, Loss: 0.6691\n",
      "Epoch: 12, Index: 78, Loss: 1.6334\n",
      "Epoch: 12, Index: 79, Loss: 0.7428\n",
      "Epoch: 12, Index: 80, Loss: 0.0281\n",
      "Epoch: 12, Index: 81, Loss: 1.6673\n",
      "Epoch: 12, Index: 82, Loss: 6.7789\n",
      "Epoch: 12, Index: 83, Loss: 1.4781\n",
      "Epoch: 12, Index: 84, Loss: 1.5837\n",
      "Epoch: 12, Index: 85, Loss: 0.3686\n",
      "Epoch: 12, Index: 86, Loss: 0.3071\n",
      "Epoch: 12, Index: 87, Loss: 0.6126\n",
      "Epoch: 12, Index: 88, Loss: 0.3603\n",
      "Epoch: 12, Index: 89, Loss: 2.4662\n",
      "Epoch: 12, Index: 90, Loss: 0.9351\n",
      "Epoch: 12, Index: 91, Loss: 2.1629\n",
      "Epoch: 12, Index: 92, Loss: 1.6258\n",
      "Epoch: 12, Index: 93, Loss: 0.5354\n",
      "Epoch: 12, Index: 94, Loss: 2.6688\n",
      "Epoch: 12, Index: 95, Loss: 0.9726\n",
      "Epoch: 12, Index: 96, Loss: 0.8301\n",
      "Epoch: 12, Index: 97, Loss: 2.8282\n",
      "Epoch: 12, Index: 98, Loss: 0.6360\n",
      "Epoch: 12, Index: 99, Loss: 0.9806\n",
      "Epoch: 12, Index: 100, Loss: 1.5416\n",
      "Epoch: 12, Index: 101, Loss: 0.1691\n",
      "Epoch: 12, Index: 102, Loss: 0.9557\n",
      "Epoch: 12, Index: 103, Loss: 0.0493\n",
      "Epoch: 12, Index: 104, Loss: 4.2356\n",
      "Epoch: 12, Index: 105, Loss: 1.4243\n",
      "Epoch: 12, Index: 106, Loss: 0.7016\n",
      "Epoch: 12, Index: 107, Loss: 1.7923\n",
      "Epoch: 12, Index: 108, Loss: 0.4489\n",
      "Epoch: 12, Index: 109, Loss: 2.5525\n",
      "Epoch: 12, Index: 110, Loss: 3.9614\n",
      "Epoch: 12, Index: 111, Loss: 0.4623\n",
      "Epoch: 12, Index: 112, Loss: 1.0721\n",
      "Epoch: 12, Index: 113, Loss: 0.3741\n",
      "Epoch: 12, Index: 114, Loss: 1.4097\n",
      "Epoch: 12, Index: 115, Loss: 3.2214\n",
      "Epoch: 12, Index: 116, Loss: 3.2454\n",
      "Epoch: 12, Index: 117, Loss: 1.6938\n",
      "Epoch: 12, Index: 118, Loss: 1.5157\n",
      "Epoch: 12, Index: 119, Loss: 9.4960\n",
      "Epoch: 12, Index: 120, Loss: 1.3722\n",
      "Epoch: 12, Index: 121, Loss: 0.6949\n",
      "Epoch: 12, Index: 122, Loss: 1.2075\n",
      "Epoch: 12, Index: 123, Loss: 0.8816\n",
      "Epoch: 12, Index: 124, Loss: 6.5849\n",
      "Epoch: 12, Index: 125, Loss: 0.9955\n",
      "Epoch: 12, Index: 126, Loss: 1.4520\n",
      "Epoch: 12, Index: 127, Loss: 2.8922\n",
      "Epoch: 12, Index: 128, Loss: 5.4108\n",
      "Epoch: 12, Index: 129, Loss: 2.6026\n",
      "Epoch: 12, Index: 130, Loss: 1.0779\n",
      "Epoch: 12, Index: 131, Loss: 0.2938\n",
      "Epoch: 12, Index: 132, Loss: 0.9972\n",
      "Epoch: 12, Index: 133, Loss: 0.7151\n",
      "Epoch: 12, Index: 134, Loss: 0.5006\n",
      "Epoch: 12, Index: 135, Loss: 0.7269\n",
      "Epoch: 12, Index: 136, Loss: 0.4591\n",
      "Epoch: 12, Index: 137, Loss: 0.2630\n",
      "Epoch: 12, Index: 138, Loss: 2.5439\n",
      "Epoch: 12, Index: 139, Loss: 2.4358\n",
      "Epoch: 12, Index: 140, Loss: 6.0459\n",
      "Epoch: 12, Index: 141, Loss: 0.6300\n",
      "Epoch: 12, Index: 142, Loss: 5.6677\n",
      "Epoch: 12, Index: 143, Loss: 1.5589\n",
      "Epoch: 12, Index: 144, Loss: 1.6113\n",
      "Epoch: 12, Index: 145, Loss: 9.0446\n",
      "Epoch: 12, Index: 146, Loss: 0.5499\n",
      "Epoch: 12, Index: 147, Loss: 1.0002\n",
      "Epoch: 12, Index: 148, Loss: 2.2143\n",
      "Epoch: 12, Index: 149, Loss: 0.1002\n",
      "Epoch: 12, Index: 150, Loss: 0.0263\n",
      "Epoch: 12, Index: 151, Loss: 0.6630\n",
      "Epoch: 12, Index: 152, Loss: 0.3039\n",
      "Epoch: 12, Index: 153, Loss: 2.3576\n",
      "Epoch: 12, Index: 154, Loss: 9.3333\n",
      "Epoch: 12, Index: 155, Loss: 0.1531\n",
      "Epoch: 12, Index: 156, Loss: 4.9259\n",
      "Epoch: 12, Index: 157, Loss: 3.6943\n",
      "Epoch: 12, Index: 158, Loss: 0.7107\n",
      "Epoch: 12, Index: 159, Loss: 1.1543\n",
      "Epoch: 12, Index: 160, Loss: 1.4660\n",
      "Epoch: 12, Index: 161, Loss: 0.2293\n",
      "Epoch: 12, Index: 162, Loss: 0.6430\n",
      "Epoch: 12, Index: 163, Loss: 1.5259\n",
      "Epoch: 12, Index: 164, Loss: 1.4737\n",
      "Epoch: 12, Index: 165, Loss: 0.8272\n",
      "Epoch: 12, Index: 166, Loss: 7.8069\n",
      "Epoch: 12, Index: 167, Loss: 0.7767\n",
      "Epoch: 12, Index: 168, Loss: 2.9763\n",
      "Epoch: 12, Index: 169, Loss: 0.0463\n",
      "Epoch: 12, Index: 170, Loss: 0.3335\n",
      "Epoch: 12, Index: 171, Loss: 0.5596\n",
      "Epoch: 12, Index: 172, Loss: 5.5560\n",
      "Epoch: 12, Index: 173, Loss: 1.9348\n",
      "Epoch: 12, Index: 174, Loss: 2.5021\n",
      "Epoch: 12, Index: 175, Loss: 2.4186\n",
      "Epoch: 12, Index: 176, Loss: 0.5120\n",
      "Epoch: 12, Index: 177, Loss: 1.2610\n",
      "Epoch: 12, Index: 178, Loss: 1.5183\n",
      "Epoch: 12, Index: 179, Loss: 0.3312\n",
      "Epoch: 12, Index: 180, Loss: 0.6305\n",
      "Epoch: 12, Index: 181, Loss: 3.3438\n",
      "Epoch: 12, Index: 182, Loss: 0.2953\n",
      "Epoch: 12, Index: 183, Loss: 3.5199\n",
      "Epoch: 12, Index: 184, Loss: 1.6427\n",
      "Epoch: 12, Index: 185, Loss: 1.1307\n",
      "Epoch: 12, Index: 186, Loss: 2.2334\n",
      "Epoch: 12, Index: 187, Loss: 1.3466\n",
      "Epoch: 12, Index: 188, Loss: 1.6079\n",
      "Epoch: 12, Index: 189, Loss: 0.2284\n",
      "Epoch: 12, Index: 190, Loss: 0.4193\n",
      "Epoch: 12, Index: 191, Loss: 1.4315\n",
      "Epoch: 12, Index: 192, Loss: 1.5188\n",
      "Epoch: 12, Index: 193, Loss: 0.7669\n",
      "Epoch: 12, Index: 194, Loss: 0.6946\n",
      "Epoch: 12, Index: 195, Loss: 1.0541\n",
      "Epoch: 12, Index: 196, Loss: 2.3398\n",
      "Epoch: 12, Index: 197, Loss: 2.3980\n",
      "Epoch: 12, Index: 198, Loss: 5.7035\n",
      "Epoch: 12, Index: 199, Loss: 0.8254\n",
      "Epoch: 12, Index: 200, Loss: 1.7795\n",
      "Epoch: 12, Index: 201, Loss: 1.8855\n",
      "Epoch: 12, Index: 202, Loss: 0.0715\n",
      "Epoch: 12, Index: 203, Loss: 1.3495\n",
      "Epoch: 12, Index: 204, Loss: 1.6474\n",
      "Epoch: 12, Index: 205, Loss: 0.2632\n",
      "Epoch: 12, Index: 206, Loss: 0.7372\n",
      "Epoch: 12, Index: 207, Loss: 2.0273\n",
      "Epoch: 12, Index: 208, Loss: 6.2546\n",
      "Epoch: 12, Index: 209, Loss: 3.7239\n",
      "Epoch: 12, Index: 210, Loss: 0.0089\n",
      "Epoch: 12, Index: 211, Loss: 0.9821\n",
      "Epoch: 12, Index: 212, Loss: 1.1250\n",
      "Epoch: 12, Index: 213, Loss: 0.0126\n",
      "Epoch: 12, Index: 214, Loss: 0.2285\n",
      "Epoch: 12, Index: 215, Loss: 0.6176\n",
      "Epoch: 12, Index: 216, Loss: 0.2131\n",
      "Epoch: 12, Index: 217, Loss: 1.0734\n",
      "Epoch: 12, Index: 218, Loss: 0.3287\n",
      "Epoch: 12, Index: 219, Loss: 1.5558\n",
      "Epoch: 12, Index: 220, Loss: 1.5757\n",
      "Epoch: 12, Index: 221, Loss: 0.7751\n",
      "Epoch: 12, Index: 222, Loss: 0.5294\n",
      "Epoch: 12, Index: 223, Loss: 3.4110\n",
      "Epoch: 12, Index: 224, Loss: 0.5234\n",
      "Epoch: 12, Index: 225, Loss: 0.6332\n",
      "Epoch: 12, Index: 226, Loss: 2.0382\n",
      "Epoch: 12, Index: 227, Loss: 1.0595\n",
      "Epoch: 12, Index: 228, Loss: 4.7209\n",
      "Epoch: 12, Index: 229, Loss: 8.0496\n",
      "Epoch: 12, Index: 230, Loss: 0.2180\n",
      "Epoch: 12, Index: 231, Loss: 1.1487\n",
      "Epoch: 12, Index: 232, Loss: 4.9072\n",
      "Epoch: 12, Index: 233, Loss: 0.5157\n",
      "Epoch: 12, Index: 234, Loss: 3.0452\n",
      "Epoch: 12, Index: 235, Loss: 0.5305\n",
      "Epoch: 12, Index: 236, Loss: 0.4456\n",
      "Epoch: 12, Index: 237, Loss: 1.8878\n",
      "Epoch: 12, Index: 238, Loss: 5.3388\n",
      "Epoch: 12, Index: 239, Loss: 1.9209\n",
      "Epoch: 12, Index: 240, Loss: 0.8956\n",
      "Epoch: 12, Index: 241, Loss: 1.0079\n",
      "Epoch: 12, Index: 242, Loss: 0.8488\n",
      "Epoch: 12, Index: 243, Loss: 2.2323\n",
      "Epoch: 12, Index: 244, Loss: 2.3776\n",
      "Epoch: 12, Index: 245, Loss: 3.8255\n",
      "Epoch: 12, Index: 246, Loss: 0.2379\n",
      "Epoch: 12, Index: 247, Loss: 1.9937\n",
      "Epoch: 12, Index: 248, Loss: 2.0316\n",
      "Epoch: 12, Index: 249, Loss: 3.1728\n",
      "Epoch: 12, Index: 250, Loss: 1.0430\n",
      "Epoch: 12, Index: 251, Loss: 1.4633\n",
      "Epoch: 12, Index: 252, Loss: 3.7140\n",
      "Epoch: 12, Index: 253, Loss: 2.4623\n",
      "Epoch: 12, Index: 254, Loss: 1.9601\n",
      "Epoch: 12, Index: 255, Loss: 1.9905\n",
      "Epoch: 12, Index: 256, Loss: 1.7091\n",
      "Epoch: 12, Index: 257, Loss: 0.3321\n",
      "Epoch: 12, Index: 258, Loss: 4.6866\n",
      "Epoch: 12, Index: 259, Loss: 0.6696\n",
      "Epoch: 12, Index: 260, Loss: 1.5080\n",
      "Epoch: 12, Index: 261, Loss: 0.4128\n",
      "Epoch: 12, Index: 262, Loss: 1.3297\n",
      "Epoch: 12, Index: 263, Loss: 0.3126\n",
      "Epoch: 12, Index: 264, Loss: 3.3495\n",
      "Epoch: 12, Index: 265, Loss: 2.1627\n",
      "Epoch: 12, Index: 266, Loss: 1.1458\n",
      "Epoch: 12, Index: 267, Loss: 1.1149\n",
      "Epoch: 12, Index: 268, Loss: 2.1766\n",
      "Epoch: 12, Index: 269, Loss: 0.1873\n",
      "Epoch: 12, Index: 270, Loss: 1.7223\n",
      "Epoch: 12, Index: 271, Loss: 2.6517\n",
      "Epoch: 12, Index: 272, Loss: 4.1068\n",
      "Epoch: 12, Index: 273, Loss: 0.6206\n",
      "Epoch: 12, Index: 274, Loss: 2.1141\n",
      "Epoch: 12, Index: 275, Loss: 4.0860\n",
      "Epoch: 12, Index: 276, Loss: 1.1061\n",
      "Epoch: 12, Index: 277, Loss: 1.9318\n",
      "Epoch: 12, Index: 278, Loss: 1.1061\n",
      "Epoch: 12, Index: 279, Loss: 3.7275\n",
      "Epoch: 12, Index: 280, Loss: 0.3282\n",
      "Epoch: 12, Index: 281, Loss: 1.2664\n",
      "Epoch: 12, Index: 282, Loss: 0.9187\n",
      "Epoch: 12, Index: 283, Loss: 2.5206\n",
      "Epoch: 12, Index: 284, Loss: 1.2124\n",
      "Epoch: 12, Index: 285, Loss: 0.0355\n",
      "Epoch: 12, Index: 286, Loss: 2.0869\n",
      "Epoch: 12, Index: 287, Loss: 2.8822\n",
      "Epoch: 12, Index: 288, Loss: 1.5813\n",
      "Epoch: 12, Index: 289, Loss: 1.3962\n",
      "Epoch: 12, Index: 290, Loss: 2.6661\n",
      "Epoch: 12, Index: 291, Loss: 1.1555\n",
      "Epoch: 12, Index: 292, Loss: 0.5556\n",
      "Epoch: 12, Index: 293, Loss: 1.8830\n",
      "Epoch: 12, Index: 294, Loss: 4.0980\n",
      "Epoch: 12, Index: 295, Loss: 0.3597\n",
      "Epoch: 12, Index: 296, Loss: 1.3696\n",
      "Epoch: 12, Index: 297, Loss: 0.8036\n",
      "Epoch: 12, Index: 298, Loss: 0.2475\n",
      "Epoch: 12, Index: 299, Loss: 0.3197\n",
      "Epoch: 12, Index: 300, Loss: 3.3392\n",
      "Epoch: 12, Index: 301, Loss: 1.0995\n",
      "Epoch: 12, Index: 302, Loss: 6.1330\n",
      "Epoch: 12, Index: 303, Loss: 0.0710\n",
      "Epoch: 12, Index: 304, Loss: 4.6491\n",
      "Epoch: 12, Index: 305, Loss: 0.5328\n",
      "Epoch: 12, Index: 306, Loss: 0.8072\n",
      "Epoch: 12, Index: 307, Loss: 6.0550\n",
      "Epoch: 12, Index: 308, Loss: 3.7527\n",
      "Epoch: 12, Index: 309, Loss: 0.1096\n",
      "Epoch: 12, Index: 310, Loss: 1.1149\n",
      "Epoch: 12, Index: 311, Loss: 1.8930\n",
      "Epoch: 12, Index: 312, Loss: 0.9731\n",
      "Epoch: 12, Index: 313, Loss: 2.0493\n",
      "Epoch: 12, Index: 314, Loss: 0.3352\n",
      "Epoch: 12, Index: 315, Loss: 0.2030\n",
      "Epoch: 12, Index: 316, Loss: 0.1986\n",
      "Epoch: 12, Index: 317, Loss: 0.2270\n",
      "Epoch: 12, Index: 318, Loss: 2.7547\n",
      "Epoch: 12, Index: 319, Loss: 0.0177\n",
      "Epoch: 12, Index: 320, Loss: 1.9206\n",
      "Epoch: 12, Index: 321, Loss: 0.4219\n",
      "Epoch: 12, Index: 322, Loss: 3.8324\n",
      "Epoch: 12, Index: 323, Loss: 1.1174\n",
      "Epoch: 12, Index: 324, Loss: 1.7643\n",
      "Epoch: 12, Index: 325, Loss: 5.2952\n",
      "Epoch: 12, Index: 326, Loss: 1.1535\n",
      "Epoch: 12, Index: 327, Loss: 1.9279\n",
      "Epoch: 12, Index: 328, Loss: 0.2704\n",
      "Epoch: 12, Index: 329, Loss: 0.4904\n",
      "Epoch: 12, Index: 330, Loss: 0.9138\n",
      "Epoch: 12, Index: 331, Loss: 1.6057\n",
      "Epoch: 12, Index: 332, Loss: 2.9782\n",
      "Epoch: 12, Index: 333, Loss: 2.6380\n",
      "Epoch: 12, Index: 334, Loss: 18.6240\n",
      "Epoch: 12, Index: 335, Loss: 0.8599\n",
      "Epoch: 12, Index: 336, Loss: 2.7609\n",
      "Epoch: 12, Index: 337, Loss: 2.4609\n",
      "Epoch: 12, Index: 338, Loss: 5.1554\n",
      "Epoch: 12, Index: 339, Loss: 3.7447\n",
      "Epoch: 12, Index: 340, Loss: 2.6551\n",
      "Epoch: 12, Index: 341, Loss: 1.8791\n",
      "Epoch: 12, Index: 342, Loss: 1.1377\n",
      "Epoch: 12, Index: 343, Loss: 2.0058\n",
      "Epoch: 12, Index: 344, Loss: 0.6331\n",
      "Epoch: 12, Index: 345, Loss: 0.0566\n",
      "Epoch: 12, Index: 346, Loss: 0.5455\n",
      "Epoch: 12, Index: 347, Loss: 1.3502\n",
      "Epoch: 12, Index: 348, Loss: 2.4055\n",
      "Epoch: 12, Index: 349, Loss: 1.0633\n",
      "Epoch: 12, Index: 350, Loss: 0.8939\n",
      "Epoch: 12, Index: 351, Loss: 2.0819\n",
      "Epoch: 12, Index: 352, Loss: 7.4429\n",
      "Epoch: 12, Index: 353, Loss: 4.9142\n",
      "Epoch: 12, Index: 354, Loss: 1.7501\n",
      "Epoch: 12, Index: 355, Loss: 1.8692\n",
      "Epoch: 12, Index: 356, Loss: 0.6970\n",
      "Epoch: 12, Index: 357, Loss: 1.0157\n",
      "Epoch: 12, Index: 358, Loss: 0.1043\n",
      "Epoch: 12, Index: 359, Loss: 2.3000\n",
      "Epoch: 12, Index: 360, Loss: 0.6404\n",
      "Epoch: 12, Index: 361, Loss: 1.0853\n",
      "Epoch: 12, Index: 362, Loss: 4.9237\n",
      "Epoch: 12, Index: 363, Loss: 0.7711\n",
      "Epoch: 12, Index: 364, Loss: 1.9471\n",
      "Epoch: 12, Index: 365, Loss: 0.0336\n",
      "Epoch: 12, Index: 366, Loss: 0.9202\n",
      "Epoch: 12, Index: 367, Loss: 1.6233\n",
      "Epoch: 12, Index: 368, Loss: 0.7533\n",
      "Epoch: 12, Index: 369, Loss: 0.6207\n",
      "Epoch: 12, Index: 370, Loss: 1.1167\n",
      "Epoch: 12, Index: 371, Loss: 3.6874\n",
      "Epoch: 12, Index: 372, Loss: 4.8119\n",
      "Epoch: 12, Index: 373, Loss: 7.0821\n",
      "Epoch: 12, Index: 374, Loss: 0.6065\n",
      "Epoch: 12, Index: 375, Loss: 0.1686\n",
      "Epoch: 12, Index: 376, Loss: 2.3781\n",
      "Epoch: 12, Index: 377, Loss: 0.0170\n",
      "Epoch: 12, Index: 378, Loss: 1.7737\n",
      "Epoch: 12, Index: 379, Loss: 0.9693\n",
      "Epoch: 12, Index: 380, Loss: 1.0178\n",
      "Epoch: 12, Index: 381, Loss: 4.9169\n",
      "Epoch: 12, Index: 382, Loss: 3.6667\n",
      "Epoch: 12, Index: 383, Loss: 1.5160\n",
      "Epoch: 12, Index: 384, Loss: 0.0852\n",
      "Epoch: 12, Index: 385, Loss: 0.2924\n",
      "Epoch: 12, Index: 386, Loss: 2.7626\n",
      "Epoch: 12, Index: 387, Loss: 1.5468\n",
      "Epoch: 12, Index: 388, Loss: 0.2964\n",
      "Epoch: 12, Index: 389, Loss: 16.8372\n",
      "Epoch: 12, Index: 390, Loss: 0.5472\n",
      "Epoch: 12, Index: 391, Loss: 1.2004\n",
      "Epoch: 12, Index: 392, Loss: 2.0579\n",
      "Epoch: 12, Index: 393, Loss: 0.6208\n",
      "Epoch: 12, Index: 394, Loss: 0.6443\n",
      "Epoch: 12, Index: 395, Loss: 0.3113\n",
      "Epoch: 12, Index: 396, Loss: 1.3112\n",
      "Epoch: 12, Index: 397, Loss: 0.0566\n",
      "Epoch: 12, Index: 398, Loss: 0.8432\n",
      "Epoch: 12, Index: 399, Loss: 0.1808\n",
      "Epoch: 12, Index: 400, Loss: 6.7267\n",
      "Epoch: 12, Index: 401, Loss: 0.0730\n",
      "Epoch: 12, Index: 402, Loss: 0.6157\n",
      "Epoch: 12, Index: 403, Loss: 0.0042\n",
      "Epoch: 12, Index: 404, Loss: 0.3073\n",
      "Epoch: 12, Index: 405, Loss: 2.1872\n",
      "Epoch: 12, Index: 406, Loss: 0.2535\n",
      "Epoch: 12, Index: 407, Loss: 0.8209\n",
      "Epoch: 12, Index: 408, Loss: 1.8493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbb5f62371649acae0490e86db78643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Index: 0, Loss: 0.3447\n",
      "Epoch: 13, Index: 1, Loss: 1.2141\n",
      "Epoch: 13, Index: 2, Loss: 0.3350\n",
      "Epoch: 13, Index: 3, Loss: 3.2197\n",
      "Epoch: 13, Index: 4, Loss: 2.7721\n",
      "Epoch: 13, Index: 5, Loss: 0.9472\n",
      "Epoch: 13, Index: 6, Loss: 0.8737\n",
      "Epoch: 13, Index: 7, Loss: 7.1491\n",
      "Epoch: 13, Index: 8, Loss: 1.2316\n",
      "Epoch: 13, Index: 9, Loss: 4.2843\n",
      "Epoch: 13, Index: 10, Loss: 2.4862\n",
      "Epoch: 13, Index: 11, Loss: 0.0940\n",
      "Epoch: 13, Index: 12, Loss: 0.7986\n",
      "Epoch: 13, Index: 13, Loss: 0.1764\n",
      "Epoch: 13, Index: 14, Loss: 0.4775\n",
      "Epoch: 13, Index: 15, Loss: 0.8600\n",
      "Epoch: 13, Index: 16, Loss: 1.9107\n",
      "Epoch: 13, Index: 17, Loss: 0.3772\n",
      "Epoch: 13, Index: 18, Loss: 1.1250\n",
      "Epoch: 13, Index: 19, Loss: 2.3695\n",
      "Epoch: 13, Index: 20, Loss: 0.0094\n",
      "Epoch: 13, Index: 21, Loss: 1.7571\n",
      "Epoch: 13, Index: 22, Loss: 0.0413\n",
      "Epoch: 13, Index: 23, Loss: 1.2294\n",
      "Epoch: 13, Index: 24, Loss: 0.7840\n",
      "Epoch: 13, Index: 25, Loss: 1.1558\n",
      "Epoch: 13, Index: 26, Loss: 1.0456\n",
      "Epoch: 13, Index: 27, Loss: 2.1665\n",
      "Epoch: 13, Index: 28, Loss: 0.3062\n",
      "Epoch: 13, Index: 29, Loss: 0.5881\n",
      "Epoch: 13, Index: 30, Loss: 4.6359\n",
      "Epoch: 13, Index: 31, Loss: 0.7819\n",
      "Epoch: 13, Index: 32, Loss: 0.4984\n",
      "Epoch: 13, Index: 33, Loss: 2.6227\n",
      "Epoch: 13, Index: 34, Loss: 2.2054\n",
      "Epoch: 13, Index: 35, Loss: 1.3330\n",
      "Epoch: 13, Index: 36, Loss: 0.0557\n",
      "Epoch: 13, Index: 37, Loss: 3.1222\n",
      "Epoch: 13, Index: 38, Loss: 1.2589\n",
      "Epoch: 13, Index: 39, Loss: 0.2301\n",
      "Epoch: 13, Index: 40, Loss: 2.3599\n",
      "Epoch: 13, Index: 41, Loss: 0.3890\n",
      "Epoch: 13, Index: 42, Loss: 1.0674\n",
      "Epoch: 13, Index: 43, Loss: 0.6337\n",
      "Epoch: 13, Index: 44, Loss: 1.7915\n",
      "Epoch: 13, Index: 45, Loss: 3.4634\n",
      "Epoch: 13, Index: 46, Loss: 2.8889\n",
      "Epoch: 13, Index: 47, Loss: 3.7170\n",
      "Epoch: 13, Index: 48, Loss: 1.5221\n",
      "Epoch: 13, Index: 49, Loss: 2.9727\n",
      "Epoch: 13, Index: 50, Loss: 0.1103\n",
      "Epoch: 13, Index: 51, Loss: 0.8767\n",
      "Epoch: 13, Index: 52, Loss: 2.1003\n",
      "Epoch: 13, Index: 53, Loss: 0.4801\n",
      "Epoch: 13, Index: 54, Loss: 4.5828\n",
      "Epoch: 13, Index: 55, Loss: 0.8458\n",
      "Epoch: 13, Index: 56, Loss: 6.0960\n",
      "Epoch: 13, Index: 57, Loss: 1.3540\n",
      "Epoch: 13, Index: 58, Loss: 2.2851\n",
      "Epoch: 13, Index: 59, Loss: 2.2717\n",
      "Epoch: 13, Index: 60, Loss: 0.9394\n",
      "Epoch: 13, Index: 61, Loss: 0.3243\n",
      "Epoch: 13, Index: 62, Loss: 1.4734\n",
      "Epoch: 13, Index: 63, Loss: 3.3656\n",
      "Epoch: 13, Index: 64, Loss: 2.0470\n",
      "Epoch: 13, Index: 65, Loss: 2.0905\n",
      "Epoch: 13, Index: 66, Loss: 0.0992\n",
      "Epoch: 13, Index: 67, Loss: 6.3827\n",
      "Epoch: 13, Index: 68, Loss: 0.7053\n",
      "Epoch: 13, Index: 69, Loss: 0.5906\n",
      "Epoch: 13, Index: 70, Loss: 2.3603\n",
      "Epoch: 13, Index: 71, Loss: 1.9175\n",
      "Epoch: 13, Index: 72, Loss: 3.6891\n",
      "Epoch: 13, Index: 73, Loss: 0.9345\n",
      "Epoch: 13, Index: 74, Loss: 1.3343\n",
      "Epoch: 13, Index: 75, Loss: 0.4399\n",
      "Epoch: 13, Index: 76, Loss: 2.7029\n",
      "Epoch: 13, Index: 77, Loss: 0.2023\n",
      "Epoch: 13, Index: 78, Loss: 0.4281\n",
      "Epoch: 13, Index: 79, Loss: 1.1930\n",
      "Epoch: 13, Index: 80, Loss: 1.9476\n",
      "Epoch: 13, Index: 81, Loss: 3.1486\n",
      "Epoch: 13, Index: 82, Loss: 0.2051\n",
      "Epoch: 13, Index: 83, Loss: 0.4700\n",
      "Epoch: 13, Index: 84, Loss: 4.5598\n",
      "Epoch: 13, Index: 85, Loss: 1.3399\n",
      "Epoch: 13, Index: 86, Loss: 0.8374\n",
      "Epoch: 13, Index: 87, Loss: 0.9313\n",
      "Epoch: 13, Index: 88, Loss: 1.5316\n",
      "Epoch: 13, Index: 89, Loss: 0.5474\n",
      "Epoch: 13, Index: 90, Loss: 2.6641\n",
      "Epoch: 13, Index: 91, Loss: 1.4329\n",
      "Epoch: 13, Index: 92, Loss: 1.4893\n",
      "Epoch: 13, Index: 93, Loss: 0.2202\n",
      "Epoch: 13, Index: 94, Loss: 0.8901\n",
      "Epoch: 13, Index: 95, Loss: 1.2715\n",
      "Epoch: 13, Index: 96, Loss: 1.6600\n",
      "Epoch: 13, Index: 97, Loss: 0.1038\n",
      "Epoch: 13, Index: 98, Loss: 5.9716\n",
      "Epoch: 13, Index: 99, Loss: 1.9391\n",
      "Epoch: 13, Index: 100, Loss: 1.5596\n",
      "Epoch: 13, Index: 101, Loss: 1.4530\n",
      "Epoch: 13, Index: 102, Loss: 4.2389\n",
      "Epoch: 13, Index: 103, Loss: 0.1429\n",
      "Epoch: 13, Index: 104, Loss: 1.2917\n",
      "Epoch: 13, Index: 105, Loss: 0.9194\n",
      "Epoch: 13, Index: 106, Loss: 0.3229\n",
      "Epoch: 13, Index: 107, Loss: 0.6907\n",
      "Epoch: 13, Index: 108, Loss: 1.3274\n",
      "Epoch: 13, Index: 109, Loss: 0.3144\n",
      "Epoch: 13, Index: 110, Loss: 1.5894\n",
      "Epoch: 13, Index: 111, Loss: 0.3238\n",
      "Epoch: 13, Index: 112, Loss: 1.8614\n",
      "Epoch: 13, Index: 113, Loss: 1.5843\n",
      "Epoch: 13, Index: 114, Loss: 0.4012\n",
      "Epoch: 13, Index: 115, Loss: 2.7609\n",
      "Epoch: 13, Index: 116, Loss: 0.9005\n",
      "Epoch: 13, Index: 117, Loss: 2.4237\n",
      "Epoch: 13, Index: 118, Loss: 1.1037\n",
      "Epoch: 13, Index: 119, Loss: 0.9404\n",
      "Epoch: 13, Index: 120, Loss: 2.5871\n",
      "Epoch: 13, Index: 121, Loss: 0.0606\n",
      "Epoch: 13, Index: 122, Loss: 6.5325\n",
      "Epoch: 13, Index: 123, Loss: 3.6212\n",
      "Epoch: 13, Index: 124, Loss: 3.2135\n",
      "Epoch: 13, Index: 125, Loss: 0.1076\n",
      "Epoch: 13, Index: 126, Loss: 0.0549\n",
      "Epoch: 13, Index: 127, Loss: 6.1498\n",
      "Epoch: 13, Index: 128, Loss: 1.0376\n",
      "Epoch: 13, Index: 129, Loss: 2.2337\n",
      "Epoch: 13, Index: 130, Loss: 2.3560\n",
      "Epoch: 13, Index: 131, Loss: 0.5006\n",
      "Epoch: 13, Index: 132, Loss: 0.8437\n",
      "Epoch: 13, Index: 133, Loss: 3.0093\n",
      "Epoch: 13, Index: 134, Loss: 0.1327\n",
      "Epoch: 13, Index: 135, Loss: 7.8411\n",
      "Epoch: 13, Index: 136, Loss: 5.1344\n",
      "Epoch: 13, Index: 137, Loss: 1.5030\n",
      "Epoch: 13, Index: 138, Loss: 0.9691\n",
      "Epoch: 13, Index: 139, Loss: 1.5007\n",
      "Epoch: 13, Index: 140, Loss: 0.0678\n",
      "Epoch: 13, Index: 141, Loss: 0.1832\n",
      "Epoch: 13, Index: 142, Loss: 2.0754\n",
      "Epoch: 13, Index: 143, Loss: 0.3433\n",
      "Epoch: 13, Index: 144, Loss: 0.8568\n",
      "Epoch: 13, Index: 145, Loss: 1.8409\n",
      "Epoch: 13, Index: 146, Loss: 0.3301\n",
      "Epoch: 13, Index: 147, Loss: 2.6459\n",
      "Epoch: 13, Index: 148, Loss: 0.8254\n",
      "Epoch: 13, Index: 149, Loss: 1.7652\n",
      "Epoch: 13, Index: 150, Loss: 2.0675\n",
      "Epoch: 13, Index: 151, Loss: 0.4059\n",
      "Epoch: 13, Index: 152, Loss: 0.3290\n",
      "Epoch: 13, Index: 153, Loss: 1.4664\n",
      "Epoch: 13, Index: 154, Loss: 0.2737\n",
      "Epoch: 13, Index: 155, Loss: 2.1099\n",
      "Epoch: 13, Index: 156, Loss: 0.0726\n",
      "Epoch: 13, Index: 157, Loss: 1.1808\n",
      "Epoch: 13, Index: 158, Loss: 0.6177\n",
      "Epoch: 13, Index: 159, Loss: 6.4918\n",
      "Epoch: 13, Index: 160, Loss: 0.0522\n",
      "Epoch: 13, Index: 161, Loss: 0.3194\n",
      "Epoch: 13, Index: 162, Loss: 0.4056\n",
      "Epoch: 13, Index: 163, Loss: 1.4619\n",
      "Epoch: 13, Index: 164, Loss: 0.8176\n",
      "Epoch: 13, Index: 165, Loss: 0.7772\n",
      "Epoch: 13, Index: 166, Loss: 1.4113\n",
      "Epoch: 13, Index: 167, Loss: 0.4392\n",
      "Epoch: 13, Index: 168, Loss: 5.8920\n",
      "Epoch: 13, Index: 169, Loss: 3.4483\n",
      "Epoch: 13, Index: 170, Loss: 1.4143\n",
      "Epoch: 13, Index: 171, Loss: 1.2416\n",
      "Epoch: 13, Index: 172, Loss: 0.5076\n",
      "Epoch: 13, Index: 173, Loss: 0.7830\n",
      "Epoch: 13, Index: 174, Loss: 1.0335\n",
      "Epoch: 13, Index: 175, Loss: 1.4235\n",
      "Epoch: 13, Index: 176, Loss: 0.7294\n",
      "Epoch: 13, Index: 177, Loss: 1.7951\n",
      "Epoch: 13, Index: 178, Loss: 0.5733\n",
      "Epoch: 13, Index: 179, Loss: 0.9283\n",
      "Epoch: 13, Index: 180, Loss: 10.8356\n",
      "Epoch: 13, Index: 181, Loss: 1.2284\n",
      "Epoch: 13, Index: 182, Loss: 3.8762\n",
      "Epoch: 13, Index: 183, Loss: 3.2042\n",
      "Epoch: 13, Index: 184, Loss: 0.9057\n",
      "Epoch: 13, Index: 185, Loss: 0.3987\n",
      "Epoch: 13, Index: 186, Loss: 2.0743\n",
      "Epoch: 13, Index: 187, Loss: 0.0009\n",
      "Epoch: 13, Index: 188, Loss: 0.0470\n",
      "Epoch: 13, Index: 189, Loss: 6.5113\n",
      "Epoch: 13, Index: 190, Loss: 0.3087\n",
      "Epoch: 13, Index: 191, Loss: 2.2894\n",
      "Epoch: 13, Index: 192, Loss: 1.6907\n",
      "Epoch: 13, Index: 193, Loss: 1.8149\n",
      "Epoch: 13, Index: 194, Loss: 1.9700\n",
      "Epoch: 13, Index: 195, Loss: 6.7149\n",
      "Epoch: 13, Index: 196, Loss: 0.5704\n",
      "Epoch: 13, Index: 197, Loss: 3.2145\n",
      "Epoch: 13, Index: 198, Loss: 0.4849\n",
      "Epoch: 13, Index: 199, Loss: 9.8696\n",
      "Epoch: 13, Index: 200, Loss: 0.7386\n",
      "Epoch: 13, Index: 201, Loss: 1.1768\n",
      "Epoch: 13, Index: 202, Loss: 0.8278\n",
      "Epoch: 13, Index: 203, Loss: 0.2935\n",
      "Epoch: 13, Index: 204, Loss: 6.0714\n",
      "Epoch: 13, Index: 205, Loss: 2.0649\n",
      "Epoch: 13, Index: 206, Loss: 4.9450\n",
      "Epoch: 13, Index: 207, Loss: 1.5048\n",
      "Epoch: 13, Index: 208, Loss: 7.3056\n",
      "Epoch: 13, Index: 209, Loss: 3.9113\n",
      "Epoch: 13, Index: 210, Loss: 1.8416\n",
      "Epoch: 13, Index: 211, Loss: 3.8498\n",
      "Epoch: 13, Index: 212, Loss: 0.3226\n",
      "Epoch: 13, Index: 213, Loss: 4.4974\n",
      "Epoch: 13, Index: 214, Loss: 2.4302\n",
      "Epoch: 13, Index: 215, Loss: 1.6938\n",
      "Epoch: 13, Index: 216, Loss: 4.9348\n",
      "Epoch: 13, Index: 217, Loss: 2.7354\n",
      "Epoch: 13, Index: 218, Loss: 1.1060\n",
      "Epoch: 13, Index: 219, Loss: 0.3275\n",
      "Epoch: 13, Index: 220, Loss: 2.4837\n",
      "Epoch: 13, Index: 221, Loss: 1.9344\n",
      "Epoch: 13, Index: 222, Loss: 1.4074\n",
      "Epoch: 13, Index: 223, Loss: 5.5064\n",
      "Epoch: 13, Index: 224, Loss: 0.2079\n",
      "Epoch: 13, Index: 225, Loss: 4.0837\n",
      "Epoch: 13, Index: 226, Loss: 2.5691\n",
      "Epoch: 13, Index: 227, Loss: 1.1152\n",
      "Epoch: 13, Index: 228, Loss: 1.2015\n",
      "Epoch: 13, Index: 229, Loss: 1.0928\n",
      "Epoch: 13, Index: 230, Loss: 3.3563\n",
      "Epoch: 13, Index: 231, Loss: 4.5276\n",
      "Epoch: 13, Index: 232, Loss: 1.4262\n",
      "Epoch: 13, Index: 233, Loss: 0.2647\n",
      "Epoch: 13, Index: 234, Loss: 0.2978\n",
      "Epoch: 13, Index: 235, Loss: 3.0190\n",
      "Epoch: 13, Index: 236, Loss: 1.5071\n",
      "Epoch: 13, Index: 237, Loss: 11.3155\n",
      "Epoch: 13, Index: 238, Loss: 0.8893\n",
      "Epoch: 13, Index: 239, Loss: 1.9884\n",
      "Epoch: 13, Index: 240, Loss: 0.3457\n",
      "Epoch: 13, Index: 241, Loss: 1.0797\n",
      "Epoch: 13, Index: 242, Loss: 2.4231\n",
      "Epoch: 13, Index: 243, Loss: 0.6371\n",
      "Epoch: 13, Index: 244, Loss: 0.3794\n",
      "Epoch: 13, Index: 245, Loss: 1.6412\n",
      "Epoch: 13, Index: 246, Loss: 2.5841\n",
      "Epoch: 13, Index: 247, Loss: 1.2880\n",
      "Epoch: 13, Index: 248, Loss: 0.4126\n",
      "Epoch: 13, Index: 249, Loss: 0.6569\n",
      "Epoch: 13, Index: 250, Loss: 15.8481\n",
      "Epoch: 13, Index: 251, Loss: 0.0115\n",
      "Epoch: 13, Index: 252, Loss: 0.0835\n",
      "Epoch: 13, Index: 253, Loss: 1.3219\n",
      "Epoch: 13, Index: 254, Loss: 0.3719\n",
      "Epoch: 13, Index: 255, Loss: 1.9959\n",
      "Epoch: 13, Index: 256, Loss: 3.4832\n",
      "Epoch: 13, Index: 257, Loss: 2.8766\n",
      "Epoch: 13, Index: 258, Loss: 2.2368\n",
      "Epoch: 13, Index: 259, Loss: 3.0024\n",
      "Epoch: 13, Index: 260, Loss: 2.2649\n",
      "Epoch: 13, Index: 261, Loss: 1.3580\n",
      "Epoch: 13, Index: 262, Loss: 3.0443\n",
      "Epoch: 13, Index: 263, Loss: 1.3173\n",
      "Epoch: 13, Index: 264, Loss: 5.9737\n",
      "Epoch: 13, Index: 265, Loss: 1.0352\n",
      "Epoch: 13, Index: 266, Loss: 2.3495\n",
      "Epoch: 13, Index: 267, Loss: 1.8446\n",
      "Epoch: 13, Index: 268, Loss: 2.1221\n",
      "Epoch: 13, Index: 269, Loss: 2.2614\n",
      "Epoch: 13, Index: 270, Loss: 4.7268\n",
      "Epoch: 13, Index: 271, Loss: 0.9031\n",
      "Epoch: 13, Index: 272, Loss: 1.0163\n",
      "Epoch: 13, Index: 273, Loss: 1.3239\n",
      "Epoch: 13, Index: 274, Loss: 1.8010\n",
      "Epoch: 13, Index: 275, Loss: 1.1634\n",
      "Epoch: 13, Index: 276, Loss: 1.3788\n",
      "Epoch: 13, Index: 277, Loss: 0.7832\n",
      "Epoch: 13, Index: 278, Loss: 0.3476\n",
      "Epoch: 13, Index: 279, Loss: 7.8006\n",
      "Epoch: 13, Index: 280, Loss: 2.9350\n",
      "Epoch: 13, Index: 281, Loss: 0.4832\n",
      "Epoch: 13, Index: 282, Loss: 0.7292\n",
      "Epoch: 13, Index: 283, Loss: 2.5014\n",
      "Epoch: 13, Index: 284, Loss: 1.7958\n",
      "Epoch: 13, Index: 285, Loss: 1.3820\n",
      "Epoch: 13, Index: 286, Loss: 1.1720\n",
      "Epoch: 13, Index: 287, Loss: 1.4725\n",
      "Epoch: 13, Index: 288, Loss: 1.4483\n",
      "Epoch: 13, Index: 289, Loss: 1.4232\n",
      "Epoch: 13, Index: 290, Loss: 2.0574\n",
      "Epoch: 13, Index: 291, Loss: 0.9246\n",
      "Epoch: 13, Index: 292, Loss: 0.5057\n",
      "Epoch: 13, Index: 293, Loss: 3.2401\n",
      "Epoch: 13, Index: 294, Loss: 3.2707\n",
      "Epoch: 13, Index: 295, Loss: 3.0635\n",
      "Epoch: 13, Index: 296, Loss: 0.7519\n",
      "Epoch: 13, Index: 297, Loss: 2.9476\n",
      "Epoch: 13, Index: 298, Loss: 0.6672\n",
      "Epoch: 13, Index: 299, Loss: 6.8687\n",
      "Epoch: 13, Index: 300, Loss: 0.4956\n",
      "Epoch: 13, Index: 301, Loss: 1.5411\n",
      "Epoch: 13, Index: 302, Loss: 2.0181\n",
      "Epoch: 13, Index: 303, Loss: 1.9562\n",
      "Epoch: 13, Index: 304, Loss: 0.0847\n",
      "Epoch: 13, Index: 305, Loss: 0.7494\n",
      "Epoch: 13, Index: 306, Loss: 1.3212\n",
      "Epoch: 13, Index: 307, Loss: 0.7644\n",
      "Epoch: 13, Index: 308, Loss: 0.3180\n",
      "Epoch: 13, Index: 309, Loss: 3.9567\n",
      "Epoch: 13, Index: 310, Loss: 2.9449\n",
      "Epoch: 13, Index: 311, Loss: 1.1093\n",
      "Epoch: 13, Index: 312, Loss: 4.9208\n",
      "Epoch: 13, Index: 313, Loss: 2.2684\n",
      "Epoch: 13, Index: 314, Loss: 0.9172\n",
      "Epoch: 13, Index: 315, Loss: 1.9694\n",
      "Epoch: 13, Index: 316, Loss: 0.8648\n",
      "Epoch: 13, Index: 317, Loss: 0.5033\n",
      "Epoch: 13, Index: 318, Loss: 0.9338\n",
      "Epoch: 13, Index: 319, Loss: 0.3349\n",
      "Epoch: 13, Index: 320, Loss: 1.5357\n",
      "Epoch: 13, Index: 321, Loss: 0.6714\n",
      "Epoch: 13, Index: 322, Loss: 0.1650\n",
      "Epoch: 13, Index: 323, Loss: 0.4108\n",
      "Epoch: 13, Index: 324, Loss: 0.7703\n",
      "Epoch: 13, Index: 325, Loss: 1.1784\n",
      "Epoch: 13, Index: 326, Loss: 3.5452\n",
      "Epoch: 13, Index: 327, Loss: 2.9915\n",
      "Epoch: 13, Index: 328, Loss: 1.5221\n",
      "Epoch: 13, Index: 329, Loss: 4.2798\n",
      "Epoch: 13, Index: 330, Loss: 1.1292\n",
      "Epoch: 13, Index: 331, Loss: 0.6681\n",
      "Epoch: 13, Index: 332, Loss: 0.0020\n",
      "Epoch: 13, Index: 333, Loss: 1.9326\n",
      "Epoch: 13, Index: 334, Loss: 0.5148\n",
      "Epoch: 13, Index: 335, Loss: 0.3253\n",
      "Epoch: 13, Index: 336, Loss: 4.3391\n",
      "Epoch: 13, Index: 337, Loss: 0.3188\n",
      "Epoch: 13, Index: 338, Loss: 1.9059\n",
      "Epoch: 13, Index: 339, Loss: 0.5085\n",
      "Epoch: 13, Index: 340, Loss: 0.1185\n",
      "Epoch: 13, Index: 341, Loss: 0.5133\n",
      "Epoch: 13, Index: 342, Loss: 0.8953\n",
      "Epoch: 13, Index: 343, Loss: 0.5055\n",
      "Epoch: 13, Index: 344, Loss: 0.9123\n",
      "Epoch: 13, Index: 345, Loss: 0.7572\n",
      "Epoch: 13, Index: 346, Loss: 1.4649\n",
      "Epoch: 13, Index: 347, Loss: 0.3063\n",
      "Epoch: 13, Index: 348, Loss: 0.2139\n",
      "Epoch: 13, Index: 349, Loss: 0.9036\n",
      "Epoch: 13, Index: 350, Loss: 1.4086\n",
      "Epoch: 13, Index: 351, Loss: 0.0730\n",
      "Epoch: 13, Index: 352, Loss: 0.6286\n",
      "Epoch: 13, Index: 353, Loss: 1.9704\n",
      "Epoch: 13, Index: 354, Loss: 0.9344\n",
      "Epoch: 13, Index: 355, Loss: 0.3986\n",
      "Epoch: 13, Index: 356, Loss: 1.7795\n",
      "Epoch: 13, Index: 357, Loss: 1.5458\n",
      "Epoch: 13, Index: 358, Loss: 0.0640\n",
      "Epoch: 13, Index: 359, Loss: 0.1995\n",
      "Epoch: 13, Index: 360, Loss: 0.1623\n",
      "Epoch: 13, Index: 361, Loss: 0.0513\n",
      "Epoch: 13, Index: 362, Loss: 0.3900\n",
      "Epoch: 13, Index: 363, Loss: 1.7922\n",
      "Epoch: 13, Index: 364, Loss: 0.5646\n",
      "Epoch: 13, Index: 365, Loss: 1.4221\n",
      "Epoch: 13, Index: 366, Loss: 0.2177\n",
      "Epoch: 13, Index: 367, Loss: 5.5196\n",
      "Epoch: 13, Index: 368, Loss: 0.2423\n",
      "Epoch: 13, Index: 369, Loss: 2.2433\n",
      "Epoch: 13, Index: 370, Loss: 1.2211\n",
      "Epoch: 13, Index: 371, Loss: 1.4783\n",
      "Epoch: 13, Index: 372, Loss: 3.0145\n",
      "Epoch: 13, Index: 373, Loss: 3.6876\n",
      "Epoch: 13, Index: 374, Loss: 0.4655\n",
      "Epoch: 13, Index: 375, Loss: 1.7498\n",
      "Epoch: 13, Index: 376, Loss: 0.1882\n",
      "Epoch: 13, Index: 377, Loss: 1.5004\n",
      "Epoch: 13, Index: 378, Loss: 0.9547\n",
      "Epoch: 13, Index: 379, Loss: 1.3185\n",
      "Epoch: 13, Index: 380, Loss: 0.2911\n",
      "Epoch: 13, Index: 381, Loss: 0.9604\n",
      "Epoch: 13, Index: 382, Loss: 0.3107\n",
      "Epoch: 13, Index: 383, Loss: 2.6524\n",
      "Epoch: 13, Index: 384, Loss: 1.3262\n",
      "Epoch: 13, Index: 385, Loss: 0.7173\n",
      "Epoch: 13, Index: 386, Loss: 0.0461\n",
      "Epoch: 13, Index: 387, Loss: 1.6345\n",
      "Epoch: 13, Index: 388, Loss: 0.0638\n",
      "Epoch: 13, Index: 389, Loss: 0.6604\n",
      "Epoch: 13, Index: 390, Loss: 0.6955\n",
      "Epoch: 13, Index: 391, Loss: 0.8680\n",
      "Epoch: 13, Index: 392, Loss: 1.4082\n",
      "Epoch: 13, Index: 393, Loss: 0.0158\n",
      "Epoch: 13, Index: 394, Loss: 1.2934\n",
      "Epoch: 13, Index: 395, Loss: 0.0771\n",
      "Epoch: 13, Index: 396, Loss: 0.4007\n",
      "Epoch: 13, Index: 397, Loss: 1.8248\n",
      "Epoch: 13, Index: 398, Loss: 0.9497\n",
      "Epoch: 13, Index: 399, Loss: 1.7817\n",
      "Epoch: 13, Index: 400, Loss: 0.7691\n",
      "Epoch: 13, Index: 401, Loss: 2.9963\n",
      "Epoch: 13, Index: 402, Loss: 0.8780\n",
      "Epoch: 13, Index: 403, Loss: 0.4123\n",
      "Epoch: 13, Index: 404, Loss: 3.1550\n",
      "Epoch: 13, Index: 405, Loss: 0.1312\n",
      "Epoch: 13, Index: 406, Loss: 0.1202\n",
      "Epoch: 13, Index: 407, Loss: 2.5299\n",
      "Epoch: 13, Index: 408, Loss: 2.8161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f831826081c043a0a33c5cdffa8e128f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Index: 0, Loss: 1.4239\n",
      "Epoch: 14, Index: 1, Loss: 0.1730\n",
      "Epoch: 14, Index: 2, Loss: 0.1593\n",
      "Epoch: 14, Index: 3, Loss: 1.6164\n",
      "Epoch: 14, Index: 4, Loss: 0.7086\n",
      "Epoch: 14, Index: 5, Loss: 3.0693\n",
      "Epoch: 14, Index: 6, Loss: 1.0929\n",
      "Epoch: 14, Index: 7, Loss: 0.7337\n",
      "Epoch: 14, Index: 8, Loss: 0.7066\n",
      "Epoch: 14, Index: 9, Loss: 0.0814\n",
      "Epoch: 14, Index: 10, Loss: 8.1245\n",
      "Epoch: 14, Index: 11, Loss: 0.4104\n",
      "Epoch: 14, Index: 12, Loss: 0.1934\n",
      "Epoch: 14, Index: 13, Loss: 6.4646\n",
      "Epoch: 14, Index: 14, Loss: 1.8915\n",
      "Epoch: 14, Index: 15, Loss: 1.5373\n",
      "Epoch: 14, Index: 16, Loss: 0.0311\n",
      "Epoch: 14, Index: 17, Loss: 1.0825\n",
      "Epoch: 14, Index: 18, Loss: 6.2177\n",
      "Epoch: 14, Index: 19, Loss: 0.5500\n",
      "Epoch: 14, Index: 20, Loss: 0.4460\n",
      "Epoch: 14, Index: 21, Loss: 1.8993\n",
      "Epoch: 14, Index: 22, Loss: 1.9080\n",
      "Epoch: 14, Index: 23, Loss: 0.3253\n",
      "Epoch: 14, Index: 24, Loss: 13.4363\n",
      "Epoch: 14, Index: 25, Loss: 1.6341\n",
      "Epoch: 14, Index: 26, Loss: 2.1187\n",
      "Epoch: 14, Index: 27, Loss: 1.1084\n",
      "Epoch: 14, Index: 28, Loss: 2.7618\n",
      "Epoch: 14, Index: 29, Loss: 2.0827\n",
      "Epoch: 14, Index: 30, Loss: 2.6930\n",
      "Epoch: 14, Index: 31, Loss: 0.7818\n",
      "Epoch: 14, Index: 32, Loss: 0.3499\n",
      "Epoch: 14, Index: 33, Loss: 3.0555\n",
      "Epoch: 14, Index: 34, Loss: 0.7385\n",
      "Epoch: 14, Index: 35, Loss: 2.5609\n",
      "Epoch: 14, Index: 36, Loss: 0.0968\n",
      "Epoch: 14, Index: 37, Loss: 0.8496\n",
      "Epoch: 14, Index: 38, Loss: 0.3939\n",
      "Epoch: 14, Index: 39, Loss: 1.5381\n",
      "Epoch: 14, Index: 40, Loss: 1.5366\n",
      "Epoch: 14, Index: 41, Loss: 6.9849\n",
      "Epoch: 14, Index: 42, Loss: 0.1088\n",
      "Epoch: 14, Index: 43, Loss: 0.0883\n",
      "Epoch: 14, Index: 44, Loss: 0.5070\n",
      "Epoch: 14, Index: 45, Loss: 3.6646\n",
      "Epoch: 14, Index: 46, Loss: 1.4254\n",
      "Epoch: 14, Index: 47, Loss: 1.6278\n",
      "Epoch: 14, Index: 48, Loss: 2.5247\n",
      "Epoch: 14, Index: 49, Loss: 0.5574\n",
      "Epoch: 14, Index: 50, Loss: 1.3947\n",
      "Epoch: 14, Index: 51, Loss: 1.5699\n",
      "Epoch: 14, Index: 52, Loss: 0.7056\n",
      "Epoch: 14, Index: 53, Loss: 3.3581\n",
      "Epoch: 14, Index: 54, Loss: 0.4146\n",
      "Epoch: 14, Index: 55, Loss: 3.4853\n",
      "Epoch: 14, Index: 56, Loss: 1.9354\n",
      "Epoch: 14, Index: 57, Loss: 2.7370\n",
      "Epoch: 14, Index: 58, Loss: 0.3574\n",
      "Epoch: 14, Index: 59, Loss: 1.9031\n",
      "Epoch: 14, Index: 60, Loss: 1.3373\n",
      "Epoch: 14, Index: 61, Loss: 0.8026\n",
      "Epoch: 14, Index: 62, Loss: 1.1838\n",
      "Epoch: 14, Index: 63, Loss: 1.9540\n",
      "Epoch: 14, Index: 64, Loss: 0.4063\n",
      "Epoch: 14, Index: 65, Loss: 0.4908\n",
      "Epoch: 14, Index: 66, Loss: 1.9569\n",
      "Epoch: 14, Index: 67, Loss: 0.4310\n",
      "Epoch: 14, Index: 68, Loss: 0.3951\n",
      "Epoch: 14, Index: 69, Loss: 0.9825\n",
      "Epoch: 14, Index: 70, Loss: 0.3505\n",
      "Epoch: 14, Index: 71, Loss: 3.5764\n",
      "Epoch: 14, Index: 72, Loss: 5.2582\n",
      "Epoch: 14, Index: 73, Loss: 3.4930\n",
      "Epoch: 14, Index: 74, Loss: 0.9121\n",
      "Epoch: 14, Index: 75, Loss: 1.7972\n",
      "Epoch: 14, Index: 76, Loss: 0.5250\n",
      "Epoch: 14, Index: 77, Loss: 2.5821\n",
      "Epoch: 14, Index: 78, Loss: 0.8305\n",
      "Epoch: 14, Index: 79, Loss: 0.6746\n",
      "Epoch: 14, Index: 80, Loss: 0.3290\n",
      "Epoch: 14, Index: 81, Loss: 2.2467\n",
      "Epoch: 14, Index: 82, Loss: 0.5074\n",
      "Epoch: 14, Index: 83, Loss: 0.0523\n",
      "Epoch: 14, Index: 84, Loss: 2.2931\n",
      "Epoch: 14, Index: 85, Loss: 0.6545\n",
      "Epoch: 14, Index: 86, Loss: 0.8041\n",
      "Epoch: 14, Index: 87, Loss: 0.9465\n",
      "Epoch: 14, Index: 88, Loss: 7.4654\n",
      "Epoch: 14, Index: 89, Loss: 1.2481\n",
      "Epoch: 14, Index: 90, Loss: 0.9897\n",
      "Epoch: 14, Index: 91, Loss: 0.0550\n",
      "Epoch: 14, Index: 92, Loss: 1.3149\n",
      "Epoch: 14, Index: 93, Loss: 0.1108\n",
      "Epoch: 14, Index: 94, Loss: 4.4522\n",
      "Epoch: 14, Index: 95, Loss: 0.5571\n",
      "Epoch: 14, Index: 96, Loss: 0.6839\n",
      "Epoch: 14, Index: 97, Loss: 4.3533\n",
      "Epoch: 14, Index: 98, Loss: 0.2555\n",
      "Epoch: 14, Index: 99, Loss: 0.0481\n",
      "Epoch: 14, Index: 100, Loss: 1.4264\n",
      "Epoch: 14, Index: 101, Loss: 0.8205\n",
      "Epoch: 14, Index: 102, Loss: 0.3594\n",
      "Epoch: 14, Index: 103, Loss: 2.3960\n",
      "Epoch: 14, Index: 104, Loss: 1.4638\n",
      "Epoch: 14, Index: 105, Loss: 15.5415\n",
      "Epoch: 14, Index: 106, Loss: 0.1736\n",
      "Epoch: 14, Index: 107, Loss: 0.7525\n",
      "Epoch: 14, Index: 108, Loss: 2.2006\n",
      "Epoch: 14, Index: 109, Loss: 2.0305\n",
      "Epoch: 14, Index: 110, Loss: 3.6273\n",
      "Epoch: 14, Index: 111, Loss: 2.2048\n",
      "Epoch: 14, Index: 112, Loss: 3.7929\n",
      "Epoch: 14, Index: 113, Loss: 7.2500\n",
      "Epoch: 14, Index: 114, Loss: 0.6612\n",
      "Epoch: 14, Index: 115, Loss: 0.5821\n",
      "Epoch: 14, Index: 116, Loss: 1.0381\n",
      "Epoch: 14, Index: 117, Loss: 0.6762\n",
      "Epoch: 14, Index: 118, Loss: 1.9450\n",
      "Epoch: 14, Index: 119, Loss: 1.4850\n",
      "Epoch: 14, Index: 120, Loss: 0.3193\n",
      "Epoch: 14, Index: 121, Loss: 0.2724\n",
      "Epoch: 14, Index: 122, Loss: 2.3553\n",
      "Epoch: 14, Index: 123, Loss: 6.4869\n",
      "Epoch: 14, Index: 124, Loss: 3.2455\n",
      "Epoch: 14, Index: 125, Loss: 0.8756\n",
      "Epoch: 14, Index: 126, Loss: 2.7183\n",
      "Epoch: 14, Index: 127, Loss: 3.5009\n",
      "Epoch: 14, Index: 128, Loss: 1.1721\n",
      "Epoch: 14, Index: 129, Loss: 0.0425\n",
      "Epoch: 14, Index: 130, Loss: 0.3353\n",
      "Epoch: 14, Index: 131, Loss: 2.6275\n",
      "Epoch: 14, Index: 132, Loss: 3.6960\n",
      "Epoch: 14, Index: 133, Loss: 1.5083\n",
      "Epoch: 14, Index: 134, Loss: 2.8503\n",
      "Epoch: 14, Index: 135, Loss: 1.7056\n",
      "Epoch: 14, Index: 136, Loss: 2.0600\n",
      "Epoch: 14, Index: 137, Loss: 2.9388\n",
      "Epoch: 14, Index: 138, Loss: 0.2103\n",
      "Epoch: 14, Index: 139, Loss: 1.4205\n",
      "Epoch: 14, Index: 140, Loss: 1.7640\n",
      "Epoch: 14, Index: 141, Loss: 1.2988\n",
      "Epoch: 14, Index: 142, Loss: 3.1256\n",
      "Epoch: 14, Index: 143, Loss: 2.0579\n",
      "Epoch: 14, Index: 144, Loss: 1.1573\n",
      "Epoch: 14, Index: 145, Loss: 1.5871\n",
      "Epoch: 14, Index: 146, Loss: 0.0594\n",
      "Epoch: 14, Index: 147, Loss: 0.6740\n",
      "Epoch: 14, Index: 148, Loss: 1.7555\n",
      "Epoch: 14, Index: 149, Loss: 1.3533\n",
      "Epoch: 14, Index: 150, Loss: 0.2158\n",
      "Epoch: 14, Index: 151, Loss: 0.7232\n",
      "Epoch: 14, Index: 152, Loss: 1.9814\n",
      "Epoch: 14, Index: 153, Loss: 1.4109\n",
      "Epoch: 14, Index: 154, Loss: 1.3853\n",
      "Epoch: 14, Index: 155, Loss: 0.9418\n",
      "Epoch: 14, Index: 156, Loss: 0.6958\n",
      "Epoch: 14, Index: 157, Loss: 2.5886\n",
      "Epoch: 14, Index: 158, Loss: 2.2083\n",
      "Epoch: 14, Index: 159, Loss: 0.0967\n",
      "Epoch: 14, Index: 160, Loss: 12.1376\n",
      "Epoch: 14, Index: 161, Loss: 0.4483\n",
      "Epoch: 14, Index: 162, Loss: 0.8378\n",
      "Epoch: 14, Index: 163, Loss: 1.9229\n",
      "Epoch: 14, Index: 164, Loss: 0.4882\n",
      "Epoch: 14, Index: 165, Loss: 0.6505\n",
      "Epoch: 14, Index: 166, Loss: 0.5394\n",
      "Epoch: 14, Index: 167, Loss: 0.1131\n",
      "Epoch: 14, Index: 168, Loss: 0.0832\n",
      "Epoch: 14, Index: 169, Loss: 2.1762\n",
      "Epoch: 14, Index: 170, Loss: 2.1038\n",
      "Epoch: 14, Index: 171, Loss: 0.6905\n",
      "Epoch: 14, Index: 172, Loss: 0.8895\n",
      "Epoch: 14, Index: 173, Loss: 2.5167\n",
      "Epoch: 14, Index: 174, Loss: 0.3871\n",
      "Epoch: 14, Index: 175, Loss: 2.0336\n",
      "Epoch: 14, Index: 176, Loss: 0.0430\n",
      "Epoch: 14, Index: 177, Loss: 3.8208\n",
      "Epoch: 14, Index: 178, Loss: 2.6005\n",
      "Epoch: 14, Index: 179, Loss: 0.9330\n",
      "Epoch: 14, Index: 180, Loss: 2.3423\n",
      "Epoch: 14, Index: 181, Loss: 0.0914\n",
      "Epoch: 14, Index: 182, Loss: 1.0354\n",
      "Epoch: 14, Index: 183, Loss: 1.0159\n",
      "Epoch: 14, Index: 184, Loss: 0.6649\n",
      "Epoch: 14, Index: 185, Loss: 2.4692\n",
      "Epoch: 14, Index: 186, Loss: 0.4313\n",
      "Epoch: 14, Index: 187, Loss: 2.2911\n",
      "Epoch: 14, Index: 188, Loss: 0.7642\n",
      "Epoch: 14, Index: 189, Loss: 2.2567\n",
      "Epoch: 14, Index: 190, Loss: 2.1069\n",
      "Epoch: 14, Index: 191, Loss: 1.5281\n",
      "Epoch: 14, Index: 192, Loss: 3.4762\n",
      "Epoch: 14, Index: 193, Loss: 1.4999\n",
      "Epoch: 14, Index: 194, Loss: 0.1124\n",
      "Epoch: 14, Index: 195, Loss: 2.8523\n",
      "Epoch: 14, Index: 196, Loss: 0.6611\n",
      "Epoch: 14, Index: 197, Loss: 6.6031\n",
      "Epoch: 14, Index: 198, Loss: 0.9402\n",
      "Epoch: 14, Index: 199, Loss: 0.6813\n",
      "Epoch: 14, Index: 200, Loss: 3.9480\n",
      "Epoch: 14, Index: 201, Loss: 0.0099\n",
      "Epoch: 14, Index: 202, Loss: 1.4335\n",
      "Epoch: 14, Index: 203, Loss: 0.1501\n",
      "Epoch: 14, Index: 204, Loss: 1.6042\n",
      "Epoch: 14, Index: 205, Loss: 0.5823\n",
      "Epoch: 14, Index: 206, Loss: 3.9251\n",
      "Epoch: 14, Index: 207, Loss: 1.6677\n",
      "Epoch: 14, Index: 208, Loss: 2.7102\n",
      "Epoch: 14, Index: 209, Loss: 3.1513\n",
      "Epoch: 14, Index: 210, Loss: 1.3345\n",
      "Epoch: 14, Index: 211, Loss: 6.4959\n",
      "Epoch: 14, Index: 212, Loss: 2.2779\n",
      "Epoch: 14, Index: 213, Loss: 3.1867\n",
      "Epoch: 14, Index: 214, Loss: 3.8979\n",
      "Epoch: 14, Index: 215, Loss: 1.4226\n",
      "Epoch: 14, Index: 216, Loss: 3.2541\n",
      "Epoch: 14, Index: 217, Loss: 0.7042\n",
      "Epoch: 14, Index: 218, Loss: 1.2970\n",
      "Epoch: 14, Index: 219, Loss: 0.3664\n",
      "Epoch: 14, Index: 220, Loss: 13.0018\n",
      "Epoch: 14, Index: 221, Loss: 1.8627\n",
      "Epoch: 14, Index: 222, Loss: 1.8585\n",
      "Epoch: 14, Index: 223, Loss: 6.7980\n",
      "Epoch: 14, Index: 224, Loss: 1.4018\n",
      "Epoch: 14, Index: 225, Loss: 0.3501\n",
      "Epoch: 14, Index: 226, Loss: 0.9305\n",
      "Epoch: 14, Index: 227, Loss: 0.9636\n",
      "Epoch: 14, Index: 228, Loss: 2.2755\n",
      "Epoch: 14, Index: 229, Loss: 0.9381\n",
      "Epoch: 14, Index: 230, Loss: 0.1627\n",
      "Epoch: 14, Index: 231, Loss: 0.5505\n",
      "Epoch: 14, Index: 232, Loss: 1.0405\n",
      "Epoch: 14, Index: 233, Loss: 1.2581\n",
      "Epoch: 14, Index: 234, Loss: 0.6515\n",
      "Epoch: 14, Index: 235, Loss: 0.7130\n",
      "Epoch: 14, Index: 236, Loss: 0.5815\n",
      "Epoch: 14, Index: 237, Loss: 3.4875\n",
      "Epoch: 14, Index: 238, Loss: 0.1326\n",
      "Epoch: 14, Index: 239, Loss: 1.3775\n",
      "Epoch: 14, Index: 240, Loss: 3.2120\n",
      "Epoch: 14, Index: 241, Loss: 0.9352\n",
      "Epoch: 14, Index: 242, Loss: 0.8626\n",
      "Epoch: 14, Index: 243, Loss: 1.4303\n",
      "Epoch: 14, Index: 244, Loss: 3.7713\n",
      "Epoch: 14, Index: 245, Loss: 0.1005\n",
      "Epoch: 14, Index: 246, Loss: 1.5835\n",
      "Epoch: 14, Index: 247, Loss: 0.5751\n",
      "Epoch: 14, Index: 248, Loss: 1.7827\n",
      "Epoch: 14, Index: 249, Loss: 1.2387\n",
      "Epoch: 14, Index: 250, Loss: 2.5328\n",
      "Epoch: 14, Index: 251, Loss: 0.2293\n",
      "Epoch: 14, Index: 252, Loss: 0.3715\n",
      "Epoch: 14, Index: 253, Loss: 0.3090\n",
      "Epoch: 14, Index: 254, Loss: 0.8854\n",
      "Epoch: 14, Index: 255, Loss: 1.5007\n",
      "Epoch: 14, Index: 256, Loss: 0.9049\n",
      "Epoch: 14, Index: 257, Loss: 0.4062\n",
      "Epoch: 14, Index: 258, Loss: 1.7245\n",
      "Epoch: 14, Index: 259, Loss: 0.7678\n",
      "Epoch: 14, Index: 260, Loss: 1.9271\n",
      "Epoch: 14, Index: 261, Loss: 3.4881\n",
      "Epoch: 14, Index: 262, Loss: 0.4783\n",
      "Epoch: 14, Index: 263, Loss: 0.9244\n",
      "Epoch: 14, Index: 264, Loss: 2.3060\n",
      "Epoch: 14, Index: 265, Loss: 1.6412\n",
      "Epoch: 14, Index: 266, Loss: 0.8636\n",
      "Epoch: 14, Index: 267, Loss: 4.2145\n",
      "Epoch: 14, Index: 268, Loss: 1.4180\n",
      "Epoch: 14, Index: 269, Loss: 2.9004\n",
      "Epoch: 14, Index: 270, Loss: 0.6690\n",
      "Epoch: 14, Index: 271, Loss: 5.4783\n",
      "Epoch: 14, Index: 272, Loss: 0.0343\n",
      "Epoch: 14, Index: 273, Loss: 0.0587\n",
      "Epoch: 14, Index: 274, Loss: 0.1787\n",
      "Epoch: 14, Index: 275, Loss: 1.1439\n",
      "Epoch: 14, Index: 276, Loss: 1.0262\n",
      "Epoch: 14, Index: 277, Loss: 1.5354\n",
      "Epoch: 14, Index: 278, Loss: 0.2258\n",
      "Epoch: 14, Index: 279, Loss: 3.0457\n",
      "Epoch: 14, Index: 280, Loss: 1.9481\n",
      "Epoch: 14, Index: 281, Loss: 1.1978\n",
      "Epoch: 14, Index: 282, Loss: 0.8162\n",
      "Epoch: 14, Index: 283, Loss: 2.1473\n",
      "Epoch: 14, Index: 284, Loss: 0.5727\n",
      "Epoch: 14, Index: 285, Loss: 1.7046\n",
      "Epoch: 14, Index: 286, Loss: 0.6322\n",
      "Epoch: 14, Index: 287, Loss: 2.6698\n",
      "Epoch: 14, Index: 288, Loss: 1.1871\n",
      "Epoch: 14, Index: 289, Loss: 0.2264\n",
      "Epoch: 14, Index: 290, Loss: 0.4407\n",
      "Epoch: 14, Index: 291, Loss: 0.4402\n",
      "Epoch: 14, Index: 292, Loss: 0.3536\n",
      "Epoch: 14, Index: 293, Loss: 0.1183\n",
      "Epoch: 14, Index: 294, Loss: 6.7997\n",
      "Epoch: 14, Index: 295, Loss: 0.5453\n",
      "Epoch: 14, Index: 296, Loss: 2.9200\n",
      "Epoch: 14, Index: 297, Loss: 2.7638\n",
      "Epoch: 14, Index: 298, Loss: 1.9800\n",
      "Epoch: 14, Index: 299, Loss: 1.1745\n",
      "Epoch: 14, Index: 300, Loss: 1.1516\n",
      "Epoch: 14, Index: 301, Loss: 0.3617\n",
      "Epoch: 14, Index: 302, Loss: 0.9805\n",
      "Epoch: 14, Index: 303, Loss: 0.3298\n",
      "Epoch: 14, Index: 304, Loss: 4.7292\n",
      "Epoch: 14, Index: 305, Loss: 0.7449\n",
      "Epoch: 14, Index: 306, Loss: 0.7452\n",
      "Epoch: 14, Index: 307, Loss: 1.8379\n",
      "Epoch: 14, Index: 308, Loss: 3.5940\n",
      "Epoch: 14, Index: 309, Loss: 2.2932\n",
      "Epoch: 14, Index: 310, Loss: 2.8320\n",
      "Epoch: 14, Index: 311, Loss: 0.9545\n",
      "Epoch: 14, Index: 312, Loss: 6.6350\n",
      "Epoch: 14, Index: 313, Loss: 2.2152\n",
      "Epoch: 14, Index: 314, Loss: 0.3023\n",
      "Epoch: 14, Index: 315, Loss: 2.7988\n",
      "Epoch: 14, Index: 316, Loss: 0.1040\n",
      "Epoch: 14, Index: 317, Loss: 1.1196\n",
      "Epoch: 14, Index: 318, Loss: 0.2192\n",
      "Epoch: 14, Index: 319, Loss: 0.0429\n",
      "Epoch: 14, Index: 320, Loss: 0.6780\n",
      "Epoch: 14, Index: 321, Loss: 1.1554\n",
      "Epoch: 14, Index: 322, Loss: 1.1626\n",
      "Epoch: 14, Index: 323, Loss: 0.2716\n",
      "Epoch: 14, Index: 324, Loss: 0.9993\n",
      "Epoch: 14, Index: 325, Loss: 2.6240\n",
      "Epoch: 14, Index: 326, Loss: 0.7169\n",
      "Epoch: 14, Index: 327, Loss: 2.6978\n",
      "Epoch: 14, Index: 328, Loss: 0.7448\n",
      "Epoch: 14, Index: 329, Loss: 7.3189\n",
      "Epoch: 14, Index: 330, Loss: 1.2936\n",
      "Epoch: 14, Index: 331, Loss: 1.9283\n",
      "Epoch: 14, Index: 332, Loss: 5.3853\n",
      "Epoch: 14, Index: 333, Loss: 0.0298\n",
      "Epoch: 14, Index: 334, Loss: 0.5493\n",
      "Epoch: 14, Index: 335, Loss: 0.7207\n",
      "Epoch: 14, Index: 336, Loss: 6.0840\n",
      "Epoch: 14, Index: 337, Loss: 2.3816\n",
      "Epoch: 14, Index: 338, Loss: 0.2667\n",
      "Epoch: 14, Index: 339, Loss: 0.2360\n",
      "Epoch: 14, Index: 340, Loss: 2.8317\n",
      "Epoch: 14, Index: 341, Loss: 1.3467\n",
      "Epoch: 14, Index: 342, Loss: 1.0504\n",
      "Epoch: 14, Index: 343, Loss: 0.4760\n",
      "Epoch: 14, Index: 344, Loss: 0.5232\n",
      "Epoch: 14, Index: 345, Loss: 5.8429\n",
      "Epoch: 14, Index: 346, Loss: 0.2385\n",
      "Epoch: 14, Index: 347, Loss: 0.6626\n",
      "Epoch: 14, Index: 348, Loss: 2.6974\n",
      "Epoch: 14, Index: 349, Loss: 0.1764\n",
      "Epoch: 14, Index: 350, Loss: 1.0303\n",
      "Epoch: 14, Index: 351, Loss: 1.4459\n",
      "Epoch: 14, Index: 352, Loss: 3.7331\n",
      "Epoch: 14, Index: 353, Loss: 0.0738\n",
      "Epoch: 14, Index: 354, Loss: 2.8356\n",
      "Epoch: 14, Index: 355, Loss: 1.0069\n",
      "Epoch: 14, Index: 356, Loss: 0.0362\n",
      "Epoch: 14, Index: 357, Loss: 3.3142\n",
      "Epoch: 14, Index: 358, Loss: 1.4886\n",
      "Epoch: 14, Index: 359, Loss: 0.9593\n",
      "Epoch: 14, Index: 360, Loss: 3.0744\n",
      "Epoch: 14, Index: 361, Loss: 1.9585\n",
      "Epoch: 14, Index: 362, Loss: 2.2495\n",
      "Epoch: 14, Index: 363, Loss: 0.4498\n",
      "Epoch: 14, Index: 364, Loss: 2.3221\n",
      "Epoch: 14, Index: 365, Loss: 2.8091\n",
      "Epoch: 14, Index: 366, Loss: 2.3669\n",
      "Epoch: 14, Index: 367, Loss: 1.5765\n",
      "Epoch: 14, Index: 368, Loss: 2.2261\n",
      "Epoch: 14, Index: 369, Loss: 1.2784\n",
      "Epoch: 14, Index: 370, Loss: 5.2687\n",
      "Epoch: 14, Index: 371, Loss: 4.9253\n",
      "Epoch: 14, Index: 372, Loss: 4.1140\n",
      "Epoch: 14, Index: 373, Loss: 5.7225\n",
      "Epoch: 14, Index: 374, Loss: 1.3934\n",
      "Epoch: 14, Index: 375, Loss: 0.0024\n",
      "Epoch: 14, Index: 376, Loss: 1.1700\n",
      "Epoch: 14, Index: 377, Loss: 0.2124\n",
      "Epoch: 14, Index: 378, Loss: 1.5669\n",
      "Epoch: 14, Index: 379, Loss: 0.4574\n",
      "Epoch: 14, Index: 380, Loss: 1.0408\n",
      "Epoch: 14, Index: 381, Loss: 0.2364\n",
      "Epoch: 14, Index: 382, Loss: 2.4841\n",
      "Epoch: 14, Index: 383, Loss: 4.9249\n",
      "Epoch: 14, Index: 384, Loss: 0.0353\n",
      "Epoch: 14, Index: 385, Loss: 2.5401\n",
      "Epoch: 14, Index: 386, Loss: 2.4460\n",
      "Epoch: 14, Index: 387, Loss: 2.9377\n",
      "Epoch: 14, Index: 388, Loss: 1.5048\n",
      "Epoch: 14, Index: 389, Loss: 3.0808\n",
      "Epoch: 14, Index: 390, Loss: 0.5471\n",
      "Epoch: 14, Index: 391, Loss: 0.0915\n",
      "Epoch: 14, Index: 392, Loss: 4.8230\n",
      "Epoch: 14, Index: 393, Loss: 1.3608\n",
      "Epoch: 14, Index: 394, Loss: 1.3114\n",
      "Epoch: 14, Index: 395, Loss: 3.2508\n",
      "Epoch: 14, Index: 396, Loss: 0.4706\n",
      "Epoch: 14, Index: 397, Loss: 6.3330\n",
      "Epoch: 14, Index: 398, Loss: 1.3707\n",
      "Epoch: 14, Index: 399, Loss: 1.2991\n",
      "Epoch: 14, Index: 400, Loss: 0.9675\n",
      "Epoch: 14, Index: 401, Loss: 0.8619\n",
      "Epoch: 14, Index: 402, Loss: 1.2419\n",
      "Epoch: 14, Index: 403, Loss: 1.3802\n",
      "Epoch: 14, Index: 404, Loss: 0.9160\n",
      "Epoch: 14, Index: 405, Loss: 5.7993\n",
      "Epoch: 14, Index: 406, Loss: 2.5083\n",
      "Epoch: 14, Index: 407, Loss: 0.3203\n",
      "Epoch: 14, Index: 408, Loss: 1.6075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3ec51ea9c5463e900279b0c0c528ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Index: 0, Loss: 3.5834\n",
      "Epoch: 15, Index: 1, Loss: 0.6838\n",
      "Epoch: 15, Index: 2, Loss: 1.6715\n",
      "Epoch: 15, Index: 3, Loss: 0.5114\n",
      "Epoch: 15, Index: 4, Loss: 0.1496\n",
      "Epoch: 15, Index: 5, Loss: 0.6035\n",
      "Epoch: 15, Index: 6, Loss: 0.6776\n",
      "Epoch: 15, Index: 7, Loss: 0.2783\n",
      "Epoch: 15, Index: 8, Loss: 1.6193\n",
      "Epoch: 15, Index: 9, Loss: 1.6200\n",
      "Epoch: 15, Index: 10, Loss: 0.3077\n",
      "Epoch: 15, Index: 11, Loss: 1.5971\n",
      "Epoch: 15, Index: 12, Loss: 0.9070\n",
      "Epoch: 15, Index: 13, Loss: 2.1373\n",
      "Epoch: 15, Index: 14, Loss: 1.0404\n",
      "Epoch: 15, Index: 15, Loss: 0.1257\n",
      "Epoch: 15, Index: 16, Loss: 5.1144\n",
      "Epoch: 15, Index: 17, Loss: 19.1741\n",
      "Epoch: 15, Index: 18, Loss: 1.5292\n",
      "Epoch: 15, Index: 19, Loss: 0.4089\n",
      "Epoch: 15, Index: 20, Loss: 1.3477\n",
      "Epoch: 15, Index: 21, Loss: 0.3750\n",
      "Epoch: 15, Index: 22, Loss: 1.4933\n",
      "Epoch: 15, Index: 23, Loss: 2.6482\n",
      "Epoch: 15, Index: 24, Loss: 0.6020\n",
      "Epoch: 15, Index: 25, Loss: 0.0712\n",
      "Epoch: 15, Index: 26, Loss: 0.6926\n",
      "Epoch: 15, Index: 27, Loss: 2.5446\n",
      "Epoch: 15, Index: 28, Loss: 2.8709\n",
      "Epoch: 15, Index: 29, Loss: 2.3723\n",
      "Epoch: 15, Index: 30, Loss: 5.4452\n",
      "Epoch: 15, Index: 31, Loss: 1.2778\n",
      "Epoch: 15, Index: 32, Loss: 0.4872\n",
      "Epoch: 15, Index: 33, Loss: 3.1465\n",
      "Epoch: 15, Index: 34, Loss: 4.7516\n",
      "Epoch: 15, Index: 35, Loss: 2.6419\n",
      "Epoch: 15, Index: 36, Loss: 1.0864\n",
      "Epoch: 15, Index: 37, Loss: 1.8117\n",
      "Epoch: 15, Index: 38, Loss: 0.6672\n",
      "Epoch: 15, Index: 39, Loss: 0.5936\n",
      "Epoch: 15, Index: 40, Loss: 0.3720\n",
      "Epoch: 15, Index: 41, Loss: 0.3715\n",
      "Epoch: 15, Index: 42, Loss: 0.0232\n",
      "Epoch: 15, Index: 43, Loss: 0.6977\n",
      "Epoch: 15, Index: 44, Loss: 1.2767\n",
      "Epoch: 15, Index: 45, Loss: 1.0918\n",
      "Epoch: 15, Index: 46, Loss: 7.4764\n",
      "Epoch: 15, Index: 47, Loss: 0.0985\n",
      "Epoch: 15, Index: 48, Loss: 6.0938\n",
      "Epoch: 15, Index: 49, Loss: 1.4939\n",
      "Epoch: 15, Index: 50, Loss: 1.7663\n",
      "Epoch: 15, Index: 51, Loss: 0.2838\n",
      "Epoch: 15, Index: 52, Loss: 3.3899\n",
      "Epoch: 15, Index: 53, Loss: 1.2497\n",
      "Epoch: 15, Index: 54, Loss: 0.4505\n",
      "Epoch: 15, Index: 55, Loss: 0.0761\n",
      "Epoch: 15, Index: 56, Loss: 2.8316\n",
      "Epoch: 15, Index: 57, Loss: 0.8837\n",
      "Epoch: 15, Index: 58, Loss: 3.0874\n",
      "Epoch: 15, Index: 59, Loss: 2.4687\n",
      "Epoch: 15, Index: 60, Loss: 2.6626\n",
      "Epoch: 15, Index: 61, Loss: 1.6792\n",
      "Epoch: 15, Index: 62, Loss: 1.1786\n",
      "Epoch: 15, Index: 63, Loss: 1.1490\n",
      "Epoch: 15, Index: 64, Loss: 0.1587\n",
      "Epoch: 15, Index: 65, Loss: 0.7546\n",
      "Epoch: 15, Index: 66, Loss: 0.1599\n",
      "Epoch: 15, Index: 67, Loss: 0.7146\n",
      "Epoch: 15, Index: 68, Loss: 0.1549\n",
      "Epoch: 15, Index: 69, Loss: 4.1664\n",
      "Epoch: 15, Index: 70, Loss: 0.0947\n",
      "Epoch: 15, Index: 71, Loss: 0.2230\n",
      "Epoch: 15, Index: 72, Loss: 2.1213\n",
      "Epoch: 15, Index: 73, Loss: 4.6813\n",
      "Epoch: 15, Index: 74, Loss: 0.3020\n",
      "Epoch: 15, Index: 75, Loss: 0.6881\n",
      "Epoch: 15, Index: 76, Loss: 1.0102\n",
      "Epoch: 15, Index: 77, Loss: 2.3691\n",
      "Epoch: 15, Index: 78, Loss: 0.0025\n",
      "Epoch: 15, Index: 79, Loss: 3.6738\n",
      "Epoch: 15, Index: 80, Loss: 0.1781\n",
      "Epoch: 15, Index: 81, Loss: 0.4875\n",
      "Epoch: 15, Index: 82, Loss: 1.7142\n",
      "Epoch: 15, Index: 83, Loss: 2.1895\n",
      "Epoch: 15, Index: 84, Loss: 0.7972\n",
      "Epoch: 15, Index: 85, Loss: 1.9046\n",
      "Epoch: 15, Index: 86, Loss: 0.7819\n",
      "Epoch: 15, Index: 87, Loss: 0.1234\n",
      "Epoch: 15, Index: 88, Loss: 0.1701\n",
      "Epoch: 15, Index: 89, Loss: 4.4860\n",
      "Epoch: 15, Index: 90, Loss: 2.4234\n",
      "Epoch: 15, Index: 91, Loss: 6.0871\n",
      "Epoch: 15, Index: 92, Loss: 1.2506\n",
      "Epoch: 15, Index: 93, Loss: 0.8177\n",
      "Epoch: 15, Index: 94, Loss: 0.4072\n",
      "Epoch: 15, Index: 95, Loss: 7.3964\n",
      "Epoch: 15, Index: 96, Loss: 2.8037\n",
      "Epoch: 15, Index: 97, Loss: 0.2747\n",
      "Epoch: 15, Index: 98, Loss: 4.3956\n",
      "Epoch: 15, Index: 99, Loss: 1.6293\n",
      "Epoch: 15, Index: 100, Loss: 2.9204\n",
      "Epoch: 15, Index: 101, Loss: 0.0822\n",
      "Epoch: 15, Index: 102, Loss: 2.0533\n",
      "Epoch: 15, Index: 103, Loss: 0.6449\n",
      "Epoch: 15, Index: 104, Loss: 1.7009\n",
      "Epoch: 15, Index: 105, Loss: 0.0404\n",
      "Epoch: 15, Index: 106, Loss: 0.1763\n",
      "Epoch: 15, Index: 107, Loss: 2.8309\n",
      "Epoch: 15, Index: 108, Loss: 0.2750\n",
      "Epoch: 15, Index: 109, Loss: 5.5352\n",
      "Epoch: 15, Index: 110, Loss: 3.0266\n",
      "Epoch: 15, Index: 111, Loss: 1.0716\n",
      "Epoch: 15, Index: 112, Loss: 2.6299\n",
      "Epoch: 15, Index: 113, Loss: 1.0491\n",
      "Epoch: 15, Index: 114, Loss: 0.7775\n",
      "Epoch: 15, Index: 115, Loss: 0.1045\n",
      "Epoch: 15, Index: 116, Loss: 0.5334\n",
      "Epoch: 15, Index: 117, Loss: 0.1708\n",
      "Epoch: 15, Index: 118, Loss: 0.1599\n",
      "Epoch: 15, Index: 119, Loss: 2.2447\n",
      "Epoch: 15, Index: 120, Loss: 0.0523\n",
      "Epoch: 15, Index: 121, Loss: 0.4086\n",
      "Epoch: 15, Index: 122, Loss: 0.7744\n",
      "Epoch: 15, Index: 123, Loss: 1.2144\n",
      "Epoch: 15, Index: 124, Loss: 0.1050\n",
      "Epoch: 15, Index: 125, Loss: 3.0502\n",
      "Epoch: 15, Index: 126, Loss: 1.8142\n",
      "Epoch: 15, Index: 127, Loss: 0.3609\n",
      "Epoch: 15, Index: 128, Loss: 1.1292\n",
      "Epoch: 15, Index: 129, Loss: 0.3607\n",
      "Epoch: 15, Index: 130, Loss: 1.2017\n",
      "Epoch: 15, Index: 131, Loss: 1.2856\n",
      "Epoch: 15, Index: 132, Loss: 0.2131\n",
      "Epoch: 15, Index: 133, Loss: 0.0128\n",
      "Epoch: 15, Index: 134, Loss: 0.9351\n",
      "Epoch: 15, Index: 135, Loss: 1.0988\n",
      "Epoch: 15, Index: 136, Loss: 0.6052\n",
      "Epoch: 15, Index: 137, Loss: 4.3419\n",
      "Epoch: 15, Index: 138, Loss: 1.9807\n",
      "Epoch: 15, Index: 139, Loss: 0.5482\n",
      "Epoch: 15, Index: 140, Loss: 5.8061\n",
      "Epoch: 15, Index: 141, Loss: 2.2960\n",
      "Epoch: 15, Index: 142, Loss: 1.9383\n",
      "Epoch: 15, Index: 143, Loss: 8.6592\n",
      "Epoch: 15, Index: 144, Loss: 9.6121\n",
      "Epoch: 15, Index: 145, Loss: 1.3779\n",
      "Epoch: 15, Index: 146, Loss: 3.3778\n",
      "Epoch: 15, Index: 147, Loss: 1.7889\n",
      "Epoch: 15, Index: 148, Loss: 0.2160\n",
      "Epoch: 15, Index: 149, Loss: 1.1357\n",
      "Epoch: 15, Index: 150, Loss: 1.0601\n",
      "Epoch: 15, Index: 151, Loss: 2.1746\n",
      "Epoch: 15, Index: 152, Loss: 3.0081\n",
      "Epoch: 15, Index: 153, Loss: 0.3331\n",
      "Epoch: 15, Index: 154, Loss: 0.5163\n",
      "Epoch: 15, Index: 155, Loss: 1.9987\n",
      "Epoch: 15, Index: 156, Loss: 2.0636\n",
      "Epoch: 15, Index: 157, Loss: 0.8537\n",
      "Epoch: 15, Index: 158, Loss: 0.1825\n",
      "Epoch: 15, Index: 159, Loss: 0.4015\n",
      "Epoch: 15, Index: 160, Loss: 0.3911\n",
      "Epoch: 15, Index: 161, Loss: 0.2295\n",
      "Epoch: 15, Index: 162, Loss: 1.7807\n",
      "Epoch: 15, Index: 163, Loss: 0.4067\n",
      "Epoch: 15, Index: 164, Loss: 0.2594\n",
      "Epoch: 15, Index: 165, Loss: 0.0206\n",
      "Epoch: 15, Index: 166, Loss: 0.5825\n",
      "Epoch: 15, Index: 167, Loss: 1.5533\n",
      "Epoch: 15, Index: 168, Loss: 0.0101\n",
      "Epoch: 15, Index: 169, Loss: 0.3197\n",
      "Epoch: 15, Index: 170, Loss: 3.0030\n",
      "Epoch: 15, Index: 171, Loss: 3.0089\n",
      "Epoch: 15, Index: 172, Loss: 0.2913\n",
      "Epoch: 15, Index: 173, Loss: 6.8060\n",
      "Epoch: 15, Index: 174, Loss: 1.7105\n",
      "Epoch: 15, Index: 175, Loss: 4.2803\n",
      "Epoch: 15, Index: 176, Loss: 1.8194\n",
      "Epoch: 15, Index: 177, Loss: 1.8362\n",
      "Epoch: 15, Index: 178, Loss: 0.8954\n",
      "Epoch: 15, Index: 179, Loss: 0.6506\n",
      "Epoch: 15, Index: 180, Loss: 0.2037\n",
      "Epoch: 15, Index: 181, Loss: 1.5155\n",
      "Epoch: 15, Index: 182, Loss: 1.1223\n",
      "Epoch: 15, Index: 183, Loss: 0.4505\n",
      "Epoch: 15, Index: 184, Loss: 3.0176\n",
      "Epoch: 15, Index: 185, Loss: 1.7975\n",
      "Epoch: 15, Index: 186, Loss: 1.4620\n",
      "Epoch: 15, Index: 187, Loss: 0.0357\n",
      "Epoch: 15, Index: 188, Loss: 8.2338\n",
      "Epoch: 15, Index: 189, Loss: 3.2663\n",
      "Epoch: 15, Index: 190, Loss: 1.4414\n",
      "Epoch: 15, Index: 191, Loss: 0.8049\n",
      "Epoch: 15, Index: 192, Loss: 0.4551\n",
      "Epoch: 15, Index: 193, Loss: 4.7233\n",
      "Epoch: 15, Index: 194, Loss: 1.5743\n",
      "Epoch: 15, Index: 195, Loss: 1.6465\n",
      "Epoch: 15, Index: 196, Loss: 2.3141\n",
      "Epoch: 15, Index: 197, Loss: 0.3430\n",
      "Epoch: 15, Index: 198, Loss: 1.7009\n",
      "Epoch: 15, Index: 199, Loss: 0.4341\n",
      "Epoch: 15, Index: 200, Loss: 1.7682\n",
      "Epoch: 15, Index: 201, Loss: 0.7787\n",
      "Epoch: 15, Index: 202, Loss: 1.9842\n",
      "Epoch: 15, Index: 203, Loss: 0.8278\n",
      "Epoch: 15, Index: 204, Loss: 1.8854\n",
      "Epoch: 15, Index: 205, Loss: 0.3794\n",
      "Epoch: 15, Index: 206, Loss: 0.6948\n",
      "Epoch: 15, Index: 207, Loss: 1.3987\n",
      "Epoch: 15, Index: 208, Loss: 1.8888\n",
      "Epoch: 15, Index: 209, Loss: 0.7010\n",
      "Epoch: 15, Index: 210, Loss: 5.5656\n",
      "Epoch: 15, Index: 211, Loss: 1.2215\n",
      "Epoch: 15, Index: 212, Loss: 1.0084\n",
      "Epoch: 15, Index: 213, Loss: 2.7046\n",
      "Epoch: 15, Index: 214, Loss: 2.0311\n",
      "Epoch: 15, Index: 215, Loss: 16.5127\n",
      "Epoch: 15, Index: 216, Loss: 0.0202\n",
      "Epoch: 15, Index: 217, Loss: 0.2979\n",
      "Epoch: 15, Index: 218, Loss: 3.4630\n",
      "Epoch: 15, Index: 219, Loss: 7.6110\n",
      "Epoch: 15, Index: 220, Loss: 3.6654\n",
      "Epoch: 15, Index: 221, Loss: 0.2629\n",
      "Epoch: 15, Index: 222, Loss: 1.3893\n",
      "Epoch: 15, Index: 223, Loss: 7.9118\n",
      "Epoch: 15, Index: 224, Loss: 0.0937\n",
      "Epoch: 15, Index: 225, Loss: 2.0986\n",
      "Epoch: 15, Index: 226, Loss: 0.2850\n",
      "Epoch: 15, Index: 227, Loss: 1.2975\n",
      "Epoch: 15, Index: 228, Loss: 0.0657\n",
      "Epoch: 15, Index: 229, Loss: 2.0683\n",
      "Epoch: 15, Index: 230, Loss: 0.5212\n",
      "Epoch: 15, Index: 231, Loss: 0.7275\n",
      "Epoch: 15, Index: 232, Loss: 0.9188\n",
      "Epoch: 15, Index: 233, Loss: 0.6603\n",
      "Epoch: 15, Index: 234, Loss: 0.7115\n",
      "Epoch: 15, Index: 235, Loss: 0.3985\n",
      "Epoch: 15, Index: 236, Loss: 2.4180\n",
      "Epoch: 15, Index: 237, Loss: 0.5796\n",
      "Epoch: 15, Index: 238, Loss: 1.1489\n",
      "Epoch: 15, Index: 239, Loss: 3.1157\n",
      "Epoch: 15, Index: 240, Loss: 1.0957\n",
      "Epoch: 15, Index: 241, Loss: 1.4653\n",
      "Epoch: 15, Index: 242, Loss: 3.4339\n",
      "Epoch: 15, Index: 243, Loss: 2.2045\n",
      "Epoch: 15, Index: 244, Loss: 5.0132\n",
      "Epoch: 15, Index: 245, Loss: 1.9164\n",
      "Epoch: 15, Index: 246, Loss: 0.1132\n",
      "Epoch: 15, Index: 247, Loss: 1.1350\n",
      "Epoch: 15, Index: 248, Loss: 4.7040\n",
      "Epoch: 15, Index: 249, Loss: 1.5476\n",
      "Epoch: 15, Index: 250, Loss: 2.8208\n",
      "Epoch: 15, Index: 251, Loss: 0.2819\n",
      "Epoch: 15, Index: 252, Loss: 5.6295\n",
      "Epoch: 15, Index: 253, Loss: 2.9399\n",
      "Epoch: 15, Index: 254, Loss: 2.3073\n",
      "Epoch: 15, Index: 255, Loss: 2.1809\n",
      "Epoch: 15, Index: 256, Loss: 1.8994\n",
      "Epoch: 15, Index: 257, Loss: 2.1692\n",
      "Epoch: 15, Index: 258, Loss: 0.7453\n",
      "Epoch: 15, Index: 259, Loss: 0.5839\n",
      "Epoch: 15, Index: 260, Loss: 4.7436\n",
      "Epoch: 15, Index: 261, Loss: 0.5474\n",
      "Epoch: 15, Index: 262, Loss: 0.8980\n",
      "Epoch: 15, Index: 263, Loss: 3.8483\n",
      "Epoch: 15, Index: 264, Loss: 0.2189\n",
      "Epoch: 15, Index: 265, Loss: 0.6943\n",
      "Epoch: 15, Index: 266, Loss: 0.2631\n",
      "Epoch: 15, Index: 267, Loss: 0.3534\n",
      "Epoch: 15, Index: 268, Loss: 1.0869\n",
      "Epoch: 15, Index: 269, Loss: 1.5791\n",
      "Epoch: 15, Index: 270, Loss: 1.0846\n",
      "Epoch: 15, Index: 271, Loss: 1.4142\n",
      "Epoch: 15, Index: 272, Loss: 1.5877\n",
      "Epoch: 15, Index: 273, Loss: 0.5097\n",
      "Epoch: 15, Index: 274, Loss: 1.0114\n",
      "Epoch: 15, Index: 275, Loss: 1.5340\n",
      "Epoch: 15, Index: 276, Loss: 0.0006\n",
      "Epoch: 15, Index: 277, Loss: 3.6891\n",
      "Epoch: 15, Index: 278, Loss: 2.3068\n",
      "Epoch: 15, Index: 279, Loss: 0.4785\n",
      "Epoch: 15, Index: 280, Loss: 3.7546\n",
      "Epoch: 15, Index: 281, Loss: 2.7306\n",
      "Epoch: 15, Index: 282, Loss: 7.4760\n",
      "Epoch: 15, Index: 283, Loss: 0.2896\n",
      "Epoch: 15, Index: 284, Loss: 5.0522\n",
      "Epoch: 15, Index: 285, Loss: 0.8040\n",
      "Epoch: 15, Index: 286, Loss: 1.5262\n",
      "Epoch: 15, Index: 287, Loss: 2.8143\n",
      "Epoch: 15, Index: 288, Loss: 0.5884\n",
      "Epoch: 15, Index: 289, Loss: 1.1251\n",
      "Epoch: 15, Index: 290, Loss: 1.7809\n",
      "Epoch: 15, Index: 291, Loss: 0.1855\n",
      "Epoch: 15, Index: 292, Loss: 1.2678\n",
      "Epoch: 15, Index: 293, Loss: 0.2090\n",
      "Epoch: 15, Index: 294, Loss: 3.9640\n",
      "Epoch: 15, Index: 295, Loss: 0.1125\n",
      "Epoch: 15, Index: 296, Loss: 0.7026\n",
      "Epoch: 15, Index: 297, Loss: 4.8688\n",
      "Epoch: 15, Index: 298, Loss: 3.1590\n",
      "Epoch: 15, Index: 299, Loss: 0.6494\n",
      "Epoch: 15, Index: 300, Loss: 0.9709\n",
      "Epoch: 15, Index: 301, Loss: 4.3873\n",
      "Epoch: 15, Index: 302, Loss: 1.8989\n",
      "Epoch: 15, Index: 303, Loss: 3.2238\n",
      "Epoch: 15, Index: 304, Loss: 0.5036\n",
      "Epoch: 15, Index: 305, Loss: 2.0148\n",
      "Epoch: 15, Index: 306, Loss: 0.2594\n",
      "Epoch: 15, Index: 307, Loss: 0.8717\n",
      "Epoch: 15, Index: 308, Loss: 0.6684\n",
      "Epoch: 15, Index: 309, Loss: 1.1138\n",
      "Epoch: 15, Index: 310, Loss: 1.5760\n",
      "Epoch: 15, Index: 311, Loss: 3.1729\n",
      "Epoch: 15, Index: 312, Loss: 0.1281\n",
      "Epoch: 15, Index: 313, Loss: 1.2736\n",
      "Epoch: 15, Index: 314, Loss: 0.4463\n",
      "Epoch: 15, Index: 315, Loss: 5.5250\n",
      "Epoch: 15, Index: 316, Loss: 0.3943\n",
      "Epoch: 15, Index: 317, Loss: 2.9246\n",
      "Epoch: 15, Index: 318, Loss: 1.8159\n",
      "Epoch: 15, Index: 319, Loss: 1.5878\n",
      "Epoch: 15, Index: 320, Loss: 0.6772\n",
      "Epoch: 15, Index: 321, Loss: 1.7642\n",
      "Epoch: 15, Index: 322, Loss: 1.9190\n",
      "Epoch: 15, Index: 323, Loss: 0.5259\n",
      "Epoch: 15, Index: 324, Loss: 6.4790\n",
      "Epoch: 15, Index: 325, Loss: 0.3501\n",
      "Epoch: 15, Index: 326, Loss: 0.5603\n",
      "Epoch: 15, Index: 327, Loss: 0.2741\n",
      "Epoch: 15, Index: 328, Loss: 1.0563\n",
      "Epoch: 15, Index: 329, Loss: 0.9496\n",
      "Epoch: 15, Index: 330, Loss: 0.1716\n",
      "Epoch: 15, Index: 331, Loss: 2.4004\n",
      "Epoch: 15, Index: 332, Loss: 1.0061\n",
      "Epoch: 15, Index: 333, Loss: 0.7376\n",
      "Epoch: 15, Index: 334, Loss: 1.1133\n",
      "Epoch: 15, Index: 335, Loss: 1.4167\n",
      "Epoch: 15, Index: 336, Loss: 8.3625\n",
      "Epoch: 15, Index: 337, Loss: 1.6205\n",
      "Epoch: 15, Index: 338, Loss: 0.5213\n",
      "Epoch: 15, Index: 339, Loss: 1.1705\n",
      "Epoch: 15, Index: 340, Loss: 2.4563\n",
      "Epoch: 15, Index: 341, Loss: 4.3994\n",
      "Epoch: 15, Index: 342, Loss: 2.2614\n",
      "Epoch: 15, Index: 343, Loss: 0.8307\n",
      "Epoch: 15, Index: 344, Loss: 1.8352\n",
      "Epoch: 15, Index: 345, Loss: 5.2807\n",
      "Epoch: 15, Index: 346, Loss: 1.4062\n",
      "Epoch: 15, Index: 347, Loss: 3.7099\n",
      "Epoch: 15, Index: 348, Loss: 0.6700\n",
      "Epoch: 15, Index: 349, Loss: 1.9832\n",
      "Epoch: 15, Index: 350, Loss: 4.8535\n",
      "Epoch: 15, Index: 351, Loss: 2.9172\n",
      "Epoch: 15, Index: 352, Loss: 0.0153\n",
      "Epoch: 15, Index: 353, Loss: 3.9818\n",
      "Epoch: 15, Index: 354, Loss: 1.1071\n",
      "Epoch: 15, Index: 355, Loss: 0.5824\n",
      "Epoch: 15, Index: 356, Loss: 0.3461\n",
      "Epoch: 15, Index: 357, Loss: 2.2354\n",
      "Epoch: 15, Index: 358, Loss: 0.1768\n",
      "Epoch: 15, Index: 359, Loss: 0.4393\n",
      "Epoch: 15, Index: 360, Loss: 0.4605\n",
      "Epoch: 15, Index: 361, Loss: 4.4064\n",
      "Epoch: 15, Index: 362, Loss: 0.5942\n",
      "Epoch: 15, Index: 363, Loss: 1.5838\n",
      "Epoch: 15, Index: 364, Loss: 0.0480\n",
      "Epoch: 15, Index: 365, Loss: 0.1877\n",
      "Epoch: 15, Index: 366, Loss: 2.2493\n",
      "Epoch: 15, Index: 367, Loss: 1.9653\n",
      "Epoch: 15, Index: 368, Loss: 1.2909\n",
      "Epoch: 15, Index: 369, Loss: 1.3570\n",
      "Epoch: 15, Index: 370, Loss: 0.4385\n",
      "Epoch: 15, Index: 371, Loss: 3.9661\n",
      "Epoch: 15, Index: 372, Loss: 0.3276\n",
      "Epoch: 15, Index: 373, Loss: 3.3928\n",
      "Epoch: 15, Index: 374, Loss: 0.1644\n",
      "Epoch: 15, Index: 375, Loss: 1.2497\n",
      "Epoch: 15, Index: 376, Loss: 0.7667\n",
      "Epoch: 15, Index: 377, Loss: 0.4882\n",
      "Epoch: 15, Index: 378, Loss: 0.5480\n",
      "Epoch: 15, Index: 379, Loss: 1.8169\n",
      "Epoch: 15, Index: 380, Loss: 2.7830\n",
      "Epoch: 15, Index: 381, Loss: 4.4400\n",
      "Epoch: 15, Index: 382, Loss: 1.6460\n",
      "Epoch: 15, Index: 383, Loss: 1.4554\n",
      "Epoch: 15, Index: 384, Loss: 4.7519\n",
      "Epoch: 15, Index: 385, Loss: 0.5956\n",
      "Epoch: 15, Index: 386, Loss: 0.2728\n",
      "Epoch: 15, Index: 387, Loss: 0.6201\n",
      "Epoch: 15, Index: 388, Loss: 0.1343\n",
      "Epoch: 15, Index: 389, Loss: 1.2375\n",
      "Epoch: 15, Index: 390, Loss: 2.7854\n",
      "Epoch: 15, Index: 391, Loss: 1.4335\n",
      "Epoch: 15, Index: 392, Loss: 0.9519\n",
      "Epoch: 15, Index: 393, Loss: 0.4281\n",
      "Epoch: 15, Index: 394, Loss: 0.6468\n",
      "Epoch: 15, Index: 395, Loss: 0.3497\n",
      "Epoch: 15, Index: 396, Loss: 3.6187\n",
      "Epoch: 15, Index: 397, Loss: 1.1385\n",
      "Epoch: 15, Index: 398, Loss: 2.7027\n",
      "Epoch: 15, Index: 399, Loss: 0.1138\n",
      "Epoch: 15, Index: 400, Loss: 2.4384\n",
      "Epoch: 15, Index: 401, Loss: 2.0514\n",
      "Epoch: 15, Index: 402, Loss: 0.0745\n",
      "Epoch: 15, Index: 403, Loss: 0.2265\n",
      "Epoch: 15, Index: 404, Loss: 1.0065\n",
      "Epoch: 15, Index: 405, Loss: 1.2770\n",
      "Epoch: 15, Index: 406, Loss: 1.1976\n",
      "Epoch: 15, Index: 407, Loss: 9.0568\n",
      "Epoch: 15, Index: 408, Loss: 0.0187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be265927be04a12a5cb84cab98bdfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Index: 0, Loss: 2.1614\n",
      "Epoch: 16, Index: 1, Loss: 1.6629\n",
      "Epoch: 16, Index: 2, Loss: 2.3494\n",
      "Epoch: 16, Index: 3, Loss: 0.4191\n",
      "Epoch: 16, Index: 4, Loss: 3.3403\n",
      "Epoch: 16, Index: 5, Loss: 0.5398\n",
      "Epoch: 16, Index: 6, Loss: 0.9711\n",
      "Epoch: 16, Index: 7, Loss: 0.8231\n",
      "Epoch: 16, Index: 8, Loss: 1.1813\n",
      "Epoch: 16, Index: 9, Loss: 2.8698\n",
      "Epoch: 16, Index: 10, Loss: 0.8412\n",
      "Epoch: 16, Index: 11, Loss: 0.9068\n",
      "Epoch: 16, Index: 12, Loss: 0.2023\n",
      "Epoch: 16, Index: 13, Loss: 1.3716\n",
      "Epoch: 16, Index: 14, Loss: 0.1690\n",
      "Epoch: 16, Index: 15, Loss: 2.1035\n",
      "Epoch: 16, Index: 16, Loss: 0.5443\n",
      "Epoch: 16, Index: 17, Loss: 1.0260\n",
      "Epoch: 16, Index: 18, Loss: 0.9731\n",
      "Epoch: 16, Index: 19, Loss: 1.0256\n",
      "Epoch: 16, Index: 20, Loss: 1.0558\n",
      "Epoch: 16, Index: 21, Loss: 0.2762\n",
      "Epoch: 16, Index: 22, Loss: 0.6378\n",
      "Epoch: 16, Index: 23, Loss: 0.0443\n",
      "Epoch: 16, Index: 24, Loss: 0.1839\n",
      "Epoch: 16, Index: 25, Loss: 1.8906\n",
      "Epoch: 16, Index: 26, Loss: 0.4301\n",
      "Epoch: 16, Index: 27, Loss: 4.5875\n",
      "Epoch: 16, Index: 28, Loss: 2.1832\n",
      "Epoch: 16, Index: 29, Loss: 0.8363\n",
      "Epoch: 16, Index: 30, Loss: 1.3635\n",
      "Epoch: 16, Index: 31, Loss: 1.5728\n",
      "Epoch: 16, Index: 32, Loss: 1.9666\n",
      "Epoch: 16, Index: 33, Loss: 0.8586\n",
      "Epoch: 16, Index: 34, Loss: 0.3529\n",
      "Epoch: 16, Index: 35, Loss: 0.2546\n",
      "Epoch: 16, Index: 36, Loss: 2.5299\n",
      "Epoch: 16, Index: 37, Loss: 0.1857\n",
      "Epoch: 16, Index: 38, Loss: 0.9518\n",
      "Epoch: 16, Index: 39, Loss: 3.8508\n",
      "Epoch: 16, Index: 40, Loss: 2.5729\n",
      "Epoch: 16, Index: 41, Loss: 0.2114\n",
      "Epoch: 16, Index: 42, Loss: 0.1301\n",
      "Epoch: 16, Index: 43, Loss: 0.0041\n",
      "Epoch: 16, Index: 44, Loss: 0.7078\n",
      "Epoch: 16, Index: 45, Loss: 1.4918\n",
      "Epoch: 16, Index: 46, Loss: 1.0404\n",
      "Epoch: 16, Index: 47, Loss: 0.8325\n",
      "Epoch: 16, Index: 48, Loss: 1.1505\n",
      "Epoch: 16, Index: 49, Loss: 0.3098\n",
      "Epoch: 16, Index: 50, Loss: 2.3060\n",
      "Epoch: 16, Index: 51, Loss: 0.9605\n",
      "Epoch: 16, Index: 52, Loss: 0.5037\n",
      "Epoch: 16, Index: 53, Loss: 0.3821\n",
      "Epoch: 16, Index: 54, Loss: 1.7651\n",
      "Epoch: 16, Index: 55, Loss: 1.6092\n",
      "Epoch: 16, Index: 56, Loss: 2.5398\n",
      "Epoch: 16, Index: 57, Loss: 4.8575\n",
      "Epoch: 16, Index: 58, Loss: 4.4528\n",
      "Epoch: 16, Index: 59, Loss: 1.9907\n",
      "Epoch: 16, Index: 60, Loss: 1.1393\n",
      "Epoch: 16, Index: 61, Loss: 3.2759\n",
      "Epoch: 16, Index: 62, Loss: 3.6565\n",
      "Epoch: 16, Index: 63, Loss: 3.5703\n",
      "Epoch: 16, Index: 64, Loss: 4.1211\n",
      "Epoch: 16, Index: 65, Loss: 0.5758\n",
      "Epoch: 16, Index: 66, Loss: 0.3970\n",
      "Epoch: 16, Index: 67, Loss: 0.3428\n",
      "Epoch: 16, Index: 68, Loss: 3.0240\n",
      "Epoch: 16, Index: 69, Loss: 0.2026\n",
      "Epoch: 16, Index: 70, Loss: 0.9987\n",
      "Epoch: 16, Index: 71, Loss: 2.3957\n",
      "Epoch: 16, Index: 72, Loss: 1.4214\n",
      "Epoch: 16, Index: 73, Loss: 0.0633\n",
      "Epoch: 16, Index: 74, Loss: 0.8741\n",
      "Epoch: 16, Index: 75, Loss: 1.3005\n",
      "Epoch: 16, Index: 76, Loss: 0.1894\n",
      "Epoch: 16, Index: 77, Loss: 1.7720\n",
      "Epoch: 16, Index: 78, Loss: 1.0754\n",
      "Epoch: 16, Index: 79, Loss: 0.8801\n",
      "Epoch: 16, Index: 80, Loss: 0.2972\n",
      "Epoch: 16, Index: 81, Loss: 0.0513\n",
      "Epoch: 16, Index: 82, Loss: 1.2313\n",
      "Epoch: 16, Index: 83, Loss: 0.3756\n",
      "Epoch: 16, Index: 84, Loss: 0.7341\n",
      "Epoch: 16, Index: 85, Loss: 0.6756\n",
      "Epoch: 16, Index: 86, Loss: 0.4852\n",
      "Epoch: 16, Index: 87, Loss: 1.3395\n",
      "Epoch: 16, Index: 88, Loss: 10.7304\n",
      "Epoch: 16, Index: 89, Loss: 2.1318\n",
      "Epoch: 16, Index: 90, Loss: 0.9487\n",
      "Epoch: 16, Index: 91, Loss: 1.7023\n",
      "Epoch: 16, Index: 92, Loss: 0.3240\n",
      "Epoch: 16, Index: 93, Loss: 0.9942\n",
      "Epoch: 16, Index: 94, Loss: 5.7800\n",
      "Epoch: 16, Index: 95, Loss: 0.1163\n",
      "Epoch: 16, Index: 96, Loss: 0.8858\n",
      "Epoch: 16, Index: 97, Loss: 1.2530\n",
      "Epoch: 16, Index: 98, Loss: 0.2129\n",
      "Epoch: 16, Index: 99, Loss: 7.9114\n",
      "Epoch: 16, Index: 100, Loss: 0.2896\n",
      "Epoch: 16, Index: 101, Loss: 1.6565\n",
      "Epoch: 16, Index: 102, Loss: 0.1813\n",
      "Epoch: 16, Index: 103, Loss: 1.8478\n",
      "Epoch: 16, Index: 104, Loss: 0.8885\n",
      "Epoch: 16, Index: 105, Loss: 1.1019\n",
      "Epoch: 16, Index: 106, Loss: 1.0449\n",
      "Epoch: 16, Index: 107, Loss: 2.0647\n",
      "Epoch: 16, Index: 108, Loss: 1.8782\n",
      "Epoch: 16, Index: 109, Loss: 1.3877\n",
      "Epoch: 16, Index: 110, Loss: 5.5110\n",
      "Epoch: 16, Index: 111, Loss: 2.4321\n",
      "Epoch: 16, Index: 112, Loss: 0.7559\n",
      "Epoch: 16, Index: 113, Loss: 0.2731\n",
      "Epoch: 16, Index: 114, Loss: 0.9924\n",
      "Epoch: 16, Index: 115, Loss: 3.3048\n",
      "Epoch: 16, Index: 116, Loss: 0.4245\n",
      "Epoch: 16, Index: 117, Loss: 2.9479\n",
      "Epoch: 16, Index: 118, Loss: 0.2378\n",
      "Epoch: 16, Index: 119, Loss: 0.3941\n",
      "Epoch: 16, Index: 120, Loss: 0.8868\n",
      "Epoch: 16, Index: 121, Loss: 0.0369\n",
      "Epoch: 16, Index: 122, Loss: 2.3865\n",
      "Epoch: 16, Index: 123, Loss: 7.8449\n",
      "Epoch: 16, Index: 124, Loss: 3.4413\n",
      "Epoch: 16, Index: 125, Loss: 4.1632\n",
      "Epoch: 16, Index: 126, Loss: 1.5721\n",
      "Epoch: 16, Index: 127, Loss: 1.1173\n",
      "Epoch: 16, Index: 128, Loss: 4.7827\n",
      "Epoch: 16, Index: 129, Loss: 3.1736\n",
      "Epoch: 16, Index: 130, Loss: 2.8386\n",
      "Epoch: 16, Index: 131, Loss: 0.0393\n",
      "Epoch: 16, Index: 132, Loss: 1.5520\n",
      "Epoch: 16, Index: 133, Loss: 2.6758\n",
      "Epoch: 16, Index: 134, Loss: 4.4928\n",
      "Epoch: 16, Index: 135, Loss: 0.4134\n",
      "Epoch: 16, Index: 136, Loss: 1.9968\n",
      "Epoch: 16, Index: 137, Loss: 0.0005\n",
      "Epoch: 16, Index: 138, Loss: 2.5391\n",
      "Epoch: 16, Index: 139, Loss: 0.4758\n",
      "Epoch: 16, Index: 140, Loss: 2.7955\n",
      "Epoch: 16, Index: 141, Loss: 1.7257\n",
      "Epoch: 16, Index: 142, Loss: 0.1130\n",
      "Epoch: 16, Index: 143, Loss: 2.8681\n",
      "Epoch: 16, Index: 144, Loss: 1.6649\n",
      "Epoch: 16, Index: 145, Loss: 4.5562\n",
      "Epoch: 16, Index: 146, Loss: 2.9085\n",
      "Epoch: 16, Index: 147, Loss: 1.7968\n",
      "Epoch: 16, Index: 148, Loss: 5.2646\n",
      "Epoch: 16, Index: 149, Loss: 1.9098\n",
      "Epoch: 16, Index: 150, Loss: 0.4717\n",
      "Epoch: 16, Index: 151, Loss: 0.5719\n",
      "Epoch: 16, Index: 152, Loss: 1.9802\n",
      "Epoch: 16, Index: 153, Loss: 5.7526\n",
      "Epoch: 16, Index: 154, Loss: 6.4418\n",
      "Epoch: 16, Index: 155, Loss: 9.6153\n",
      "Epoch: 16, Index: 156, Loss: 0.6304\n",
      "Epoch: 16, Index: 157, Loss: 0.8697\n",
      "Epoch: 16, Index: 158, Loss: 1.1203\n",
      "Epoch: 16, Index: 159, Loss: 0.4521\n",
      "Epoch: 16, Index: 160, Loss: 2.0275\n",
      "Epoch: 16, Index: 161, Loss: 13.1418\n",
      "Epoch: 16, Index: 162, Loss: 1.1725\n",
      "Epoch: 16, Index: 163, Loss: 3.1720\n",
      "Epoch: 16, Index: 164, Loss: 1.4586\n",
      "Epoch: 16, Index: 165, Loss: 0.8532\n",
      "Epoch: 16, Index: 166, Loss: 2.7581\n",
      "Epoch: 16, Index: 167, Loss: 1.2348\n",
      "Epoch: 16, Index: 168, Loss: 2.5794\n",
      "Epoch: 16, Index: 169, Loss: 4.4924\n",
      "Epoch: 16, Index: 170, Loss: 2.6329\n",
      "Epoch: 16, Index: 171, Loss: 0.0832\n",
      "Epoch: 16, Index: 172, Loss: 3.2241\n",
      "Epoch: 16, Index: 173, Loss: 2.0376\n",
      "Epoch: 16, Index: 174, Loss: 3.8583\n",
      "Epoch: 16, Index: 175, Loss: 1.4605\n",
      "Epoch: 16, Index: 176, Loss: 1.2390\n",
      "Epoch: 16, Index: 177, Loss: 2.0546\n",
      "Epoch: 16, Index: 178, Loss: 1.3697\n",
      "Epoch: 16, Index: 179, Loss: 3.2294\n",
      "Epoch: 16, Index: 180, Loss: 0.6796\n",
      "Epoch: 16, Index: 181, Loss: 1.0606\n",
      "Epoch: 16, Index: 182, Loss: 7.1669\n",
      "Epoch: 16, Index: 183, Loss: 2.9243\n",
      "Epoch: 16, Index: 184, Loss: 2.1536\n",
      "Epoch: 16, Index: 185, Loss: 0.0527\n",
      "Epoch: 16, Index: 186, Loss: 0.6734\n",
      "Epoch: 16, Index: 187, Loss: 0.1086\n",
      "Epoch: 16, Index: 188, Loss: 0.5676\n",
      "Epoch: 16, Index: 189, Loss: 2.3286\n",
      "Epoch: 16, Index: 190, Loss: 1.9238\n",
      "Epoch: 16, Index: 191, Loss: 0.3610\n",
      "Epoch: 16, Index: 192, Loss: 1.1561\n",
      "Epoch: 16, Index: 193, Loss: 0.8976\n",
      "Epoch: 16, Index: 194, Loss: 1.3698\n",
      "Epoch: 16, Index: 195, Loss: 3.0323\n",
      "Epoch: 16, Index: 196, Loss: 1.0314\n",
      "Epoch: 16, Index: 197, Loss: 2.8483\n",
      "Epoch: 16, Index: 198, Loss: 1.1969\n",
      "Epoch: 16, Index: 199, Loss: 2.3729\n",
      "Epoch: 16, Index: 200, Loss: 1.2484\n",
      "Epoch: 16, Index: 201, Loss: 0.1104\n",
      "Epoch: 16, Index: 202, Loss: 1.3290\n",
      "Epoch: 16, Index: 203, Loss: 1.6021\n",
      "Epoch: 16, Index: 204, Loss: 1.1868\n",
      "Epoch: 16, Index: 205, Loss: 0.9159\n",
      "Epoch: 16, Index: 206, Loss: 0.1355\n",
      "Epoch: 16, Index: 207, Loss: 1.6412\n",
      "Epoch: 16, Index: 208, Loss: 0.0508\n",
      "Epoch: 16, Index: 209, Loss: 1.1656\n",
      "Epoch: 16, Index: 210, Loss: 0.6189\n",
      "Epoch: 16, Index: 211, Loss: 0.0860\n",
      "Epoch: 16, Index: 212, Loss: 0.4299\n",
      "Epoch: 16, Index: 213, Loss: 0.0792\n",
      "Epoch: 16, Index: 214, Loss: 0.1724\n",
      "Epoch: 16, Index: 215, Loss: 1.8582\n",
      "Epoch: 16, Index: 216, Loss: 5.4255\n",
      "Epoch: 16, Index: 217, Loss: 2.1935\n",
      "Epoch: 16, Index: 218, Loss: 4.5282\n",
      "Epoch: 16, Index: 219, Loss: 5.7757\n",
      "Epoch: 16, Index: 220, Loss: 0.8409\n",
      "Epoch: 16, Index: 221, Loss: 0.4181\n",
      "Epoch: 16, Index: 222, Loss: 2.2069\n",
      "Epoch: 16, Index: 223, Loss: 1.5225\n",
      "Epoch: 16, Index: 224, Loss: 0.0401\n",
      "Epoch: 16, Index: 225, Loss: 1.8009\n",
      "Epoch: 16, Index: 226, Loss: 1.5504\n",
      "Epoch: 16, Index: 227, Loss: 3.6696\n",
      "Epoch: 16, Index: 228, Loss: 0.7135\n",
      "Epoch: 16, Index: 229, Loss: 2.8283\n",
      "Epoch: 16, Index: 230, Loss: 2.5676\n",
      "Epoch: 16, Index: 231, Loss: 10.9670\n",
      "Epoch: 16, Index: 232, Loss: 2.1681\n",
      "Epoch: 16, Index: 233, Loss: 5.5667\n",
      "Epoch: 16, Index: 234, Loss: 3.4270\n",
      "Epoch: 16, Index: 235, Loss: 4.5625\n",
      "Epoch: 16, Index: 236, Loss: 4.9966\n",
      "Epoch: 16, Index: 237, Loss: 4.8133\n",
      "Epoch: 16, Index: 238, Loss: 0.2942\n",
      "Epoch: 16, Index: 239, Loss: 0.6679\n",
      "Epoch: 16, Index: 240, Loss: 1.3310\n",
      "Epoch: 16, Index: 241, Loss: 0.7350\n",
      "Epoch: 16, Index: 242, Loss: 1.3489\n",
      "Epoch: 16, Index: 243, Loss: 1.8338\n",
      "Epoch: 16, Index: 244, Loss: 0.7736\n",
      "Epoch: 16, Index: 245, Loss: 12.3946\n",
      "Epoch: 16, Index: 246, Loss: 1.6390\n",
      "Epoch: 16, Index: 247, Loss: 2.3469\n",
      "Epoch: 16, Index: 248, Loss: 0.9163\n",
      "Epoch: 16, Index: 249, Loss: 1.6929\n",
      "Epoch: 16, Index: 250, Loss: 0.2588\n",
      "Epoch: 16, Index: 251, Loss: 0.9300\n",
      "Epoch: 16, Index: 252, Loss: 0.5756\n",
      "Epoch: 16, Index: 253, Loss: 5.2900\n",
      "Epoch: 16, Index: 254, Loss: 1.8841\n",
      "Epoch: 16, Index: 255, Loss: 0.2403\n",
      "Epoch: 16, Index: 256, Loss: 3.5233\n",
      "Epoch: 16, Index: 257, Loss: 1.4367\n",
      "Epoch: 16, Index: 258, Loss: 0.8657\n",
      "Epoch: 16, Index: 259, Loss: 1.1935\n",
      "Epoch: 16, Index: 260, Loss: 0.3297\n",
      "Epoch: 16, Index: 261, Loss: 0.8221\n",
      "Epoch: 16, Index: 262, Loss: 0.9994\n",
      "Epoch: 16, Index: 263, Loss: 0.7851\n",
      "Epoch: 16, Index: 264, Loss: 0.1057\n",
      "Epoch: 16, Index: 265, Loss: 5.5271\n",
      "Epoch: 16, Index: 266, Loss: 2.5027\n",
      "Epoch: 16, Index: 267, Loss: 1.4532\n",
      "Epoch: 16, Index: 268, Loss: 1.3649\n",
      "Epoch: 16, Index: 269, Loss: 1.1167\n",
      "Epoch: 16, Index: 270, Loss: 0.1828\n",
      "Epoch: 16, Index: 271, Loss: 0.3135\n",
      "Epoch: 16, Index: 272, Loss: 1.6758\n",
      "Epoch: 16, Index: 273, Loss: 0.8745\n",
      "Epoch: 16, Index: 274, Loss: 2.1971\n",
      "Epoch: 16, Index: 275, Loss: 0.6203\n",
      "Epoch: 16, Index: 276, Loss: 1.2402\n",
      "Epoch: 16, Index: 277, Loss: 3.4703\n",
      "Epoch: 16, Index: 278, Loss: 0.2703\n",
      "Epoch: 16, Index: 279, Loss: 3.7879\n",
      "Epoch: 16, Index: 280, Loss: 3.9913\n",
      "Epoch: 16, Index: 281, Loss: 1.7516\n",
      "Epoch: 16, Index: 282, Loss: 0.5519\n",
      "Epoch: 16, Index: 283, Loss: 0.6569\n",
      "Epoch: 16, Index: 284, Loss: 1.7623\n",
      "Epoch: 16, Index: 285, Loss: 4.8709\n",
      "Epoch: 16, Index: 286, Loss: 0.6968\n",
      "Epoch: 16, Index: 287, Loss: 2.7937\n",
      "Epoch: 16, Index: 288, Loss: 1.1277\n",
      "Epoch: 16, Index: 289, Loss: 0.1649\n",
      "Epoch: 16, Index: 290, Loss: 0.0981\n",
      "Epoch: 16, Index: 291, Loss: 0.2431\n",
      "Epoch: 16, Index: 292, Loss: 3.0914\n",
      "Epoch: 16, Index: 293, Loss: 1.1488\n",
      "Epoch: 16, Index: 294, Loss: 0.5309\n",
      "Epoch: 16, Index: 295, Loss: 1.3214\n",
      "Epoch: 16, Index: 296, Loss: 1.3831\n",
      "Epoch: 16, Index: 297, Loss: 4.6666\n",
      "Epoch: 16, Index: 298, Loss: 10.5887\n",
      "Epoch: 16, Index: 299, Loss: 5.3400\n",
      "Epoch: 16, Index: 300, Loss: 9.3895\n",
      "Epoch: 16, Index: 301, Loss: 7.2781\n",
      "Epoch: 16, Index: 302, Loss: 7.3498\n",
      "Epoch: 16, Index: 303, Loss: 12.4266\n",
      "Epoch: 16, Index: 304, Loss: 19.3364\n",
      "Epoch: 16, Index: 305, Loss: 12.4445\n",
      "Epoch: 16, Index: 306, Loss: 3.4856\n",
      "Epoch: 16, Index: 307, Loss: 3.7070\n",
      "Epoch: 16, Index: 308, Loss: 0.3639\n",
      "Epoch: 16, Index: 309, Loss: 3.3777\n",
      "Epoch: 16, Index: 310, Loss: 5.9709\n",
      "Epoch: 16, Index: 311, Loss: 0.3998\n",
      "Epoch: 16, Index: 312, Loss: 0.3912\n",
      "Epoch: 16, Index: 313, Loss: 3.2599\n",
      "Epoch: 16, Index: 314, Loss: 7.0585\n",
      "Epoch: 16, Index: 315, Loss: 0.6871\n",
      "Epoch: 16, Index: 316, Loss: 1.4342\n",
      "Epoch: 16, Index: 317, Loss: 0.4476\n",
      "Epoch: 16, Index: 318, Loss: 1.1285\n",
      "Epoch: 16, Index: 319, Loss: 0.6672\n",
      "Epoch: 16, Index: 320, Loss: 0.6812\n",
      "Epoch: 16, Index: 321, Loss: 0.1757\n",
      "Epoch: 16, Index: 322, Loss: 3.3358\n",
      "Epoch: 16, Index: 323, Loss: 3.8702\n",
      "Epoch: 16, Index: 324, Loss: 1.8020\n",
      "Epoch: 16, Index: 325, Loss: 0.2594\n",
      "Epoch: 16, Index: 326, Loss: 5.3419\n",
      "Epoch: 16, Index: 327, Loss: 0.2572\n",
      "Epoch: 16, Index: 328, Loss: 0.5699\n",
      "Epoch: 16, Index: 329, Loss: 2.5192\n",
      "Epoch: 16, Index: 330, Loss: 1.8604\n",
      "Epoch: 16, Index: 331, Loss: 2.3051\n",
      "Epoch: 16, Index: 332, Loss: 328.8512\n",
      "Epoch: 16, Index: 333, Loss: 2.2377\n",
      "Epoch: 16, Index: 334, Loss: 2.0721\n",
      "Epoch: 16, Index: 335, Loss: 0.5183\n",
      "Epoch: 16, Index: 336, Loss: 18.7947\n",
      "Epoch: 16, Index: 337, Loss: 41.3495\n",
      "Epoch: 16, Index: 338, Loss: 3.4555\n",
      "Epoch: 16, Index: 339, Loss: 1.7999\n",
      "Epoch: 16, Index: 340, Loss: 0.7755\n",
      "Epoch: 16, Index: 341, Loss: 7.5559\n",
      "Epoch: 16, Index: 342, Loss: 0.2464\n",
      "Epoch: 16, Index: 343, Loss: 2.5620\n",
      "Epoch: 16, Index: 344, Loss: 1.9061\n",
      "Epoch: 16, Index: 345, Loss: 0.0585\n",
      "Epoch: 16, Index: 346, Loss: 3.5015\n",
      "Epoch: 16, Index: 347, Loss: 0.7105\n",
      "Epoch: 16, Index: 348, Loss: 0.2858\n",
      "Epoch: 16, Index: 349, Loss: 4.7285\n",
      "Epoch: 16, Index: 350, Loss: 0.8420\n",
      "Epoch: 16, Index: 351, Loss: 0.4165\n",
      "Epoch: 16, Index: 352, Loss: 3.8116\n",
      "Epoch: 16, Index: 353, Loss: 2.0708\n",
      "Epoch: 16, Index: 354, Loss: 6.4110\n",
      "Epoch: 16, Index: 355, Loss: 0.8949\n",
      "Epoch: 16, Index: 356, Loss: 0.2542\n",
      "Epoch: 16, Index: 357, Loss: 0.4452\n",
      "Epoch: 16, Index: 358, Loss: 0.9680\n",
      "Epoch: 16, Index: 359, Loss: 3.5015\n",
      "Epoch: 16, Index: 360, Loss: 15.9447\n",
      "Epoch: 16, Index: 361, Loss: 14.4921\n",
      "Epoch: 16, Index: 362, Loss: 11.2321\n",
      "Epoch: 16, Index: 363, Loss: 9.8500\n",
      "Epoch: 16, Index: 364, Loss: 16.9928\n",
      "Epoch: 16, Index: 365, Loss: 2.7208\n",
      "Epoch: 16, Index: 366, Loss: 2.7826\n",
      "Epoch: 16, Index: 367, Loss: 0.3113\n",
      "Epoch: 16, Index: 368, Loss: 2.6753\n",
      "Epoch: 16, Index: 369, Loss: 1.6683\n",
      "Epoch: 16, Index: 370, Loss: 1.1918\n",
      "Epoch: 16, Index: 371, Loss: 6.7500\n",
      "Epoch: 16, Index: 372, Loss: 2.9425\n",
      "Epoch: 16, Index: 373, Loss: 1.7605\n",
      "Epoch: 16, Index: 374, Loss: 0.2810\n",
      "Epoch: 16, Index: 375, Loss: 8.5304\n",
      "Epoch: 16, Index: 376, Loss: 11.8489\n",
      "Epoch: 16, Index: 377, Loss: 2.4146\n",
      "Epoch: 16, Index: 378, Loss: 3.8534\n",
      "Epoch: 16, Index: 379, Loss: 1.2838\n",
      "Epoch: 16, Index: 380, Loss: 2.5431\n",
      "Epoch: 16, Index: 381, Loss: 1.1841\n",
      "Epoch: 16, Index: 382, Loss: 0.8878\n",
      "Epoch: 16, Index: 383, Loss: 8.0249\n",
      "Epoch: 16, Index: 384, Loss: 20.2793\n",
      "Epoch: 16, Index: 385, Loss: 9.8954\n",
      "Epoch: 16, Index: 386, Loss: 2.7375\n",
      "Epoch: 16, Index: 387, Loss: 0.4279\n",
      "Epoch: 16, Index: 388, Loss: 4.4177\n",
      "Epoch: 16, Index: 389, Loss: 2.3204\n",
      "Epoch: 16, Index: 390, Loss: 0.0598\n",
      "Epoch: 16, Index: 391, Loss: 4.4545\n",
      "Epoch: 16, Index: 392, Loss: 7.7203\n",
      "Epoch: 16, Index: 393, Loss: 1.0138\n",
      "Epoch: 16, Index: 394, Loss: 2.3655\n",
      "Epoch: 16, Index: 395, Loss: 1.8631\n",
      "Epoch: 16, Index: 396, Loss: 4.3657\n",
      "Epoch: 16, Index: 397, Loss: 0.7304\n",
      "Epoch: 16, Index: 398, Loss: 1.6282\n",
      "Epoch: 16, Index: 399, Loss: 2.9805\n",
      "Epoch: 16, Index: 400, Loss: 1.2196\n",
      "Epoch: 16, Index: 401, Loss: 1.9424\n",
      "Epoch: 16, Index: 402, Loss: 0.5596\n",
      "Epoch: 16, Index: 403, Loss: 0.8683\n",
      "Epoch: 16, Index: 404, Loss: 0.7094\n",
      "Epoch: 16, Index: 405, Loss: 4.2651\n",
      "Epoch: 16, Index: 406, Loss: 4.1978\n",
      "Epoch: 16, Index: 407, Loss: 1.9175\n",
      "Epoch: 16, Index: 408, Loss: 1.1118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b891cca76845328dc78188a05a51b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Index: 0, Loss: 1.5146\n",
      "Epoch: 17, Index: 1, Loss: 1.9974\n",
      "Epoch: 17, Index: 2, Loss: 1.4342\n",
      "Epoch: 17, Index: 3, Loss: 7.9539\n",
      "Epoch: 17, Index: 4, Loss: 0.9285\n",
      "Epoch: 17, Index: 5, Loss: 0.5699\n",
      "Epoch: 17, Index: 6, Loss: 2.8952\n",
      "Epoch: 17, Index: 7, Loss: 0.2028\n",
      "Epoch: 17, Index: 8, Loss: 0.7931\n",
      "Epoch: 17, Index: 9, Loss: 0.3612\n",
      "Epoch: 17, Index: 10, Loss: 1.2070\n",
      "Epoch: 17, Index: 11, Loss: 1.5510\n",
      "Epoch: 17, Index: 12, Loss: 1.0000\n",
      "Epoch: 17, Index: 13, Loss: 0.6076\n",
      "Epoch: 17, Index: 14, Loss: 0.6488\n",
      "Epoch: 17, Index: 15, Loss: 5.8940\n",
      "Epoch: 17, Index: 16, Loss: 0.3596\n",
      "Epoch: 17, Index: 17, Loss: 5.1305\n",
      "Epoch: 17, Index: 18, Loss: 0.5348\n",
      "Epoch: 17, Index: 19, Loss: 0.2022\n",
      "Epoch: 17, Index: 20, Loss: 2.1239\n",
      "Epoch: 17, Index: 21, Loss: 3.1880\n",
      "Epoch: 17, Index: 22, Loss: 0.4643\n",
      "Epoch: 17, Index: 23, Loss: 3.9341\n",
      "Epoch: 17, Index: 24, Loss: 0.1731\n",
      "Epoch: 17, Index: 25, Loss: 3.8434\n",
      "Epoch: 17, Index: 26, Loss: 1.2137\n",
      "Epoch: 17, Index: 27, Loss: 4.6533\n",
      "Epoch: 17, Index: 28, Loss: 0.2178\n",
      "Epoch: 17, Index: 29, Loss: 1.5208\n",
      "Epoch: 17, Index: 30, Loss: 0.0481\n",
      "Epoch: 17, Index: 31, Loss: 0.8186\n",
      "Epoch: 17, Index: 32, Loss: 0.0601\n",
      "Epoch: 17, Index: 33, Loss: 4.1241\n",
      "Epoch: 17, Index: 34, Loss: 1.6475\n",
      "Epoch: 17, Index: 35, Loss: 0.3194\n",
      "Epoch: 17, Index: 36, Loss: 0.7087\n",
      "Epoch: 17, Index: 37, Loss: 0.4753\n",
      "Epoch: 17, Index: 38, Loss: 7.2046\n",
      "Epoch: 17, Index: 39, Loss: 0.2390\n",
      "Epoch: 17, Index: 40, Loss: 3.6509\n",
      "Epoch: 17, Index: 41, Loss: 0.9287\n",
      "Epoch: 17, Index: 42, Loss: 0.2581\n",
      "Epoch: 17, Index: 43, Loss: 0.7822\n",
      "Epoch: 17, Index: 44, Loss: 0.1191\n",
      "Epoch: 17, Index: 45, Loss: 0.4471\n",
      "Epoch: 17, Index: 46, Loss: 0.7531\n",
      "Epoch: 17, Index: 47, Loss: 0.3694\n",
      "Epoch: 17, Index: 48, Loss: 2.0545\n",
      "Epoch: 17, Index: 49, Loss: 2.0078\n",
      "Epoch: 17, Index: 50, Loss: 2.6891\n",
      "Epoch: 17, Index: 51, Loss: 0.2066\n",
      "Epoch: 17, Index: 52, Loss: 1.3923\n",
      "Epoch: 17, Index: 53, Loss: 2.6315\n",
      "Epoch: 17, Index: 54, Loss: 1.9212\n",
      "Epoch: 17, Index: 55, Loss: 0.5937\n",
      "Epoch: 17, Index: 56, Loss: 5.4442\n",
      "Epoch: 17, Index: 57, Loss: 0.5216\n",
      "Epoch: 17, Index: 58, Loss: 4.6075\n",
      "Epoch: 17, Index: 59, Loss: 0.2669\n",
      "Epoch: 17, Index: 60, Loss: 5.4150\n",
      "Epoch: 17, Index: 61, Loss: 1.3888\n",
      "Epoch: 17, Index: 62, Loss: 0.0993\n",
      "Epoch: 17, Index: 63, Loss: 2.4661\n",
      "Epoch: 17, Index: 64, Loss: 1.4650\n",
      "Epoch: 17, Index: 65, Loss: 1.5319\n",
      "Epoch: 17, Index: 66, Loss: 1.7265\n",
      "Epoch: 17, Index: 67, Loss: 1.2628\n",
      "Epoch: 17, Index: 68, Loss: 0.1220\n",
      "Epoch: 17, Index: 69, Loss: 2.8883\n",
      "Epoch: 17, Index: 70, Loss: 0.1605\n",
      "Epoch: 17, Index: 71, Loss: 2.2544\n",
      "Epoch: 17, Index: 72, Loss: 4.2548\n",
      "Epoch: 17, Index: 73, Loss: 16.5839\n",
      "Epoch: 17, Index: 74, Loss: 0.5378\n",
      "Epoch: 17, Index: 75, Loss: 0.1941\n",
      "Epoch: 17, Index: 76, Loss: 0.2315\n",
      "Epoch: 17, Index: 77, Loss: 1.6881\n",
      "Epoch: 17, Index: 78, Loss: 2.0153\n",
      "Epoch: 17, Index: 79, Loss: 2.2595\n",
      "Epoch: 17, Index: 80, Loss: 1.5837\n",
      "Epoch: 17, Index: 81, Loss: 1.8344\n",
      "Epoch: 17, Index: 82, Loss: 1.0003\n",
      "Epoch: 17, Index: 83, Loss: 1.5902\n",
      "Epoch: 17, Index: 84, Loss: 0.6830\n",
      "Epoch: 17, Index: 85, Loss: 0.8226\n",
      "Epoch: 17, Index: 86, Loss: 0.2626\n",
      "Epoch: 17, Index: 87, Loss: 2.5007\n",
      "Epoch: 17, Index: 88, Loss: 4.6083\n",
      "Epoch: 17, Index: 89, Loss: 0.0441\n",
      "Epoch: 17, Index: 90, Loss: 1.6402\n",
      "Epoch: 17, Index: 91, Loss: 0.8168\n",
      "Epoch: 17, Index: 92, Loss: 0.9015\n",
      "Epoch: 17, Index: 93, Loss: 3.1005\n",
      "Epoch: 17, Index: 94, Loss: 2.0103\n",
      "Epoch: 17, Index: 95, Loss: 2.3415\n",
      "Epoch: 17, Index: 96, Loss: 7.0394\n",
      "Epoch: 17, Index: 97, Loss: 1.3294\n",
      "Epoch: 17, Index: 98, Loss: 4.9373\n",
      "Epoch: 17, Index: 99, Loss: 0.7416\n",
      "Epoch: 17, Index: 100, Loss: 0.8973\n",
      "Epoch: 17, Index: 101, Loss: 1.2106\n",
      "Epoch: 17, Index: 102, Loss: 2.1225\n",
      "Epoch: 17, Index: 103, Loss: 1.8936\n",
      "Epoch: 17, Index: 104, Loss: 1.2194\n",
      "Epoch: 17, Index: 105, Loss: 1.4708\n",
      "Epoch: 17, Index: 106, Loss: 0.6085\n",
      "Epoch: 17, Index: 107, Loss: 2.6828\n",
      "Epoch: 17, Index: 108, Loss: 0.5041\n",
      "Epoch: 17, Index: 109, Loss: 0.2175\n",
      "Epoch: 17, Index: 110, Loss: 1.6478\n",
      "Epoch: 17, Index: 111, Loss: 1.3614\n",
      "Epoch: 17, Index: 112, Loss: 0.7845\n",
      "Epoch: 17, Index: 113, Loss: 1.4703\n",
      "Epoch: 17, Index: 114, Loss: 2.5518\n",
      "Epoch: 17, Index: 115, Loss: 1.0805\n",
      "Epoch: 17, Index: 116, Loss: 3.6400\n",
      "Epoch: 17, Index: 117, Loss: 1.5657\n",
      "Epoch: 17, Index: 118, Loss: 4.0391\n",
      "Epoch: 17, Index: 119, Loss: 1.0223\n",
      "Epoch: 17, Index: 120, Loss: 0.0913\n",
      "Epoch: 17, Index: 121, Loss: 4.4715\n",
      "Epoch: 17, Index: 122, Loss: 2.1131\n",
      "Epoch: 17, Index: 123, Loss: 0.6970\n",
      "Epoch: 17, Index: 124, Loss: 2.0587\n",
      "Epoch: 17, Index: 125, Loss: 1.2567\n",
      "Epoch: 17, Index: 126, Loss: 4.0026\n",
      "Epoch: 17, Index: 127, Loss: 0.4625\n",
      "Epoch: 17, Index: 128, Loss: 2.2807\n",
      "Epoch: 17, Index: 129, Loss: 0.9420\n",
      "Epoch: 17, Index: 130, Loss: 3.1952\n",
      "Epoch: 17, Index: 131, Loss: 0.7466\n",
      "Epoch: 17, Index: 132, Loss: 0.0335\n",
      "Epoch: 17, Index: 133, Loss: 1.0563\n",
      "Epoch: 17, Index: 134, Loss: 0.0452\n",
      "Epoch: 17, Index: 135, Loss: 1.3828\n",
      "Epoch: 17, Index: 136, Loss: 1.0616\n",
      "Epoch: 17, Index: 137, Loss: 0.1979\n",
      "Epoch: 17, Index: 138, Loss: 0.7198\n",
      "Epoch: 17, Index: 139, Loss: 1.2836\n",
      "Epoch: 17, Index: 140, Loss: 0.5563\n",
      "Epoch: 17, Index: 141, Loss: 0.7384\n",
      "Epoch: 17, Index: 142, Loss: 0.9319\n",
      "Epoch: 17, Index: 143, Loss: 0.9045\n",
      "Epoch: 17, Index: 144, Loss: 0.6490\n",
      "Epoch: 17, Index: 145, Loss: 1.1566\n",
      "Epoch: 17, Index: 146, Loss: 0.3867\n",
      "Epoch: 17, Index: 147, Loss: 0.9542\n",
      "Epoch: 17, Index: 148, Loss: 0.3931\n",
      "Epoch: 17, Index: 149, Loss: 0.4411\n",
      "Epoch: 17, Index: 150, Loss: 2.1313\n",
      "Epoch: 17, Index: 151, Loss: 1.3043\n",
      "Epoch: 17, Index: 152, Loss: 2.7799\n",
      "Epoch: 17, Index: 153, Loss: 0.0813\n",
      "Epoch: 17, Index: 154, Loss: 0.7549\n",
      "Epoch: 17, Index: 155, Loss: 6.3195\n",
      "Epoch: 17, Index: 156, Loss: 1.0195\n",
      "Epoch: 17, Index: 157, Loss: 1.4515\n",
      "Epoch: 17, Index: 158, Loss: 2.0959\n",
      "Epoch: 17, Index: 159, Loss: 1.5873\n",
      "Epoch: 17, Index: 160, Loss: 0.5631\n",
      "Epoch: 17, Index: 161, Loss: 0.6350\n",
      "Epoch: 17, Index: 162, Loss: 0.9503\n",
      "Epoch: 17, Index: 163, Loss: 2.4795\n",
      "Epoch: 17, Index: 164, Loss: 7.5389\n",
      "Epoch: 17, Index: 165, Loss: 1.7817\n",
      "Epoch: 17, Index: 166, Loss: 2.0514\n",
      "Epoch: 17, Index: 167, Loss: 0.5036\n",
      "Epoch: 17, Index: 168, Loss: 2.0638\n",
      "Epoch: 17, Index: 169, Loss: 0.3286\n",
      "Epoch: 17, Index: 170, Loss: 1.9361\n",
      "Epoch: 17, Index: 171, Loss: 0.4569\n",
      "Epoch: 17, Index: 172, Loss: 0.4052\n",
      "Epoch: 17, Index: 173, Loss: 3.2548\n",
      "Epoch: 17, Index: 174, Loss: 1.7231\n",
      "Epoch: 17, Index: 175, Loss: 0.7407\n",
      "Epoch: 17, Index: 176, Loss: 10.4218\n",
      "Epoch: 17, Index: 177, Loss: 0.3309\n",
      "Epoch: 17, Index: 178, Loss: 0.1022\n",
      "Epoch: 17, Index: 179, Loss: 0.2280\n",
      "Epoch: 17, Index: 180, Loss: 0.8071\n",
      "Epoch: 17, Index: 181, Loss: 6.1302\n",
      "Epoch: 17, Index: 182, Loss: 1.1285\n",
      "Epoch: 17, Index: 183, Loss: 1.2098\n",
      "Epoch: 17, Index: 184, Loss: 0.2698\n",
      "Epoch: 17, Index: 185, Loss: 1.7359\n",
      "Epoch: 17, Index: 186, Loss: 8.6105\n",
      "Epoch: 17, Index: 187, Loss: 3.6608\n",
      "Epoch: 17, Index: 188, Loss: 3.3940\n",
      "Epoch: 17, Index: 189, Loss: 2.6007\n",
      "Epoch: 17, Index: 190, Loss: 4.7810\n",
      "Epoch: 17, Index: 191, Loss: 4.1866\n",
      "Epoch: 17, Index: 192, Loss: 0.9200\n",
      "Epoch: 17, Index: 193, Loss: 2.7745\n",
      "Epoch: 17, Index: 194, Loss: 1.1636\n",
      "Epoch: 17, Index: 195, Loss: 1.1999\n",
      "Epoch: 17, Index: 196, Loss: 0.1122\n",
      "Epoch: 17, Index: 197, Loss: 4.2788\n",
      "Epoch: 17, Index: 198, Loss: 1.3268\n",
      "Epoch: 17, Index: 199, Loss: 1.1059\n",
      "Epoch: 17, Index: 200, Loss: 1.7733\n",
      "Epoch: 17, Index: 201, Loss: 0.1942\n",
      "Epoch: 17, Index: 202, Loss: 1.8714\n",
      "Epoch: 17, Index: 203, Loss: 0.9903\n",
      "Epoch: 17, Index: 204, Loss: 2.5465\n",
      "Epoch: 17, Index: 205, Loss: 0.2501\n",
      "Epoch: 17, Index: 206, Loss: 0.2206\n",
      "Epoch: 17, Index: 207, Loss: 1.4549\n",
      "Epoch: 17, Index: 208, Loss: 0.8193\n",
      "Epoch: 17, Index: 209, Loss: 0.5218\n",
      "Epoch: 17, Index: 210, Loss: 0.0383\n",
      "Epoch: 17, Index: 211, Loss: 2.2599\n",
      "Epoch: 17, Index: 212, Loss: 0.3415\n",
      "Epoch: 17, Index: 213, Loss: 4.1340\n",
      "Epoch: 17, Index: 214, Loss: 2.0909\n",
      "Epoch: 17, Index: 215, Loss: 0.4438\n",
      "Epoch: 17, Index: 216, Loss: 6.2446\n",
      "Epoch: 17, Index: 217, Loss: 1.0018\n",
      "Epoch: 17, Index: 218, Loss: 0.3203\n",
      "Epoch: 17, Index: 219, Loss: 1.0699\n",
      "Epoch: 17, Index: 220, Loss: 1.3781\n",
      "Epoch: 17, Index: 221, Loss: 0.8666\n",
      "Epoch: 17, Index: 222, Loss: 6.0663\n",
      "Epoch: 17, Index: 223, Loss: 2.5330\n",
      "Epoch: 17, Index: 224, Loss: 2.6925\n",
      "Epoch: 17, Index: 225, Loss: 4.0243\n",
      "Epoch: 17, Index: 226, Loss: 0.6391\n",
      "Epoch: 17, Index: 227, Loss: 3.6326\n",
      "Epoch: 17, Index: 228, Loss: 2.0435\n",
      "Epoch: 17, Index: 229, Loss: 6.0008\n",
      "Epoch: 17, Index: 230, Loss: 0.6012\n",
      "Epoch: 17, Index: 231, Loss: 5.0581\n",
      "Epoch: 17, Index: 232, Loss: 0.1185\n",
      "Epoch: 17, Index: 233, Loss: 3.6975\n",
      "Epoch: 17, Index: 234, Loss: 0.5479\n",
      "Epoch: 17, Index: 235, Loss: 1.5059\n",
      "Epoch: 17, Index: 236, Loss: 1.4554\n",
      "Epoch: 17, Index: 237, Loss: 0.0424\n",
      "Epoch: 17, Index: 238, Loss: 0.6075\n",
      "Epoch: 17, Index: 239, Loss: 3.5224\n",
      "Epoch: 17, Index: 240, Loss: 2.8508\n",
      "Epoch: 17, Index: 241, Loss: 0.4395\n",
      "Epoch: 17, Index: 242, Loss: 0.7141\n",
      "Epoch: 17, Index: 243, Loss: 1.7417\n",
      "Epoch: 17, Index: 244, Loss: 3.6919\n",
      "Epoch: 17, Index: 245, Loss: 1.5310\n",
      "Epoch: 17, Index: 246, Loss: 0.6905\n",
      "Epoch: 17, Index: 247, Loss: 2.9625\n",
      "Epoch: 17, Index: 248, Loss: 0.3096\n",
      "Epoch: 17, Index: 249, Loss: 0.5685\n",
      "Epoch: 17, Index: 250, Loss: 0.7279\n",
      "Epoch: 17, Index: 251, Loss: 0.0499\n",
      "Epoch: 17, Index: 252, Loss: 1.7434\n",
      "Epoch: 17, Index: 253, Loss: 1.1459\n",
      "Epoch: 17, Index: 254, Loss: 3.3136\n",
      "Epoch: 17, Index: 255, Loss: 0.9037\n",
      "Epoch: 17, Index: 256, Loss: 3.9010\n",
      "Epoch: 17, Index: 257, Loss: 0.9741\n",
      "Epoch: 17, Index: 258, Loss: 4.7827\n",
      "Epoch: 17, Index: 259, Loss: 1.0688\n",
      "Epoch: 17, Index: 260, Loss: 10.3382\n",
      "Epoch: 17, Index: 261, Loss: 1.0625\n",
      "Epoch: 17, Index: 262, Loss: 0.2759\n",
      "Epoch: 17, Index: 263, Loss: 3.8659\n",
      "Epoch: 17, Index: 264, Loss: 1.8045\n",
      "Epoch: 17, Index: 265, Loss: 0.8090\n",
      "Epoch: 17, Index: 266, Loss: 2.2423\n",
      "Epoch: 17, Index: 267, Loss: 1.6977\n",
      "Epoch: 17, Index: 268, Loss: 2.1881\n",
      "Epoch: 17, Index: 269, Loss: 14.2326\n",
      "Epoch: 17, Index: 270, Loss: 1.1309\n",
      "Epoch: 17, Index: 271, Loss: 0.4154\n",
      "Epoch: 17, Index: 272, Loss: 0.1315\n",
      "Epoch: 17, Index: 273, Loss: 0.2461\n",
      "Epoch: 17, Index: 274, Loss: 7.2249\n",
      "Epoch: 17, Index: 275, Loss: 3.5666\n",
      "Epoch: 17, Index: 276, Loss: 5.1954\n",
      "Epoch: 17, Index: 277, Loss: 0.3590\n",
      "Epoch: 17, Index: 278, Loss: 0.0539\n",
      "Epoch: 17, Index: 279, Loss: 0.1975\n",
      "Epoch: 17, Index: 280, Loss: 2.3524\n",
      "Epoch: 17, Index: 281, Loss: 3.1105\n",
      "Epoch: 17, Index: 282, Loss: 1.3263\n",
      "Epoch: 17, Index: 283, Loss: 2.5970\n",
      "Epoch: 17, Index: 284, Loss: 1.2914\n",
      "Epoch: 17, Index: 285, Loss: 0.1571\n",
      "Epoch: 17, Index: 286, Loss: 1.4919\n",
      "Epoch: 17, Index: 287, Loss: 2.1351\n",
      "Epoch: 17, Index: 288, Loss: 0.7737\n",
      "Epoch: 17, Index: 289, Loss: 4.5795\n",
      "Epoch: 17, Index: 290, Loss: 1.3001\n",
      "Epoch: 17, Index: 291, Loss: 0.0933\n",
      "Epoch: 17, Index: 292, Loss: 0.7729\n",
      "Epoch: 17, Index: 293, Loss: 1.2527\n",
      "Epoch: 17, Index: 294, Loss: 0.2034\n",
      "Epoch: 17, Index: 295, Loss: 3.0440\n",
      "Epoch: 17, Index: 296, Loss: 4.1120\n",
      "Epoch: 17, Index: 297, Loss: 0.0883\n",
      "Epoch: 17, Index: 298, Loss: 2.0583\n",
      "Epoch: 17, Index: 299, Loss: 0.2661\n",
      "Epoch: 17, Index: 300, Loss: 3.9064\n",
      "Epoch: 17, Index: 301, Loss: 0.5988\n",
      "Epoch: 17, Index: 302, Loss: 1.5678\n",
      "Epoch: 17, Index: 303, Loss: 3.5069\n",
      "Epoch: 17, Index: 304, Loss: 0.4909\n",
      "Epoch: 17, Index: 305, Loss: 2.5544\n",
      "Epoch: 17, Index: 306, Loss: 3.4766\n",
      "Epoch: 17, Index: 307, Loss: 4.1376\n",
      "Epoch: 17, Index: 308, Loss: 0.7751\n",
      "Epoch: 17, Index: 309, Loss: 0.3727\n",
      "Epoch: 17, Index: 310, Loss: 0.4154\n",
      "Epoch: 17, Index: 311, Loss: 1.5399\n",
      "Epoch: 17, Index: 312, Loss: 0.5737\n",
      "Epoch: 17, Index: 313, Loss: 0.0007\n",
      "Epoch: 17, Index: 314, Loss: 0.7366\n",
      "Epoch: 17, Index: 315, Loss: 1.0733\n",
      "Epoch: 17, Index: 316, Loss: 0.3270\n",
      "Epoch: 17, Index: 317, Loss: 2.5569\n",
      "Epoch: 17, Index: 318, Loss: 2.9376\n",
      "Epoch: 17, Index: 319, Loss: 1.5835\n",
      "Epoch: 17, Index: 320, Loss: 0.0741\n",
      "Epoch: 17, Index: 321, Loss: 0.2006\n",
      "Epoch: 17, Index: 322, Loss: 3.5964\n",
      "Epoch: 17, Index: 323, Loss: 1.0124\n",
      "Epoch: 17, Index: 324, Loss: 1.8118\n",
      "Epoch: 17, Index: 325, Loss: 0.8952\n",
      "Epoch: 17, Index: 326, Loss: 0.9096\n",
      "Epoch: 17, Index: 327, Loss: 0.7874\n",
      "Epoch: 17, Index: 328, Loss: 5.0976\n",
      "Epoch: 17, Index: 329, Loss: 0.3039\n",
      "Epoch: 17, Index: 330, Loss: 1.5713\n",
      "Epoch: 17, Index: 331, Loss: 0.9371\n",
      "Epoch: 17, Index: 332, Loss: 1.4135\n",
      "Epoch: 17, Index: 333, Loss: 0.4075\n",
      "Epoch: 17, Index: 334, Loss: 0.3368\n",
      "Epoch: 17, Index: 335, Loss: 0.8147\n",
      "Epoch: 17, Index: 336, Loss: 0.1676\n",
      "Epoch: 17, Index: 337, Loss: 2.3102\n",
      "Epoch: 17, Index: 338, Loss: 1.3321\n",
      "Epoch: 17, Index: 339, Loss: 4.7571\n",
      "Epoch: 17, Index: 340, Loss: 1.1165\n",
      "Epoch: 17, Index: 341, Loss: 0.6387\n",
      "Epoch: 17, Index: 342, Loss: 2.7965\n",
      "Epoch: 17, Index: 343, Loss: 0.8170\n",
      "Epoch: 17, Index: 344, Loss: 0.9238\n",
      "Epoch: 17, Index: 345, Loss: 0.8322\n",
      "Epoch: 17, Index: 346, Loss: 0.1479\n",
      "Epoch: 17, Index: 347, Loss: 3.9485\n",
      "Epoch: 17, Index: 348, Loss: 1.3066\n",
      "Epoch: 17, Index: 349, Loss: 3.0855\n",
      "Epoch: 17, Index: 350, Loss: 0.6592\n",
      "Epoch: 17, Index: 351, Loss: 1.8936\n",
      "Epoch: 17, Index: 352, Loss: 1.4420\n",
      "Epoch: 17, Index: 353, Loss: 1.3224\n",
      "Epoch: 17, Index: 354, Loss: 1.0627\n",
      "Epoch: 17, Index: 355, Loss: 1.2490\n",
      "Epoch: 17, Index: 356, Loss: 0.7221\n",
      "Epoch: 17, Index: 357, Loss: 2.0673\n",
      "Epoch: 17, Index: 358, Loss: 0.6028\n",
      "Epoch: 17, Index: 359, Loss: 2.7475\n",
      "Epoch: 17, Index: 360, Loss: 8.6640\n",
      "Epoch: 17, Index: 361, Loss: 3.3470\n",
      "Epoch: 17, Index: 362, Loss: 1.5561\n",
      "Epoch: 17, Index: 363, Loss: 1.2599\n",
      "Epoch: 17, Index: 364, Loss: 1.0881\n",
      "Epoch: 17, Index: 365, Loss: 3.5626\n",
      "Epoch: 17, Index: 366, Loss: 1.8004\n",
      "Epoch: 17, Index: 367, Loss: 5.0220\n",
      "Epoch: 17, Index: 368, Loss: 0.0963\n",
      "Epoch: 17, Index: 369, Loss: 1.8556\n",
      "Epoch: 17, Index: 370, Loss: 1.0305\n",
      "Epoch: 17, Index: 371, Loss: 0.7405\n",
      "Epoch: 17, Index: 372, Loss: 0.7587\n",
      "Epoch: 17, Index: 373, Loss: 0.2349\n",
      "Epoch: 17, Index: 374, Loss: 5.7683\n",
      "Epoch: 17, Index: 375, Loss: 0.8133\n",
      "Epoch: 17, Index: 376, Loss: 2.9902\n",
      "Epoch: 17, Index: 377, Loss: 0.1879\n",
      "Epoch: 17, Index: 378, Loss: 6.8210\n",
      "Epoch: 17, Index: 379, Loss: 3.8951\n",
      "Epoch: 17, Index: 380, Loss: 0.0353\n",
      "Epoch: 17, Index: 381, Loss: 0.3156\n",
      "Epoch: 17, Index: 382, Loss: 1.7101\n",
      "Epoch: 17, Index: 383, Loss: 0.0464\n",
      "Epoch: 17, Index: 384, Loss: 4.7461\n",
      "Epoch: 17, Index: 385, Loss: 0.0739\n",
      "Epoch: 17, Index: 386, Loss: 1.7553\n",
      "Epoch: 17, Index: 387, Loss: 0.8503\n",
      "Epoch: 17, Index: 388, Loss: 1.9486\n",
      "Epoch: 17, Index: 389, Loss: 0.5256\n",
      "Epoch: 17, Index: 390, Loss: 1.1417\n",
      "Epoch: 17, Index: 391, Loss: 0.7535\n",
      "Epoch: 17, Index: 392, Loss: 0.5005\n",
      "Epoch: 17, Index: 393, Loss: 1.2040\n",
      "Epoch: 17, Index: 394, Loss: 1.5243\n",
      "Epoch: 17, Index: 395, Loss: 0.4250\n",
      "Epoch: 17, Index: 396, Loss: 1.6522\n",
      "Epoch: 17, Index: 397, Loss: 2.7115\n",
      "Epoch: 17, Index: 398, Loss: 1.0394\n",
      "Epoch: 17, Index: 399, Loss: 0.2836\n",
      "Epoch: 17, Index: 400, Loss: 0.3685\n",
      "Epoch: 17, Index: 401, Loss: 1.8642\n",
      "Epoch: 17, Index: 402, Loss: 0.0931\n",
      "Epoch: 17, Index: 403, Loss: 0.2870\n",
      "Epoch: 17, Index: 404, Loss: 1.7255\n",
      "Epoch: 17, Index: 405, Loss: 3.6305\n",
      "Epoch: 17, Index: 406, Loss: 2.1646\n",
      "Epoch: 17, Index: 407, Loss: 1.1232\n",
      "Epoch: 17, Index: 408, Loss: 0.1335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896f72888fbd45fca564f283bd96eaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Index: 0, Loss: 0.6797\n",
      "Epoch: 18, Index: 1, Loss: 2.0896\n",
      "Epoch: 18, Index: 2, Loss: 3.7310\n",
      "Epoch: 18, Index: 3, Loss: 1.8808\n",
      "Epoch: 18, Index: 4, Loss: 0.0430\n",
      "Epoch: 18, Index: 5, Loss: 1.1651\n",
      "Epoch: 18, Index: 6, Loss: 0.9835\n",
      "Epoch: 18, Index: 7, Loss: 0.7923\n",
      "Epoch: 18, Index: 8, Loss: 13.3541\n",
      "Epoch: 18, Index: 9, Loss: 0.1378\n",
      "Epoch: 18, Index: 10, Loss: 1.1659\n",
      "Epoch: 18, Index: 11, Loss: 7.8643\n",
      "Epoch: 18, Index: 12, Loss: 0.1016\n",
      "Epoch: 18, Index: 13, Loss: 4.1122\n",
      "Epoch: 18, Index: 14, Loss: 0.2268\n",
      "Epoch: 18, Index: 15, Loss: 1.4709\n",
      "Epoch: 18, Index: 16, Loss: 2.2159\n",
      "Epoch: 18, Index: 17, Loss: 2.3942\n",
      "Epoch: 18, Index: 18, Loss: 0.6480\n",
      "Epoch: 18, Index: 19, Loss: 3.1289\n",
      "Epoch: 18, Index: 20, Loss: 3.4345\n",
      "Epoch: 18, Index: 21, Loss: 4.8770\n",
      "Epoch: 18, Index: 22, Loss: 0.1010\n",
      "Epoch: 18, Index: 23, Loss: 0.5590\n",
      "Epoch: 18, Index: 24, Loss: 0.5058\n",
      "Epoch: 18, Index: 25, Loss: 0.4772\n",
      "Epoch: 18, Index: 26, Loss: 1.2898\n",
      "Epoch: 18, Index: 27, Loss: 4.2485\n",
      "Epoch: 18, Index: 28, Loss: 1.7765\n",
      "Epoch: 18, Index: 29, Loss: 0.7250\n",
      "Epoch: 18, Index: 30, Loss: 0.5781\n",
      "Epoch: 18, Index: 31, Loss: 10.0722\n",
      "Epoch: 18, Index: 32, Loss: 1.3900\n",
      "Epoch: 18, Index: 33, Loss: 0.0375\n",
      "Epoch: 18, Index: 34, Loss: 0.3489\n",
      "Epoch: 18, Index: 35, Loss: 0.4375\n",
      "Epoch: 18, Index: 36, Loss: 2.6793\n",
      "Epoch: 18, Index: 37, Loss: 0.1589\n",
      "Epoch: 18, Index: 38, Loss: 1.3108\n",
      "Epoch: 18, Index: 39, Loss: 8.2530\n",
      "Epoch: 18, Index: 40, Loss: 1.1886\n",
      "Epoch: 18, Index: 41, Loss: 0.6834\n",
      "Epoch: 18, Index: 42, Loss: 0.7645\n",
      "Epoch: 18, Index: 43, Loss: 0.9407\n",
      "Epoch: 18, Index: 44, Loss: 0.2228\n",
      "Epoch: 18, Index: 45, Loss: 0.1007\n",
      "Epoch: 18, Index: 46, Loss: 0.1968\n",
      "Epoch: 18, Index: 47, Loss: 1.4499\n",
      "Epoch: 18, Index: 48, Loss: 1.0027\n",
      "Epoch: 18, Index: 49, Loss: 0.4816\n",
      "Epoch: 18, Index: 50, Loss: 1.4639\n",
      "Epoch: 18, Index: 51, Loss: 1.0227\n",
      "Epoch: 18, Index: 52, Loss: 2.3882\n",
      "Epoch: 18, Index: 53, Loss: 0.0595\n",
      "Epoch: 18, Index: 54, Loss: 2.0143\n",
      "Epoch: 18, Index: 55, Loss: 4.2887\n",
      "Epoch: 18, Index: 56, Loss: 2.0898\n",
      "Epoch: 18, Index: 57, Loss: 1.6793\n",
      "Epoch: 18, Index: 58, Loss: 2.9480\n",
      "Epoch: 18, Index: 59, Loss: 0.8081\n",
      "Epoch: 18, Index: 60, Loss: 1.2917\n",
      "Epoch: 18, Index: 61, Loss: 1.2875\n",
      "Epoch: 18, Index: 62, Loss: 0.9374\n",
      "Epoch: 18, Index: 63, Loss: 0.0440\n",
      "Epoch: 18, Index: 64, Loss: 0.9417\n",
      "Epoch: 18, Index: 65, Loss: 0.2633\n",
      "Epoch: 18, Index: 66, Loss: 0.3970\n",
      "Epoch: 18, Index: 67, Loss: 8.5482\n",
      "Epoch: 18, Index: 68, Loss: 0.3446\n",
      "Epoch: 18, Index: 69, Loss: 0.8088\n",
      "Epoch: 18, Index: 70, Loss: 0.8513\n",
      "Epoch: 18, Index: 71, Loss: 0.2489\n",
      "Epoch: 18, Index: 72, Loss: 0.4079\n",
      "Epoch: 18, Index: 73, Loss: 2.2978\n",
      "Epoch: 18, Index: 74, Loss: 1.2643\n",
      "Epoch: 18, Index: 75, Loss: 1.1896\n",
      "Epoch: 18, Index: 76, Loss: 0.2627\n",
      "Epoch: 18, Index: 77, Loss: 7.0194\n",
      "Epoch: 18, Index: 78, Loss: 0.2002\n",
      "Epoch: 18, Index: 79, Loss: 0.1460\n",
      "Epoch: 18, Index: 80, Loss: 0.0760\n",
      "Epoch: 18, Index: 81, Loss: 2.0488\n",
      "Epoch: 18, Index: 82, Loss: 4.2135\n",
      "Epoch: 18, Index: 83, Loss: 1.3883\n",
      "Epoch: 18, Index: 84, Loss: 0.2305\n",
      "Epoch: 18, Index: 85, Loss: 1.2505\n",
      "Epoch: 18, Index: 86, Loss: 0.2461\n",
      "Epoch: 18, Index: 87, Loss: 3.1631\n",
      "Epoch: 18, Index: 88, Loss: 2.3986\n",
      "Epoch: 18, Index: 89, Loss: 3.2297\n",
      "Epoch: 18, Index: 90, Loss: 1.1331\n",
      "Epoch: 18, Index: 91, Loss: 0.4852\n",
      "Epoch: 18, Index: 92, Loss: 2.3020\n",
      "Epoch: 18, Index: 93, Loss: 2.0905\n",
      "Epoch: 18, Index: 94, Loss: 4.5003\n",
      "Epoch: 18, Index: 95, Loss: 1.6710\n",
      "Epoch: 18, Index: 96, Loss: 1.7582\n",
      "Epoch: 18, Index: 97, Loss: 0.0227\n",
      "Epoch: 18, Index: 98, Loss: 1.4193\n",
      "Epoch: 18, Index: 99, Loss: 1.2584\n",
      "Epoch: 18, Index: 100, Loss: 1.1752\n",
      "Epoch: 18, Index: 101, Loss: 1.5753\n",
      "Epoch: 18, Index: 102, Loss: 0.9113\n",
      "Epoch: 18, Index: 103, Loss: 1.7335\n",
      "Epoch: 18, Index: 104, Loss: 2.0979\n",
      "Epoch: 18, Index: 105, Loss: 1.3167\n",
      "Epoch: 18, Index: 106, Loss: 0.1354\n",
      "Epoch: 18, Index: 107, Loss: 1.4214\n",
      "Epoch: 18, Index: 108, Loss: 0.2937\n",
      "Epoch: 18, Index: 109, Loss: 1.0941\n",
      "Epoch: 18, Index: 110, Loss: 0.4744\n",
      "Epoch: 18, Index: 111, Loss: 0.0313\n",
      "Epoch: 18, Index: 112, Loss: 0.7823\n",
      "Epoch: 18, Index: 113, Loss: 1.6257\n",
      "Epoch: 18, Index: 114, Loss: 0.5843\n",
      "Epoch: 18, Index: 115, Loss: 0.5614\n",
      "Epoch: 18, Index: 116, Loss: 3.7315\n",
      "Epoch: 18, Index: 117, Loss: 2.0773\n",
      "Epoch: 18, Index: 118, Loss: 1.1259\n",
      "Epoch: 18, Index: 119, Loss: 2.6586\n",
      "Epoch: 18, Index: 120, Loss: 5.9341\n",
      "Epoch: 18, Index: 121, Loss: 3.0443\n",
      "Epoch: 18, Index: 122, Loss: 5.9569\n",
      "Epoch: 18, Index: 123, Loss: 2.5169\n",
      "Epoch: 18, Index: 124, Loss: 3.0620\n",
      "Epoch: 18, Index: 125, Loss: 3.9399\n",
      "Epoch: 18, Index: 126, Loss: 1.2141\n",
      "Epoch: 18, Index: 127, Loss: 1.0669\n",
      "Epoch: 18, Index: 128, Loss: 0.1489\n",
      "Epoch: 18, Index: 129, Loss: 1.6873\n",
      "Epoch: 18, Index: 130, Loss: 1.4953\n",
      "Epoch: 18, Index: 131, Loss: 0.7745\n",
      "Epoch: 18, Index: 132, Loss: 2.0354\n",
      "Epoch: 18, Index: 133, Loss: 3.3459\n",
      "Epoch: 18, Index: 134, Loss: 0.5331\n",
      "Epoch: 18, Index: 135, Loss: 3.7592\n",
      "Epoch: 18, Index: 136, Loss: 1.6485\n",
      "Epoch: 18, Index: 137, Loss: 0.7255\n",
      "Epoch: 18, Index: 138, Loss: 4.4411\n",
      "Epoch: 18, Index: 139, Loss: 1.8224\n",
      "Epoch: 18, Index: 140, Loss: 0.6782\n",
      "Epoch: 18, Index: 141, Loss: 2.9081\n",
      "Epoch: 18, Index: 142, Loss: 7.5250\n",
      "Epoch: 18, Index: 143, Loss: 0.4211\n",
      "Epoch: 18, Index: 144, Loss: 6.0870\n",
      "Epoch: 18, Index: 145, Loss: 3.4336\n",
      "Epoch: 18, Index: 146, Loss: 1.5792\n",
      "Epoch: 18, Index: 147, Loss: 1.9319\n",
      "Epoch: 18, Index: 148, Loss: 1.6334\n",
      "Epoch: 18, Index: 149, Loss: 1.5660\n",
      "Epoch: 18, Index: 150, Loss: 0.3243\n",
      "Epoch: 18, Index: 151, Loss: 1.8600\n",
      "Epoch: 18, Index: 152, Loss: 1.7828\n",
      "Epoch: 18, Index: 153, Loss: 2.0579\n",
      "Epoch: 18, Index: 154, Loss: 2.0759\n",
      "Epoch: 18, Index: 155, Loss: 0.3316\n",
      "Epoch: 18, Index: 156, Loss: 0.0439\n",
      "Epoch: 18, Index: 157, Loss: 2.6273\n",
      "Epoch: 18, Index: 158, Loss: 2.3836\n",
      "Epoch: 18, Index: 159, Loss: 1.2963\n",
      "Epoch: 18, Index: 160, Loss: 0.0973\n",
      "Epoch: 18, Index: 161, Loss: 1.8326\n",
      "Epoch: 18, Index: 162, Loss: 0.0127\n",
      "Epoch: 18, Index: 163, Loss: 1.3715\n",
      "Epoch: 18, Index: 164, Loss: 3.0070\n",
      "Epoch: 18, Index: 165, Loss: 0.1823\n",
      "Epoch: 18, Index: 166, Loss: 5.9569\n",
      "Epoch: 18, Index: 167, Loss: 3.9575\n",
      "Epoch: 18, Index: 168, Loss: 0.4557\n",
      "Epoch: 18, Index: 169, Loss: 0.3217\n",
      "Epoch: 18, Index: 170, Loss: 8.1593\n",
      "Epoch: 18, Index: 171, Loss: 0.1997\n",
      "Epoch: 18, Index: 172, Loss: 0.3150\n",
      "Epoch: 18, Index: 173, Loss: 0.1826\n",
      "Epoch: 18, Index: 174, Loss: 4.6686\n",
      "Epoch: 18, Index: 175, Loss: 3.1318\n",
      "Epoch: 18, Index: 176, Loss: 3.1705\n",
      "Epoch: 18, Index: 177, Loss: 2.3400\n",
      "Epoch: 18, Index: 178, Loss: 1.1887\n",
      "Epoch: 18, Index: 179, Loss: 5.8926\n",
      "Epoch: 18, Index: 180, Loss: 0.4467\n",
      "Epoch: 18, Index: 181, Loss: 0.5577\n",
      "Epoch: 18, Index: 182, Loss: 7.2693\n",
      "Epoch: 18, Index: 183, Loss: 2.1552\n",
      "Epoch: 18, Index: 184, Loss: 0.0283\n",
      "Epoch: 18, Index: 185, Loss: 0.9313\n",
      "Epoch: 18, Index: 186, Loss: 0.6816\n",
      "Epoch: 18, Index: 187, Loss: 1.5799\n",
      "Epoch: 18, Index: 188, Loss: 0.5461\n",
      "Epoch: 18, Index: 189, Loss: 0.5537\n",
      "Epoch: 18, Index: 190, Loss: 1.2471\n",
      "Epoch: 18, Index: 191, Loss: 0.1708\n",
      "Epoch: 18, Index: 192, Loss: 0.7481\n",
      "Epoch: 18, Index: 193, Loss: 0.9316\n",
      "Epoch: 18, Index: 194, Loss: 0.6822\n",
      "Epoch: 18, Index: 195, Loss: 1.6384\n",
      "Epoch: 18, Index: 196, Loss: 0.3326\n",
      "Epoch: 18, Index: 197, Loss: 2.8009\n",
      "Epoch: 18, Index: 198, Loss: 2.4349\n",
      "Epoch: 18, Index: 199, Loss: 1.2124\n",
      "Epoch: 18, Index: 200, Loss: 4.1357\n",
      "Epoch: 18, Index: 201, Loss: 0.2955\n",
      "Epoch: 18, Index: 202, Loss: 1.0083\n",
      "Epoch: 18, Index: 203, Loss: 1.1631\n",
      "Epoch: 18, Index: 204, Loss: 0.1959\n",
      "Epoch: 18, Index: 205, Loss: 1.4172\n",
      "Epoch: 18, Index: 206, Loss: 1.0860\n",
      "Epoch: 18, Index: 207, Loss: 1.2344\n",
      "Epoch: 18, Index: 208, Loss: 1.4630\n",
      "Epoch: 18, Index: 209, Loss: 0.0003\n",
      "Epoch: 18, Index: 210, Loss: 1.3564\n",
      "Epoch: 18, Index: 211, Loss: 0.4640\n",
      "Epoch: 18, Index: 212, Loss: 0.5352\n",
      "Epoch: 18, Index: 213, Loss: 0.0291\n",
      "Epoch: 18, Index: 214, Loss: 2.1071\n",
      "Epoch: 18, Index: 215, Loss: 1.4595\n",
      "Epoch: 18, Index: 216, Loss: 2.2306\n",
      "Epoch: 18, Index: 217, Loss: 0.5072\n",
      "Epoch: 18, Index: 218, Loss: 1.7293\n",
      "Epoch: 18, Index: 219, Loss: 1.4047\n",
      "Epoch: 18, Index: 220, Loss: 0.5000\n",
      "Epoch: 18, Index: 221, Loss: 0.8287\n",
      "Epoch: 18, Index: 222, Loss: 0.6306\n",
      "Epoch: 18, Index: 223, Loss: 0.6526\n",
      "Epoch: 18, Index: 224, Loss: 3.6721\n",
      "Epoch: 18, Index: 225, Loss: 2.3774\n",
      "Epoch: 18, Index: 226, Loss: 0.3622\n",
      "Epoch: 18, Index: 227, Loss: 0.1852\n",
      "Epoch: 18, Index: 228, Loss: 0.5211\n",
      "Epoch: 18, Index: 229, Loss: 4.0356\n",
      "Epoch: 18, Index: 230, Loss: 5.3607\n",
      "Epoch: 18, Index: 231, Loss: 1.1658\n",
      "Epoch: 18, Index: 232, Loss: 0.3698\n",
      "Epoch: 18, Index: 233, Loss: 0.9266\n",
      "Epoch: 18, Index: 234, Loss: 0.3178\n",
      "Epoch: 18, Index: 235, Loss: 0.3653\n",
      "Epoch: 18, Index: 236, Loss: 0.1508\n",
      "Epoch: 18, Index: 237, Loss: 0.6503\n",
      "Epoch: 18, Index: 238, Loss: 0.8777\n",
      "Epoch: 18, Index: 239, Loss: 3.8138\n",
      "Epoch: 18, Index: 240, Loss: 0.6388\n",
      "Epoch: 18, Index: 241, Loss: 0.8101\n",
      "Epoch: 18, Index: 242, Loss: 3.0126\n",
      "Epoch: 18, Index: 243, Loss: 0.5812\n",
      "Epoch: 18, Index: 244, Loss: 1.4952\n",
      "Epoch: 18, Index: 245, Loss: 0.5862\n",
      "Epoch: 18, Index: 246, Loss: 0.8217\n",
      "Epoch: 18, Index: 247, Loss: 0.8946\n",
      "Epoch: 18, Index: 248, Loss: 0.2011\n",
      "Epoch: 18, Index: 249, Loss: 0.8329\n",
      "Epoch: 18, Index: 250, Loss: 6.5387\n",
      "Epoch: 18, Index: 251, Loss: 0.8691\n",
      "Epoch: 18, Index: 252, Loss: 0.7451\n",
      "Epoch: 18, Index: 253, Loss: 3.9789\n",
      "Epoch: 18, Index: 254, Loss: 1.2561\n",
      "Epoch: 18, Index: 255, Loss: 1.8906\n",
      "Epoch: 18, Index: 256, Loss: 4.4919\n",
      "Epoch: 18, Index: 257, Loss: 0.4566\n",
      "Epoch: 18, Index: 258, Loss: 0.2993\n",
      "Epoch: 18, Index: 259, Loss: 0.7642\n",
      "Epoch: 18, Index: 260, Loss: 2.1546\n",
      "Epoch: 18, Index: 261, Loss: 5.1796\n",
      "Epoch: 18, Index: 262, Loss: 2.8790\n",
      "Epoch: 18, Index: 263, Loss: 1.7219\n",
      "Epoch: 18, Index: 264, Loss: 0.6000\n",
      "Epoch: 18, Index: 265, Loss: 7.6138\n",
      "Epoch: 18, Index: 266, Loss: 0.2474\n",
      "Epoch: 18, Index: 267, Loss: 1.7754\n",
      "Epoch: 18, Index: 268, Loss: 0.7872\n",
      "Epoch: 18, Index: 269, Loss: 0.2154\n",
      "Epoch: 18, Index: 270, Loss: 0.7971\n",
      "Epoch: 18, Index: 271, Loss: 2.1501\n",
      "Epoch: 18, Index: 272, Loss: 3.0293\n",
      "Epoch: 18, Index: 273, Loss: 2.4748\n",
      "Epoch: 18, Index: 274, Loss: 1.8775\n",
      "Epoch: 18, Index: 275, Loss: 1.5331\n",
      "Epoch: 18, Index: 276, Loss: 0.0185\n",
      "Epoch: 18, Index: 277, Loss: 1.9396\n",
      "Epoch: 18, Index: 278, Loss: 6.3216\n",
      "Epoch: 18, Index: 279, Loss: 0.1708\n",
      "Epoch: 18, Index: 280, Loss: 5.5911\n",
      "Epoch: 18, Index: 281, Loss: 1.4836\n",
      "Epoch: 18, Index: 282, Loss: 1.3948\n",
      "Epoch: 18, Index: 283, Loss: 1.6054\n",
      "Epoch: 18, Index: 284, Loss: 3.8699\n",
      "Epoch: 18, Index: 285, Loss: 0.6185\n",
      "Epoch: 18, Index: 286, Loss: 2.4878\n",
      "Epoch: 18, Index: 287, Loss: 1.2008\n",
      "Epoch: 18, Index: 288, Loss: 0.5636\n",
      "Epoch: 18, Index: 289, Loss: 3.6150\n",
      "Epoch: 18, Index: 290, Loss: 0.7977\n",
      "Epoch: 18, Index: 291, Loss: 0.7823\n",
      "Epoch: 18, Index: 292, Loss: 1.7787\n",
      "Epoch: 18, Index: 293, Loss: 2.6524\n",
      "Epoch: 18, Index: 294, Loss: 1.7116\n",
      "Epoch: 18, Index: 295, Loss: 1.8715\n",
      "Epoch: 18, Index: 296, Loss: 0.8341\n",
      "Epoch: 18, Index: 297, Loss: 2.6994\n",
      "Epoch: 18, Index: 298, Loss: 3.4440\n",
      "Epoch: 18, Index: 299, Loss: 0.6469\n",
      "Epoch: 18, Index: 300, Loss: 3.8860\n",
      "Epoch: 18, Index: 301, Loss: 0.2795\n",
      "Epoch: 18, Index: 302, Loss: 0.0797\n",
      "Epoch: 18, Index: 303, Loss: 2.2101\n",
      "Epoch: 18, Index: 304, Loss: 0.1067\n",
      "Epoch: 18, Index: 305, Loss: 4.6328\n",
      "Epoch: 18, Index: 306, Loss: 0.4967\n",
      "Epoch: 18, Index: 307, Loss: 1.8756\n",
      "Epoch: 18, Index: 308, Loss: 0.5811\n",
      "Epoch: 18, Index: 309, Loss: 1.5065\n",
      "Epoch: 18, Index: 310, Loss: 0.1986\n",
      "Epoch: 18, Index: 311, Loss: 0.5031\n",
      "Epoch: 18, Index: 312, Loss: 0.4713\n",
      "Epoch: 18, Index: 313, Loss: 0.1765\n",
      "Epoch: 18, Index: 314, Loss: 0.4096\n",
      "Epoch: 18, Index: 315, Loss: 1.5378\n",
      "Epoch: 18, Index: 316, Loss: 0.2002\n",
      "Epoch: 18, Index: 317, Loss: 2.0034\n",
      "Epoch: 18, Index: 318, Loss: 0.2392\n",
      "Epoch: 18, Index: 319, Loss: 1.8030\n",
      "Epoch: 18, Index: 320, Loss: 1.5411\n",
      "Epoch: 18, Index: 321, Loss: 2.1286\n",
      "Epoch: 18, Index: 322, Loss: 1.4941\n",
      "Epoch: 18, Index: 323, Loss: 1.2893\n",
      "Epoch: 18, Index: 324, Loss: 8.6909\n",
      "Epoch: 18, Index: 325, Loss: 0.2863\n",
      "Epoch: 18, Index: 326, Loss: 3.0417\n",
      "Epoch: 18, Index: 327, Loss: 0.1109\n",
      "Epoch: 18, Index: 328, Loss: 1.1429\n",
      "Epoch: 18, Index: 329, Loss: 0.3941\n",
      "Epoch: 18, Index: 330, Loss: 0.7750\n",
      "Epoch: 18, Index: 331, Loss: 0.4421\n",
      "Epoch: 18, Index: 332, Loss: 2.2593\n",
      "Epoch: 18, Index: 333, Loss: 0.7968\n",
      "Epoch: 18, Index: 334, Loss: 2.4302\n",
      "Epoch: 18, Index: 335, Loss: 0.2020\n",
      "Epoch: 18, Index: 336, Loss: 1.3578\n",
      "Epoch: 18, Index: 337, Loss: 2.5641\n",
      "Epoch: 18, Index: 338, Loss: 6.2916\n",
      "Epoch: 18, Index: 339, Loss: 1.1207\n",
      "Epoch: 18, Index: 340, Loss: 0.9270\n",
      "Epoch: 18, Index: 341, Loss: 0.8548\n",
      "Epoch: 18, Index: 342, Loss: 0.3875\n",
      "Epoch: 18, Index: 343, Loss: 0.4902\n",
      "Epoch: 18, Index: 344, Loss: 0.6235\n",
      "Epoch: 18, Index: 345, Loss: 0.6041\n",
      "Epoch: 18, Index: 346, Loss: 0.5285\n",
      "Epoch: 18, Index: 347, Loss: 8.6089\n",
      "Epoch: 18, Index: 348, Loss: 2.7727\n",
      "Epoch: 18, Index: 349, Loss: 0.8197\n",
      "Epoch: 18, Index: 350, Loss: 1.6498\n",
      "Epoch: 18, Index: 351, Loss: 0.0531\n",
      "Epoch: 18, Index: 352, Loss: 0.0492\n",
      "Epoch: 18, Index: 353, Loss: 0.5786\n",
      "Epoch: 18, Index: 354, Loss: 1.7836\n",
      "Epoch: 18, Index: 355, Loss: 0.0700\n",
      "Epoch: 18, Index: 356, Loss: 0.6861\n",
      "Epoch: 18, Index: 357, Loss: 1.6204\n",
      "Epoch: 18, Index: 358, Loss: 1.5870\n",
      "Epoch: 18, Index: 359, Loss: 0.3879\n",
      "Epoch: 18, Index: 360, Loss: 0.1792\n",
      "Epoch: 18, Index: 361, Loss: 1.5590\n",
      "Epoch: 18, Index: 362, Loss: 1.4426\n",
      "Epoch: 18, Index: 363, Loss: 0.7277\n",
      "Epoch: 18, Index: 364, Loss: 0.5795\n",
      "Epoch: 18, Index: 365, Loss: 1.8502\n",
      "Epoch: 18, Index: 366, Loss: 1.0556\n",
      "Epoch: 18, Index: 367, Loss: 1.2048\n",
      "Epoch: 18, Index: 368, Loss: 2.8530\n",
      "Epoch: 18, Index: 369, Loss: 0.4594\n",
      "Epoch: 18, Index: 370, Loss: 1.1442\n",
      "Epoch: 18, Index: 371, Loss: 1.2970\n",
      "Epoch: 18, Index: 372, Loss: 2.5245\n",
      "Epoch: 18, Index: 373, Loss: 4.4816\n",
      "Epoch: 18, Index: 374, Loss: 2.3534\n",
      "Epoch: 18, Index: 375, Loss: 1.1038\n",
      "Epoch: 18, Index: 376, Loss: 2.1091\n",
      "Epoch: 18, Index: 377, Loss: 2.8331\n",
      "Epoch: 18, Index: 378, Loss: 0.2717\n",
      "Epoch: 18, Index: 379, Loss: 0.2570\n",
      "Epoch: 18, Index: 380, Loss: 1.0328\n",
      "Epoch: 18, Index: 381, Loss: 1.9183\n",
      "Epoch: 18, Index: 382, Loss: 1.9939\n",
      "Epoch: 18, Index: 383, Loss: 2.7718\n",
      "Epoch: 18, Index: 384, Loss: 3.4438\n",
      "Epoch: 18, Index: 385, Loss: 1.8872\n",
      "Epoch: 18, Index: 386, Loss: 0.0170\n",
      "Epoch: 18, Index: 387, Loss: 1.3745\n",
      "Epoch: 18, Index: 388, Loss: 14.8568\n",
      "Epoch: 18, Index: 389, Loss: 3.0185\n",
      "Epoch: 18, Index: 390, Loss: 0.6166\n",
      "Epoch: 18, Index: 391, Loss: 4.4073\n",
      "Epoch: 18, Index: 392, Loss: 0.2187\n",
      "Epoch: 18, Index: 393, Loss: 0.2400\n",
      "Epoch: 18, Index: 394, Loss: 0.1594\n",
      "Epoch: 18, Index: 395, Loss: 0.1298\n",
      "Epoch: 18, Index: 396, Loss: 1.6381\n",
      "Epoch: 18, Index: 397, Loss: 0.2865\n",
      "Epoch: 18, Index: 398, Loss: 0.5588\n",
      "Epoch: 18, Index: 399, Loss: 3.1133\n",
      "Epoch: 18, Index: 400, Loss: 2.0366\n",
      "Epoch: 18, Index: 401, Loss: 0.7075\n",
      "Epoch: 18, Index: 402, Loss: 4.8536\n",
      "Epoch: 18, Index: 403, Loss: 3.3063\n",
      "Epoch: 18, Index: 404, Loss: 0.0239\n",
      "Epoch: 18, Index: 405, Loss: 2.9081\n",
      "Epoch: 18, Index: 406, Loss: 1.3537\n",
      "Epoch: 18, Index: 407, Loss: 1.8841\n",
      "Epoch: 18, Index: 408, Loss: 1.0652\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec9ce71b57546b8b87e5609cb743f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Index: 0, Loss: 0.1253\n",
      "Epoch: 19, Index: 1, Loss: 2.4292\n",
      "Epoch: 19, Index: 2, Loss: 0.4882\n",
      "Epoch: 19, Index: 3, Loss: 1.8733\n",
      "Epoch: 19, Index: 4, Loss: 2.9731\n",
      "Epoch: 19, Index: 5, Loss: 1.9196\n",
      "Epoch: 19, Index: 6, Loss: 0.2610\n",
      "Epoch: 19, Index: 7, Loss: 0.2305\n",
      "Epoch: 19, Index: 8, Loss: 1.0173\n",
      "Epoch: 19, Index: 9, Loss: 2.7639\n",
      "Epoch: 19, Index: 10, Loss: 1.7408\n",
      "Epoch: 19, Index: 11, Loss: 0.9262\n",
      "Epoch: 19, Index: 12, Loss: 0.8871\n",
      "Epoch: 19, Index: 13, Loss: 0.0588\n",
      "Epoch: 19, Index: 14, Loss: 0.0159\n",
      "Epoch: 19, Index: 15, Loss: 11.0656\n",
      "Epoch: 19, Index: 16, Loss: 1.7936\n",
      "Epoch: 19, Index: 17, Loss: 3.0719\n",
      "Epoch: 19, Index: 18, Loss: 0.6325\n",
      "Epoch: 19, Index: 19, Loss: 1.0029\n",
      "Epoch: 19, Index: 20, Loss: 0.1587\n",
      "Epoch: 19, Index: 21, Loss: 2.7090\n",
      "Epoch: 19, Index: 22, Loss: 0.4114\n",
      "Epoch: 19, Index: 23, Loss: 0.0561\n",
      "Epoch: 19, Index: 24, Loss: 1.4673\n",
      "Epoch: 19, Index: 25, Loss: 1.0045\n",
      "Epoch: 19, Index: 26, Loss: 0.7169\n",
      "Epoch: 19, Index: 27, Loss: 0.4346\n",
      "Epoch: 19, Index: 28, Loss: 7.7671\n",
      "Epoch: 19, Index: 29, Loss: 1.9545\n",
      "Epoch: 19, Index: 30, Loss: 0.8180\n",
      "Epoch: 19, Index: 31, Loss: 0.3180\n",
      "Epoch: 19, Index: 32, Loss: 0.3926\n",
      "Epoch: 19, Index: 33, Loss: 0.2477\n",
      "Epoch: 19, Index: 34, Loss: 0.6646\n",
      "Epoch: 19, Index: 35, Loss: 0.1391\n",
      "Epoch: 19, Index: 36, Loss: 0.2829\n",
      "Epoch: 19, Index: 37, Loss: 2.8278\n",
      "Epoch: 19, Index: 38, Loss: 2.6596\n",
      "Epoch: 19, Index: 39, Loss: 0.9868\n",
      "Epoch: 19, Index: 40, Loss: 0.3617\n",
      "Epoch: 19, Index: 41, Loss: 1.7220\n",
      "Epoch: 19, Index: 42, Loss: 3.7585\n",
      "Epoch: 19, Index: 43, Loss: 0.2203\n",
      "Epoch: 19, Index: 44, Loss: 1.0858\n",
      "Epoch: 19, Index: 45, Loss: 2.2005\n",
      "Epoch: 19, Index: 46, Loss: 0.6454\n",
      "Epoch: 19, Index: 47, Loss: 2.4908\n",
      "Epoch: 19, Index: 48, Loss: 0.3980\n",
      "Epoch: 19, Index: 49, Loss: 1.3422\n",
      "Epoch: 19, Index: 50, Loss: 1.2749\n",
      "Epoch: 19, Index: 51, Loss: 0.2642\n",
      "Epoch: 19, Index: 52, Loss: 3.6374\n",
      "Epoch: 19, Index: 53, Loss: 15.5720\n",
      "Epoch: 19, Index: 54, Loss: 4.9153\n",
      "Epoch: 19, Index: 55, Loss: 0.6112\n",
      "Epoch: 19, Index: 56, Loss: 0.0411\n",
      "Epoch: 19, Index: 57, Loss: 0.3525\n",
      "Epoch: 19, Index: 58, Loss: 2.5332\n",
      "Epoch: 19, Index: 59, Loss: 0.9235\n",
      "Epoch: 19, Index: 60, Loss: 1.9626\n",
      "Epoch: 19, Index: 61, Loss: 5.8391\n",
      "Epoch: 19, Index: 62, Loss: 1.0652\n",
      "Epoch: 19, Index: 63, Loss: 0.8886\n",
      "Epoch: 19, Index: 64, Loss: 0.0875\n",
      "Epoch: 19, Index: 65, Loss: 0.1170\n",
      "Epoch: 19, Index: 66, Loss: 1.1389\n",
      "Epoch: 19, Index: 67, Loss: 4.8732\n",
      "Epoch: 19, Index: 68, Loss: 1.4317\n",
      "Epoch: 19, Index: 69, Loss: 1.4289\n",
      "Epoch: 19, Index: 70, Loss: 1.8248\n",
      "Epoch: 19, Index: 71, Loss: 1.0728\n",
      "Epoch: 19, Index: 72, Loss: 0.2285\n",
      "Epoch: 19, Index: 73, Loss: 2.9036\n",
      "Epoch: 19, Index: 74, Loss: 0.9486\n",
      "Epoch: 19, Index: 75, Loss: 0.8329\n",
      "Epoch: 19, Index: 76, Loss: 1.2281\n",
      "Epoch: 19, Index: 77, Loss: 1.2424\n",
      "Epoch: 19, Index: 78, Loss: 5.7258\n",
      "Epoch: 19, Index: 79, Loss: 1.1433\n",
      "Epoch: 19, Index: 80, Loss: 2.6586\n",
      "Epoch: 19, Index: 81, Loss: 2.0280\n",
      "Epoch: 19, Index: 82, Loss: 0.4756\n",
      "Epoch: 19, Index: 83, Loss: 0.0125\n",
      "Epoch: 19, Index: 84, Loss: 0.1292\n",
      "Epoch: 19, Index: 85, Loss: 2.5237\n",
      "Epoch: 19, Index: 86, Loss: 3.0001\n",
      "Epoch: 19, Index: 87, Loss: 2.1321\n",
      "Epoch: 19, Index: 88, Loss: 18.0732\n",
      "Epoch: 19, Index: 89, Loss: 2.1271\n",
      "Epoch: 19, Index: 90, Loss: 4.0430\n",
      "Epoch: 19, Index: 91, Loss: 1.5803\n",
      "Epoch: 19, Index: 92, Loss: 1.2765\n",
      "Epoch: 19, Index: 93, Loss: 1.2335\n",
      "Epoch: 19, Index: 94, Loss: 1.0236\n",
      "Epoch: 19, Index: 95, Loss: 3.9204\n",
      "Epoch: 19, Index: 96, Loss: 2.4844\n",
      "Epoch: 19, Index: 97, Loss: 1.4298\n",
      "Epoch: 19, Index: 98, Loss: 1.9490\n",
      "Epoch: 19, Index: 99, Loss: 0.9232\n",
      "Epoch: 19, Index: 100, Loss: 0.5005\n",
      "Epoch: 19, Index: 101, Loss: 3.4982\n",
      "Epoch: 19, Index: 102, Loss: 0.9927\n",
      "Epoch: 19, Index: 103, Loss: 1.2633\n",
      "Epoch: 19, Index: 104, Loss: 0.7398\n",
      "Epoch: 19, Index: 105, Loss: 0.0368\n",
      "Epoch: 19, Index: 106, Loss: 3.1373\n",
      "Epoch: 19, Index: 107, Loss: 5.0478\n",
      "Epoch: 19, Index: 108, Loss: 1.3411\n",
      "Epoch: 19, Index: 109, Loss: 6.4962\n",
      "Epoch: 19, Index: 110, Loss: 0.8193\n",
      "Epoch: 19, Index: 111, Loss: 0.5313\n",
      "Epoch: 19, Index: 112, Loss: 0.7336\n",
      "Epoch: 19, Index: 113, Loss: 2.2662\n",
      "Epoch: 19, Index: 114, Loss: 0.8775\n",
      "Epoch: 19, Index: 115, Loss: 1.9114\n",
      "Epoch: 19, Index: 116, Loss: 1.7147\n",
      "Epoch: 19, Index: 117, Loss: 1.2194\n",
      "Epoch: 19, Index: 118, Loss: 1.9729\n",
      "Epoch: 19, Index: 119, Loss: 1.0050\n",
      "Epoch: 19, Index: 120, Loss: 1.3908\n",
      "Epoch: 19, Index: 121, Loss: 3.6975\n",
      "Epoch: 19, Index: 122, Loss: 0.0049\n",
      "Epoch: 19, Index: 123, Loss: 1.9772\n",
      "Epoch: 19, Index: 124, Loss: 2.3279\n",
      "Epoch: 19, Index: 125, Loss: 4.5382\n",
      "Epoch: 19, Index: 126, Loss: 1.9018\n",
      "Epoch: 19, Index: 127, Loss: 2.0166\n",
      "Epoch: 19, Index: 128, Loss: 0.2847\n",
      "Epoch: 19, Index: 129, Loss: 0.6714\n",
      "Epoch: 19, Index: 130, Loss: 0.8838\n",
      "Epoch: 19, Index: 131, Loss: 5.5256\n",
      "Epoch: 19, Index: 132, Loss: 1.5914\n",
      "Epoch: 19, Index: 133, Loss: 0.9062\n",
      "Epoch: 19, Index: 134, Loss: 1.0788\n",
      "Epoch: 19, Index: 135, Loss: 1.4068\n",
      "Epoch: 19, Index: 136, Loss: 0.9643\n",
      "Epoch: 19, Index: 137, Loss: 1.8724\n",
      "Epoch: 19, Index: 138, Loss: 0.9496\n",
      "Epoch: 19, Index: 139, Loss: 0.6847\n",
      "Epoch: 19, Index: 140, Loss: 0.6791\n",
      "Epoch: 19, Index: 141, Loss: 4.5807\n",
      "Epoch: 19, Index: 142, Loss: 0.4332\n",
      "Epoch: 19, Index: 143, Loss: 3.4921\n",
      "Epoch: 19, Index: 144, Loss: 0.4589\n",
      "Epoch: 19, Index: 145, Loss: 0.4814\n",
      "Epoch: 19, Index: 146, Loss: 0.2919\n",
      "Epoch: 19, Index: 147, Loss: 0.6503\n",
      "Epoch: 19, Index: 148, Loss: 1.2445\n",
      "Epoch: 19, Index: 149, Loss: 0.0965\n",
      "Epoch: 19, Index: 150, Loss: 4.0147\n",
      "Epoch: 19, Index: 151, Loss: 1.1056\n",
      "Epoch: 19, Index: 152, Loss: 2.3450\n",
      "Epoch: 19, Index: 153, Loss: 2.6051\n",
      "Epoch: 19, Index: 154, Loss: 0.7239\n",
      "Epoch: 19, Index: 155, Loss: 0.7360\n",
      "Epoch: 19, Index: 156, Loss: 0.5904\n",
      "Epoch: 19, Index: 157, Loss: 0.8373\n",
      "Epoch: 19, Index: 158, Loss: 0.5473\n",
      "Epoch: 19, Index: 159, Loss: 2.0203\n",
      "Epoch: 19, Index: 160, Loss: 1.4837\n",
      "Epoch: 19, Index: 161, Loss: 3.2853\n",
      "Epoch: 19, Index: 162, Loss: 0.4173\n",
      "Epoch: 19, Index: 163, Loss: 2.5118\n",
      "Epoch: 19, Index: 164, Loss: 0.5537\n",
      "Epoch: 19, Index: 165, Loss: 0.9685\n",
      "Epoch: 19, Index: 166, Loss: 6.0244\n",
      "Epoch: 19, Index: 167, Loss: 0.4160\n",
      "Epoch: 19, Index: 168, Loss: 1.1668\n",
      "Epoch: 19, Index: 169, Loss: 1.3539\n",
      "Epoch: 19, Index: 170, Loss: 0.1677\n",
      "Epoch: 19, Index: 171, Loss: 2.1511\n",
      "Epoch: 19, Index: 172, Loss: 3.1248\n",
      "Epoch: 19, Index: 173, Loss: 1.7291\n",
      "Epoch: 19, Index: 174, Loss: 1.1633\n",
      "Epoch: 19, Index: 175, Loss: 0.2212\n",
      "Epoch: 19, Index: 176, Loss: 13.4215\n",
      "Epoch: 19, Index: 177, Loss: 0.3068\n",
      "Epoch: 19, Index: 178, Loss: 1.6458\n",
      "Epoch: 19, Index: 179, Loss: 1.3013\n",
      "Epoch: 19, Index: 180, Loss: 2.8672\n",
      "Epoch: 19, Index: 181, Loss: 3.6941\n",
      "Epoch: 19, Index: 182, Loss: 0.1031\n",
      "Epoch: 19, Index: 183, Loss: 0.8785\n",
      "Epoch: 19, Index: 184, Loss: 0.1763\n",
      "Epoch: 19, Index: 185, Loss: 4.5367\n",
      "Epoch: 19, Index: 186, Loss: 1.8815\n",
      "Epoch: 19, Index: 187, Loss: 3.9767\n",
      "Epoch: 19, Index: 188, Loss: 1.2191\n",
      "Epoch: 19, Index: 189, Loss: 2.7295\n",
      "Epoch: 19, Index: 190, Loss: 1.0345\n",
      "Epoch: 19, Index: 191, Loss: 3.0224\n",
      "Epoch: 19, Index: 192, Loss: 1.9419\n",
      "Epoch: 19, Index: 193, Loss: 3.8367\n",
      "Epoch: 19, Index: 194, Loss: 0.0086\n",
      "Epoch: 19, Index: 195, Loss: 1.2016\n",
      "Epoch: 19, Index: 196, Loss: 1.1757\n",
      "Epoch: 19, Index: 197, Loss: 0.8720\n",
      "Epoch: 19, Index: 198, Loss: 2.0707\n",
      "Epoch: 19, Index: 199, Loss: 0.8902\n",
      "Epoch: 19, Index: 200, Loss: 1.9069\n",
      "Epoch: 19, Index: 201, Loss: 0.0917\n",
      "Epoch: 19, Index: 202, Loss: 0.6978\n",
      "Epoch: 19, Index: 203, Loss: 3.4664\n",
      "Epoch: 19, Index: 204, Loss: 0.2460\n",
      "Epoch: 19, Index: 205, Loss: 1.6941\n",
      "Epoch: 19, Index: 206, Loss: 6.8052\n",
      "Epoch: 19, Index: 207, Loss: 0.4574\n",
      "Epoch: 19, Index: 208, Loss: 0.4814\n",
      "Epoch: 19, Index: 209, Loss: 2.2692\n",
      "Epoch: 19, Index: 210, Loss: 2.6763\n",
      "Epoch: 19, Index: 211, Loss: 1.6881\n",
      "Epoch: 19, Index: 212, Loss: 0.3552\n",
      "Epoch: 19, Index: 213, Loss: 5.1075\n",
      "Epoch: 19, Index: 214, Loss: 0.2180\n",
      "Epoch: 19, Index: 215, Loss: 4.8008\n",
      "Epoch: 19, Index: 216, Loss: 1.8006\n",
      "Epoch: 19, Index: 217, Loss: 0.5406\n",
      "Epoch: 19, Index: 218, Loss: 4.5788\n",
      "Epoch: 19, Index: 219, Loss: 1.4405\n",
      "Epoch: 19, Index: 220, Loss: 2.7052\n",
      "Epoch: 19, Index: 221, Loss: 0.8456\n",
      "Epoch: 19, Index: 222, Loss: 0.4364\n",
      "Epoch: 19, Index: 223, Loss: 0.4464\n",
      "Epoch: 19, Index: 224, Loss: 5.4088\n",
      "Epoch: 19, Index: 225, Loss: 2.6467\n",
      "Epoch: 19, Index: 226, Loss: 4.5244\n",
      "Epoch: 19, Index: 227, Loss: 2.2923\n",
      "Epoch: 19, Index: 228, Loss: 2.4022\n",
      "Epoch: 19, Index: 229, Loss: 1.7652\n",
      "Epoch: 19, Index: 230, Loss: 0.5333\n",
      "Epoch: 19, Index: 231, Loss: 0.4762\n",
      "Epoch: 19, Index: 232, Loss: 0.5672\n",
      "Epoch: 19, Index: 233, Loss: 5.4511\n",
      "Epoch: 19, Index: 234, Loss: 0.0460\n",
      "Epoch: 19, Index: 235, Loss: 0.1144\n",
      "Epoch: 19, Index: 236, Loss: 7.6425\n",
      "Epoch: 19, Index: 237, Loss: 2.0950\n",
      "Epoch: 19, Index: 238, Loss: 0.0839\n",
      "Epoch: 19, Index: 239, Loss: 2.9746\n",
      "Epoch: 19, Index: 240, Loss: 0.8479\n",
      "Epoch: 19, Index: 241, Loss: 0.9308\n",
      "Epoch: 19, Index: 242, Loss: 0.4946\n",
      "Epoch: 19, Index: 243, Loss: 0.7689\n",
      "Epoch: 19, Index: 244, Loss: 1.3238\n",
      "Epoch: 19, Index: 245, Loss: 1.7529\n",
      "Epoch: 19, Index: 246, Loss: 0.2983\n",
      "Epoch: 19, Index: 247, Loss: 3.0652\n",
      "Epoch: 19, Index: 248, Loss: 2.7833\n",
      "Epoch: 19, Index: 249, Loss: 1.2285\n",
      "Epoch: 19, Index: 250, Loss: 1.7448\n",
      "Epoch: 19, Index: 251, Loss: 2.0574\n",
      "Epoch: 19, Index: 252, Loss: 6.6394\n",
      "Epoch: 19, Index: 253, Loss: 1.6490\n",
      "Epoch: 19, Index: 254, Loss: 3.4612\n",
      "Epoch: 19, Index: 255, Loss: 1.3067\n",
      "Epoch: 19, Index: 256, Loss: 0.8047\n",
      "Epoch: 19, Index: 257, Loss: 0.1735\n",
      "Epoch: 19, Index: 258, Loss: 2.1991\n",
      "Epoch: 19, Index: 259, Loss: 5.7695\n",
      "Epoch: 19, Index: 260, Loss: 4.0594\n",
      "Epoch: 19, Index: 261, Loss: 0.6787\n",
      "Epoch: 19, Index: 262, Loss: 2.2109\n",
      "Epoch: 19, Index: 263, Loss: 7.0459\n",
      "Epoch: 19, Index: 264, Loss: 1.4406\n",
      "Epoch: 19, Index: 265, Loss: 0.4393\n",
      "Epoch: 19, Index: 266, Loss: 3.1307\n",
      "Epoch: 19, Index: 267, Loss: 0.2302\n",
      "Epoch: 19, Index: 268, Loss: 0.4157\n",
      "Epoch: 19, Index: 269, Loss: 3.6858\n",
      "Epoch: 19, Index: 270, Loss: 2.0628\n",
      "Epoch: 19, Index: 271, Loss: 0.6151\n",
      "Epoch: 19, Index: 272, Loss: 2.0630\n",
      "Epoch: 19, Index: 273, Loss: 3.1528\n",
      "Epoch: 19, Index: 274, Loss: 2.8173\n",
      "Epoch: 19, Index: 275, Loss: 1.4495\n",
      "Epoch: 19, Index: 276, Loss: 1.0951\n",
      "Epoch: 19, Index: 277, Loss: 2.9193\n",
      "Epoch: 19, Index: 278, Loss: 2.5288\n",
      "Epoch: 19, Index: 279, Loss: 0.4602\n",
      "Epoch: 19, Index: 280, Loss: 0.0377\n",
      "Epoch: 19, Index: 281, Loss: 2.1669\n",
      "Epoch: 19, Index: 282, Loss: 0.3098\n",
      "Epoch: 19, Index: 283, Loss: 0.6876\n",
      "Epoch: 19, Index: 284, Loss: 1.6689\n",
      "Epoch: 19, Index: 285, Loss: 2.0650\n",
      "Epoch: 19, Index: 286, Loss: 1.4942\n",
      "Epoch: 19, Index: 287, Loss: 3.0371\n",
      "Epoch: 19, Index: 288, Loss: 0.1887\n",
      "Epoch: 19, Index: 289, Loss: 5.9034\n",
      "Epoch: 19, Index: 290, Loss: 0.9059\n",
      "Epoch: 19, Index: 291, Loss: 1.2496\n",
      "Epoch: 19, Index: 292, Loss: 2.0833\n",
      "Epoch: 19, Index: 293, Loss: 3.5005\n",
      "Epoch: 19, Index: 294, Loss: 3.6330\n",
      "Epoch: 19, Index: 295, Loss: 0.0899\n",
      "Epoch: 19, Index: 296, Loss: 2.2855\n",
      "Epoch: 19, Index: 297, Loss: 1.4298\n",
      "Epoch: 19, Index: 298, Loss: 1.6129\n",
      "Epoch: 19, Index: 299, Loss: 0.3279\n",
      "Epoch: 19, Index: 300, Loss: 2.1834\n",
      "Epoch: 19, Index: 301, Loss: 0.5215\n",
      "Epoch: 19, Index: 302, Loss: 1.3676\n",
      "Epoch: 19, Index: 303, Loss: 2.1391\n",
      "Epoch: 19, Index: 304, Loss: 0.7668\n",
      "Epoch: 19, Index: 305, Loss: 0.1053\n",
      "Epoch: 19, Index: 306, Loss: 2.6035\n",
      "Epoch: 19, Index: 307, Loss: 0.8950\n",
      "Epoch: 19, Index: 308, Loss: 1.0269\n",
      "Epoch: 19, Index: 309, Loss: 0.1949\n",
      "Epoch: 19, Index: 310, Loss: 1.3069\n",
      "Epoch: 19, Index: 311, Loss: 11.7505\n",
      "Epoch: 19, Index: 312, Loss: 0.4545\n",
      "Epoch: 19, Index: 313, Loss: 0.6256\n",
      "Epoch: 19, Index: 314, Loss: 4.6770\n",
      "Epoch: 19, Index: 315, Loss: 0.0624\n",
      "Epoch: 19, Index: 316, Loss: 0.5537\n",
      "Epoch: 19, Index: 317, Loss: 0.3718\n",
      "Epoch: 19, Index: 318, Loss: 0.5348\n",
      "Epoch: 19, Index: 319, Loss: 0.4964\n",
      "Epoch: 19, Index: 320, Loss: 1.0499\n",
      "Epoch: 19, Index: 321, Loss: 3.1012\n",
      "Epoch: 19, Index: 322, Loss: 0.6334\n",
      "Epoch: 19, Index: 323, Loss: 1.4727\n",
      "Epoch: 19, Index: 324, Loss: 3.0857\n",
      "Epoch: 19, Index: 325, Loss: 1.0514\n",
      "Epoch: 19, Index: 326, Loss: 0.4933\n",
      "Epoch: 19, Index: 327, Loss: 1.0224\n",
      "Epoch: 19, Index: 328, Loss: 0.1388\n",
      "Epoch: 19, Index: 329, Loss: 1.1952\n",
      "Epoch: 19, Index: 330, Loss: 1.0322\n",
      "Epoch: 19, Index: 331, Loss: 1.3376\n",
      "Epoch: 19, Index: 332, Loss: 0.6373\n",
      "Epoch: 19, Index: 333, Loss: 0.7351\n",
      "Epoch: 19, Index: 334, Loss: 0.7868\n",
      "Epoch: 19, Index: 335, Loss: 1.0491\n",
      "Epoch: 19, Index: 336, Loss: 2.4037\n",
      "Epoch: 19, Index: 337, Loss: 5.7418\n",
      "Epoch: 19, Index: 338, Loss: 0.1246\n",
      "Epoch: 19, Index: 339, Loss: 0.7567\n",
      "Epoch: 19, Index: 340, Loss: 0.8593\n",
      "Epoch: 19, Index: 341, Loss: 0.1765\n",
      "Epoch: 19, Index: 342, Loss: 0.4248\n",
      "Epoch: 19, Index: 343, Loss: 8.6156\n",
      "Epoch: 19, Index: 344, Loss: 0.2278\n",
      "Epoch: 19, Index: 345, Loss: 1.0602\n",
      "Epoch: 19, Index: 346, Loss: 0.0614\n",
      "Epoch: 19, Index: 347, Loss: 3.9086\n",
      "Epoch: 19, Index: 348, Loss: 1.1504\n",
      "Epoch: 19, Index: 349, Loss: 0.2791\n",
      "Epoch: 19, Index: 350, Loss: 3.6045\n",
      "Epoch: 19, Index: 351, Loss: 0.3740\n",
      "Epoch: 19, Index: 352, Loss: 4.6191\n",
      "Epoch: 19, Index: 353, Loss: 0.2264\n",
      "Epoch: 19, Index: 354, Loss: 0.2337\n",
      "Epoch: 19, Index: 355, Loss: 3.2675\n",
      "Epoch: 19, Index: 356, Loss: 2.1826\n",
      "Epoch: 19, Index: 357, Loss: 1.1417\n",
      "Epoch: 19, Index: 358, Loss: 6.4525\n",
      "Epoch: 19, Index: 359, Loss: 0.7011\n",
      "Epoch: 19, Index: 360, Loss: 0.2832\n",
      "Epoch: 19, Index: 361, Loss: 1.1186\n",
      "Epoch: 19, Index: 362, Loss: 0.6764\n",
      "Epoch: 19, Index: 363, Loss: 0.4585\n",
      "Epoch: 19, Index: 364, Loss: 5.3312\n",
      "Epoch: 19, Index: 365, Loss: 0.5496\n",
      "Epoch: 19, Index: 366, Loss: 1.9892\n",
      "Epoch: 19, Index: 367, Loss: 0.6184\n",
      "Epoch: 19, Index: 368, Loss: 2.5643\n",
      "Epoch: 19, Index: 369, Loss: 0.5272\n",
      "Epoch: 19, Index: 370, Loss: 0.6256\n",
      "Epoch: 19, Index: 371, Loss: 0.7364\n",
      "Epoch: 19, Index: 372, Loss: 2.2897\n",
      "Epoch: 19, Index: 373, Loss: 4.4756\n",
      "Epoch: 19, Index: 374, Loss: 6.7780\n",
      "Epoch: 19, Index: 375, Loss: 0.4109\n",
      "Epoch: 19, Index: 376, Loss: 0.8958\n",
      "Epoch: 19, Index: 377, Loss: 1.8981\n",
      "Epoch: 19, Index: 378, Loss: 0.6049\n",
      "Epoch: 19, Index: 379, Loss: 1.1889\n",
      "Epoch: 19, Index: 380, Loss: 1.4825\n",
      "Epoch: 19, Index: 381, Loss: 0.5632\n",
      "Epoch: 19, Index: 382, Loss: 0.4728\n",
      "Epoch: 19, Index: 383, Loss: 9.9435\n",
      "Epoch: 19, Index: 384, Loss: 1.2019\n",
      "Epoch: 19, Index: 385, Loss: 1.6891\n",
      "Epoch: 19, Index: 386, Loss: 3.4771\n",
      "Epoch: 19, Index: 387, Loss: 1.1891\n",
      "Epoch: 19, Index: 388, Loss: 0.1715\n",
      "Epoch: 19, Index: 389, Loss: 1.9767\n",
      "Epoch: 19, Index: 390, Loss: 2.1640\n",
      "Epoch: 19, Index: 391, Loss: 3.1262\n",
      "Epoch: 19, Index: 392, Loss: 2.8149\n",
      "Epoch: 19, Index: 393, Loss: 1.1267\n",
      "Epoch: 19, Index: 394, Loss: 1.6979\n",
      "Epoch: 19, Index: 395, Loss: 1.5219\n",
      "Epoch: 19, Index: 396, Loss: 0.1934\n",
      "Epoch: 19, Index: 397, Loss: 1.9460\n",
      "Epoch: 19, Index: 398, Loss: 0.7494\n",
      "Epoch: 19, Index: 399, Loss: 1.4066\n",
      "Epoch: 19, Index: 400, Loss: 1.6406\n",
      "Epoch: 19, Index: 401, Loss: 3.9804\n",
      "Epoch: 19, Index: 402, Loss: 0.4325\n",
      "Epoch: 19, Index: 403, Loss: 0.8619\n",
      "Epoch: 19, Index: 404, Loss: 1.1481\n",
      "Epoch: 19, Index: 405, Loss: 0.9814\n",
      "Epoch: 19, Index: 406, Loss: 0.8842\n",
      "Epoch: 19, Index: 407, Loss: 0.7329\n",
      "Epoch: 19, Index: 408, Loss: 0.5964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08d7b9381b040b78f0104bc04dec128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Index: 0, Loss: 0.2012\n",
      "Epoch: 20, Index: 1, Loss: 2.0930\n",
      "Epoch: 20, Index: 2, Loss: 1.9187\n",
      "Epoch: 20, Index: 3, Loss: 5.7469\n",
      "Epoch: 20, Index: 4, Loss: 0.1749\n",
      "Epoch: 20, Index: 5, Loss: 1.3195\n",
      "Epoch: 20, Index: 6, Loss: 1.9301\n",
      "Epoch: 20, Index: 7, Loss: 0.7936\n",
      "Epoch: 20, Index: 8, Loss: 2.5038\n",
      "Epoch: 20, Index: 9, Loss: 1.7743\n",
      "Epoch: 20, Index: 10, Loss: 0.4345\n",
      "Epoch: 20, Index: 11, Loss: 4.5305\n",
      "Epoch: 20, Index: 12, Loss: 2.4952\n",
      "Epoch: 20, Index: 13, Loss: 0.6124\n",
      "Epoch: 20, Index: 14, Loss: 0.4106\n",
      "Epoch: 20, Index: 15, Loss: 2.5457\n",
      "Epoch: 20, Index: 16, Loss: 1.5627\n",
      "Epoch: 20, Index: 17, Loss: 0.8193\n",
      "Epoch: 20, Index: 18, Loss: 0.4977\n",
      "Epoch: 20, Index: 19, Loss: 6.3062\n",
      "Epoch: 20, Index: 20, Loss: 0.7081\n",
      "Epoch: 20, Index: 21, Loss: 0.4292\n",
      "Epoch: 20, Index: 22, Loss: 3.6622\n",
      "Epoch: 20, Index: 23, Loss: 0.2460\n",
      "Epoch: 20, Index: 24, Loss: 0.0411\n",
      "Epoch: 20, Index: 25, Loss: 3.2209\n",
      "Epoch: 20, Index: 26, Loss: 0.5847\n",
      "Epoch: 20, Index: 27, Loss: 1.4520\n",
      "Epoch: 20, Index: 28, Loss: 1.0169\n",
      "Epoch: 20, Index: 29, Loss: 0.0435\n",
      "Epoch: 20, Index: 30, Loss: 2.2215\n",
      "Epoch: 20, Index: 31, Loss: 0.9449\n",
      "Epoch: 20, Index: 32, Loss: 0.6598\n",
      "Epoch: 20, Index: 33, Loss: 2.0340\n",
      "Epoch: 20, Index: 34, Loss: 1.6522\n",
      "Epoch: 20, Index: 35, Loss: 0.4777\n",
      "Epoch: 20, Index: 36, Loss: 2.4664\n",
      "Epoch: 20, Index: 37, Loss: 3.6615\n",
      "Epoch: 20, Index: 38, Loss: 2.5946\n",
      "Epoch: 20, Index: 39, Loss: 3.1671\n",
      "Epoch: 20, Index: 40, Loss: 1.1048\n",
      "Epoch: 20, Index: 41, Loss: 4.9783\n",
      "Epoch: 20, Index: 42, Loss: 1.6358\n",
      "Epoch: 20, Index: 43, Loss: 3.5149\n",
      "Epoch: 20, Index: 44, Loss: 0.4325\n",
      "Epoch: 20, Index: 45, Loss: 0.4833\n",
      "Epoch: 20, Index: 46, Loss: 0.9253\n",
      "Epoch: 20, Index: 47, Loss: 0.4364\n",
      "Epoch: 20, Index: 48, Loss: 3.4065\n",
      "Epoch: 20, Index: 49, Loss: 0.2819\n",
      "Epoch: 20, Index: 50, Loss: 3.4816\n",
      "Epoch: 20, Index: 51, Loss: 3.5103\n",
      "Epoch: 20, Index: 52, Loss: 2.5805\n",
      "Epoch: 20, Index: 53, Loss: 3.4703\n",
      "Epoch: 20, Index: 54, Loss: 0.5332\n",
      "Epoch: 20, Index: 55, Loss: 0.2648\n",
      "Epoch: 20, Index: 56, Loss: 3.5443\n",
      "Epoch: 20, Index: 57, Loss: 3.3973\n",
      "Epoch: 20, Index: 58, Loss: 1.4934\n",
      "Epoch: 20, Index: 59, Loss: 0.7073\n",
      "Epoch: 20, Index: 60, Loss: 2.2284\n",
      "Epoch: 20, Index: 61, Loss: 0.0870\n",
      "Epoch: 20, Index: 62, Loss: 0.0588\n",
      "Epoch: 20, Index: 63, Loss: 1.0382\n",
      "Epoch: 20, Index: 64, Loss: 1.6160\n",
      "Epoch: 20, Index: 65, Loss: 1.3193\n",
      "Epoch: 20, Index: 66, Loss: 3.9041\n",
      "Epoch: 20, Index: 67, Loss: 2.1945\n",
      "Epoch: 20, Index: 68, Loss: 0.9288\n",
      "Epoch: 20, Index: 69, Loss: 0.2394\n",
      "Epoch: 20, Index: 70, Loss: 1.9242\n",
      "Epoch: 20, Index: 71, Loss: 0.4300\n",
      "Epoch: 20, Index: 72, Loss: 2.5756\n",
      "Epoch: 20, Index: 73, Loss: 0.4953\n",
      "Epoch: 20, Index: 74, Loss: 0.4240\n",
      "Epoch: 20, Index: 75, Loss: 1.7999\n",
      "Epoch: 20, Index: 76, Loss: 0.8825\n",
      "Epoch: 20, Index: 77, Loss: 0.1911\n",
      "Epoch: 20, Index: 78, Loss: 0.6754\n",
      "Epoch: 20, Index: 79, Loss: 1.9319\n",
      "Epoch: 20, Index: 80, Loss: 2.7605\n",
      "Epoch: 20, Index: 81, Loss: 3.2962\n",
      "Epoch: 20, Index: 82, Loss: 0.3875\n",
      "Epoch: 20, Index: 83, Loss: 0.2372\n",
      "Epoch: 20, Index: 84, Loss: 1.2705\n",
      "Epoch: 20, Index: 85, Loss: 2.1271\n",
      "Epoch: 20, Index: 86, Loss: 9.3855\n",
      "Epoch: 20, Index: 87, Loss: 3.5599\n",
      "Epoch: 20, Index: 88, Loss: 2.3070\n",
      "Epoch: 20, Index: 89, Loss: 0.9953\n",
      "Epoch: 20, Index: 90, Loss: 0.8069\n",
      "Epoch: 20, Index: 91, Loss: 3.1168\n",
      "Epoch: 20, Index: 92, Loss: 2.4013\n",
      "Epoch: 20, Index: 93, Loss: 1.1173\n",
      "Epoch: 20, Index: 94, Loss: 1.1903\n",
      "Epoch: 20, Index: 95, Loss: 2.2731\n",
      "Epoch: 20, Index: 96, Loss: 0.7757\n",
      "Epoch: 20, Index: 97, Loss: 3.6270\n",
      "Epoch: 20, Index: 98, Loss: 2.4575\n",
      "Epoch: 20, Index: 99, Loss: 4.1596\n",
      "Epoch: 20, Index: 100, Loss: 3.2961\n",
      "Epoch: 20, Index: 101, Loss: 2.1765\n",
      "Epoch: 20, Index: 102, Loss: 1.7087\n",
      "Epoch: 20, Index: 103, Loss: 0.3098\n",
      "Epoch: 20, Index: 104, Loss: 0.1364\n",
      "Epoch: 20, Index: 105, Loss: 1.4048\n",
      "Epoch: 20, Index: 106, Loss: 0.2812\n",
      "Epoch: 20, Index: 107, Loss: 0.4221\n",
      "Epoch: 20, Index: 108, Loss: 2.0734\n",
      "Epoch: 20, Index: 109, Loss: 2.1457\n",
      "Epoch: 20, Index: 110, Loss: 3.2649\n",
      "Epoch: 20, Index: 111, Loss: 4.1682\n",
      "Epoch: 20, Index: 112, Loss: 5.9131\n",
      "Epoch: 20, Index: 113, Loss: 0.8571\n",
      "Epoch: 20, Index: 114, Loss: 3.0315\n",
      "Epoch: 20, Index: 115, Loss: 1.3851\n",
      "Epoch: 20, Index: 116, Loss: 0.6084\n",
      "Epoch: 20, Index: 117, Loss: 1.9974\n",
      "Epoch: 20, Index: 118, Loss: 2.6387\n",
      "Epoch: 20, Index: 119, Loss: 3.0534\n",
      "Epoch: 20, Index: 120, Loss: 0.6184\n",
      "Epoch: 20, Index: 121, Loss: 0.0415\n",
      "Epoch: 20, Index: 122, Loss: 0.3525\n",
      "Epoch: 20, Index: 123, Loss: 0.1889\n",
      "Epoch: 20, Index: 124, Loss: 1.3678\n",
      "Epoch: 20, Index: 125, Loss: 0.5657\n",
      "Epoch: 20, Index: 126, Loss: 0.8966\n",
      "Epoch: 20, Index: 127, Loss: 0.4555\n",
      "Epoch: 20, Index: 128, Loss: 0.2747\n",
      "Epoch: 20, Index: 129, Loss: 0.8587\n",
      "Epoch: 20, Index: 130, Loss: 2.9358\n",
      "Epoch: 20, Index: 131, Loss: 0.1735\n",
      "Epoch: 20, Index: 132, Loss: 0.5348\n",
      "Epoch: 20, Index: 133, Loss: 1.4172\n",
      "Epoch: 20, Index: 134, Loss: 0.4181\n",
      "Epoch: 20, Index: 135, Loss: 8.8432\n",
      "Epoch: 20, Index: 136, Loss: 6.9914\n",
      "Epoch: 20, Index: 137, Loss: 1.2415\n",
      "Epoch: 20, Index: 138, Loss: 2.6299\n",
      "Epoch: 20, Index: 139, Loss: 0.6443\n",
      "Epoch: 20, Index: 140, Loss: 6.0091\n",
      "Epoch: 20, Index: 141, Loss: 0.2663\n",
      "Epoch: 20, Index: 142, Loss: 2.8388\n",
      "Epoch: 20, Index: 143, Loss: 2.8779\n",
      "Epoch: 20, Index: 144, Loss: 0.6513\n",
      "Epoch: 20, Index: 145, Loss: 1.5924\n",
      "Epoch: 20, Index: 146, Loss: 3.0835\n",
      "Epoch: 20, Index: 147, Loss: 0.6532\n",
      "Epoch: 20, Index: 148, Loss: 0.2263\n",
      "Epoch: 20, Index: 149, Loss: 0.0043\n",
      "Epoch: 20, Index: 150, Loss: 0.9464\n",
      "Epoch: 20, Index: 151, Loss: 0.3521\n",
      "Epoch: 20, Index: 152, Loss: 0.1269\n",
      "Epoch: 20, Index: 153, Loss: 1.1428\n",
      "Epoch: 20, Index: 154, Loss: 1.7481\n",
      "Epoch: 20, Index: 155, Loss: 1.4933\n",
      "Epoch: 20, Index: 156, Loss: 1.8668\n",
      "Epoch: 20, Index: 157, Loss: 0.1748\n",
      "Epoch: 20, Index: 158, Loss: 0.5185\n",
      "Epoch: 20, Index: 159, Loss: 1.2547\n",
      "Epoch: 20, Index: 160, Loss: 0.4351\n",
      "Epoch: 20, Index: 161, Loss: 2.9541\n",
      "Epoch: 20, Index: 162, Loss: 0.0061\n",
      "Epoch: 20, Index: 163, Loss: 0.8746\n",
      "Epoch: 20, Index: 164, Loss: 2.1029\n",
      "Epoch: 20, Index: 165, Loss: 3.3696\n",
      "Epoch: 20, Index: 166, Loss: 0.1740\n",
      "Epoch: 20, Index: 167, Loss: 2.9780\n",
      "Epoch: 20, Index: 168, Loss: 0.5932\n",
      "Epoch: 20, Index: 169, Loss: 13.2190\n",
      "Epoch: 20, Index: 170, Loss: 0.8180\n",
      "Epoch: 20, Index: 171, Loss: 2.3751\n",
      "Epoch: 20, Index: 172, Loss: 2.1518\n",
      "Epoch: 20, Index: 173, Loss: 0.4771\n",
      "Epoch: 20, Index: 174, Loss: 0.7113\n",
      "Epoch: 20, Index: 175, Loss: 0.6143\n",
      "Epoch: 20, Index: 176, Loss: 1.5976\n",
      "Epoch: 20, Index: 177, Loss: 0.7743\n",
      "Epoch: 20, Index: 178, Loss: 2.5626\n",
      "Epoch: 20, Index: 179, Loss: 0.3719\n",
      "Epoch: 20, Index: 180, Loss: 5.4673\n",
      "Epoch: 20, Index: 181, Loss: 3.0032\n",
      "Epoch: 20, Index: 182, Loss: 2.2374\n",
      "Epoch: 20, Index: 183, Loss: 1.2958\n",
      "Epoch: 20, Index: 184, Loss: 0.4313\n",
      "Epoch: 20, Index: 185, Loss: 0.6282\n",
      "Epoch: 20, Index: 186, Loss: 0.7307\n",
      "Epoch: 20, Index: 187, Loss: 3.2024\n",
      "Epoch: 20, Index: 188, Loss: 1.5858\n",
      "Epoch: 20, Index: 189, Loss: 12.8633\n",
      "Epoch: 20, Index: 190, Loss: 2.3252\n",
      "Epoch: 20, Index: 191, Loss: 1.4554\n",
      "Epoch: 20, Index: 192, Loss: 0.1236\n",
      "Epoch: 20, Index: 193, Loss: 0.5156\n",
      "Epoch: 20, Index: 194, Loss: 3.8993\n",
      "Epoch: 20, Index: 195, Loss: 0.0523\n",
      "Epoch: 20, Index: 196, Loss: 0.0138\n",
      "Epoch: 20, Index: 197, Loss: 0.9132\n",
      "Epoch: 20, Index: 198, Loss: 0.9163\n",
      "Epoch: 20, Index: 199, Loss: 1.3894\n",
      "Epoch: 20, Index: 200, Loss: 0.1099\n",
      "Epoch: 20, Index: 201, Loss: 0.2710\n",
      "Epoch: 20, Index: 202, Loss: 0.6092\n",
      "Epoch: 20, Index: 203, Loss: 1.1677\n",
      "Epoch: 20, Index: 204, Loss: 0.1959\n",
      "Epoch: 20, Index: 205, Loss: 1.4574\n",
      "Epoch: 20, Index: 206, Loss: 0.9755\n",
      "Epoch: 20, Index: 207, Loss: 1.2853\n",
      "Epoch: 20, Index: 208, Loss: 0.6786\n",
      "Epoch: 20, Index: 209, Loss: 4.5517\n",
      "Epoch: 20, Index: 210, Loss: 0.2849\n",
      "Epoch: 20, Index: 211, Loss: 2.5156\n",
      "Epoch: 20, Index: 212, Loss: 2.0038\n",
      "Epoch: 20, Index: 213, Loss: 0.2800\n",
      "Epoch: 20, Index: 214, Loss: 5.4002\n",
      "Epoch: 20, Index: 215, Loss: 1.9928\n",
      "Epoch: 20, Index: 216, Loss: 0.5024\n",
      "Epoch: 20, Index: 217, Loss: 8.9613\n",
      "Epoch: 20, Index: 218, Loss: 1.6258\n",
      "Epoch: 20, Index: 219, Loss: 1.3831\n",
      "Epoch: 20, Index: 220, Loss: 0.0305\n",
      "Epoch: 20, Index: 221, Loss: 0.3001\n",
      "Epoch: 20, Index: 222, Loss: 1.2874\n",
      "Epoch: 20, Index: 223, Loss: 1.2158\n",
      "Epoch: 20, Index: 224, Loss: 1.1723\n",
      "Epoch: 20, Index: 225, Loss: 0.7495\n",
      "Epoch: 20, Index: 226, Loss: 0.5763\n",
      "Epoch: 20, Index: 227, Loss: 1.7646\n",
      "Epoch: 20, Index: 228, Loss: 0.5925\n",
      "Epoch: 20, Index: 229, Loss: 0.7454\n",
      "Epoch: 20, Index: 230, Loss: 1.2253\n",
      "Epoch: 20, Index: 231, Loss: 0.0839\n",
      "Epoch: 20, Index: 232, Loss: 1.2838\n",
      "Epoch: 20, Index: 233, Loss: 2.2381\n",
      "Epoch: 20, Index: 234, Loss: 2.3380\n",
      "Epoch: 20, Index: 235, Loss: 2.2267\n",
      "Epoch: 20, Index: 236, Loss: 0.9380\n",
      "Epoch: 20, Index: 237, Loss: 2.4502\n",
      "Epoch: 20, Index: 238, Loss: 1.5856\n",
      "Epoch: 20, Index: 239, Loss: 0.0182\n",
      "Epoch: 20, Index: 240, Loss: 2.1293\n",
      "Epoch: 20, Index: 241, Loss: 2.0777\n",
      "Epoch: 20, Index: 242, Loss: 0.3568\n",
      "Epoch: 20, Index: 243, Loss: 2.0892\n",
      "Epoch: 20, Index: 244, Loss: 0.8850\n",
      "Epoch: 20, Index: 245, Loss: 2.9108\n",
      "Epoch: 20, Index: 246, Loss: 0.2506\n",
      "Epoch: 20, Index: 247, Loss: 0.1680\n",
      "Epoch: 20, Index: 248, Loss: 4.6155\n",
      "Epoch: 20, Index: 249, Loss: 0.5108\n",
      "Epoch: 20, Index: 250, Loss: 0.2136\n",
      "Epoch: 20, Index: 251, Loss: 1.1597\n",
      "Epoch: 20, Index: 252, Loss: 0.5526\n",
      "Epoch: 20, Index: 253, Loss: 0.5684\n",
      "Epoch: 20, Index: 254, Loss: 0.7143\n",
      "Epoch: 20, Index: 255, Loss: 0.3712\n",
      "Epoch: 20, Index: 256, Loss: 0.6883\n",
      "Epoch: 20, Index: 257, Loss: 3.9304\n",
      "Epoch: 20, Index: 258, Loss: 2.6522\n",
      "Epoch: 20, Index: 259, Loss: 1.3539\n",
      "Epoch: 20, Index: 260, Loss: 0.9888\n",
      "Epoch: 20, Index: 261, Loss: 1.5046\n",
      "Epoch: 20, Index: 262, Loss: 2.1007\n",
      "Epoch: 20, Index: 263, Loss: 0.0588\n",
      "Epoch: 20, Index: 264, Loss: 0.2009\n",
      "Epoch: 20, Index: 265, Loss: 2.1455\n",
      "Epoch: 20, Index: 266, Loss: 0.5756\n",
      "Epoch: 20, Index: 267, Loss: 1.5479\n",
      "Epoch: 20, Index: 268, Loss: 2.2682\n",
      "Epoch: 20, Index: 269, Loss: 3.4609\n",
      "Epoch: 20, Index: 270, Loss: 2.2614\n",
      "Epoch: 20, Index: 271, Loss: 1.8335\n",
      "Epoch: 20, Index: 272, Loss: 0.8400\n",
      "Epoch: 20, Index: 273, Loss: 0.2091\n",
      "Epoch: 20, Index: 274, Loss: 1.4178\n",
      "Epoch: 20, Index: 275, Loss: 0.5273\n",
      "Epoch: 20, Index: 276, Loss: 0.8178\n",
      "Epoch: 20, Index: 277, Loss: 0.5027\n",
      "Epoch: 20, Index: 278, Loss: 4.5992\n",
      "Epoch: 20, Index: 279, Loss: 1.4435\n",
      "Epoch: 20, Index: 280, Loss: 0.1674\n",
      "Epoch: 20, Index: 281, Loss: 3.9998\n",
      "Epoch: 20, Index: 282, Loss: 2.0487\n",
      "Epoch: 20, Index: 283, Loss: 2.6172\n",
      "Epoch: 20, Index: 284, Loss: 0.6329\n",
      "Epoch: 20, Index: 285, Loss: 1.4544\n",
      "Epoch: 20, Index: 286, Loss: 1.5786\n",
      "Epoch: 20, Index: 287, Loss: 1.6565\n",
      "Epoch: 20, Index: 288, Loss: 0.6958\n",
      "Epoch: 20, Index: 289, Loss: 0.7891\n",
      "Epoch: 20, Index: 290, Loss: 2.2805\n",
      "Epoch: 20, Index: 291, Loss: 0.5380\n",
      "Epoch: 20, Index: 292, Loss: 0.9776\n",
      "Epoch: 20, Index: 293, Loss: 0.3231\n",
      "Epoch: 20, Index: 294, Loss: 1.2781\n",
      "Epoch: 20, Index: 295, Loss: 1.8759\n",
      "Epoch: 20, Index: 296, Loss: 0.1121\n",
      "Epoch: 20, Index: 297, Loss: 11.0693\n",
      "Epoch: 20, Index: 298, Loss: 0.8581\n",
      "Epoch: 20, Index: 299, Loss: 0.6286\n",
      "Epoch: 20, Index: 300, Loss: 2.8287\n",
      "Epoch: 20, Index: 301, Loss: 0.1667\n",
      "Epoch: 20, Index: 302, Loss: 0.1028\n",
      "Epoch: 20, Index: 303, Loss: 1.4937\n",
      "Epoch: 20, Index: 304, Loss: 0.4062\n",
      "Epoch: 20, Index: 305, Loss: 0.4634\n",
      "Epoch: 20, Index: 306, Loss: 3.5719\n",
      "Epoch: 20, Index: 307, Loss: 0.5395\n",
      "Epoch: 20, Index: 308, Loss: 0.6840\n",
      "Epoch: 20, Index: 309, Loss: 2.2517\n",
      "Epoch: 20, Index: 310, Loss: 0.1834\n",
      "Epoch: 20, Index: 311, Loss: 0.6970\n",
      "Epoch: 20, Index: 312, Loss: 7.0021\n",
      "Epoch: 20, Index: 313, Loss: 0.4979\n",
      "Epoch: 20, Index: 314, Loss: 2.1679\n",
      "Epoch: 20, Index: 315, Loss: 2.1314\n",
      "Epoch: 20, Index: 316, Loss: 8.3180\n",
      "Epoch: 20, Index: 317, Loss: 0.0536\n",
      "Epoch: 20, Index: 318, Loss: 1.3561\n",
      "Epoch: 20, Index: 319, Loss: 1.0469\n",
      "Epoch: 20, Index: 320, Loss: 7.2416\n",
      "Epoch: 20, Index: 321, Loss: 0.9055\n",
      "Epoch: 20, Index: 322, Loss: 1.0929\n",
      "Epoch: 20, Index: 323, Loss: 0.2014\n",
      "Epoch: 20, Index: 324, Loss: 1.4394\n",
      "Epoch: 20, Index: 325, Loss: 1.6460\n",
      "Epoch: 20, Index: 326, Loss: 0.1334\n",
      "Epoch: 20, Index: 327, Loss: 0.8131\n",
      "Epoch: 20, Index: 328, Loss: 2.6715\n",
      "Epoch: 20, Index: 329, Loss: 0.0300\n",
      "Epoch: 20, Index: 330, Loss: 1.7370\n",
      "Epoch: 20, Index: 331, Loss: 0.2159\n",
      "Epoch: 20, Index: 332, Loss: 0.4151\n",
      "Epoch: 20, Index: 333, Loss: 1.1423\n",
      "Epoch: 20, Index: 334, Loss: 0.0278\n",
      "Epoch: 20, Index: 335, Loss: 2.1445\n",
      "Epoch: 20, Index: 336, Loss: 1.9038\n",
      "Epoch: 20, Index: 337, Loss: 0.6267\n",
      "Epoch: 20, Index: 338, Loss: 0.5106\n",
      "Epoch: 20, Index: 339, Loss: 2.5855\n",
      "Epoch: 20, Index: 340, Loss: 2.0446\n",
      "Epoch: 20, Index: 341, Loss: 1.7652\n",
      "Epoch: 20, Index: 342, Loss: 5.6981\n",
      "Epoch: 20, Index: 343, Loss: 0.2796\n",
      "Epoch: 20, Index: 344, Loss: 0.7851\n",
      "Epoch: 20, Index: 345, Loss: 1.6391\n",
      "Epoch: 20, Index: 346, Loss: 0.2702\n",
      "Epoch: 20, Index: 347, Loss: 7.0449\n",
      "Epoch: 20, Index: 348, Loss: 2.3861\n",
      "Epoch: 20, Index: 349, Loss: 0.2731\n",
      "Epoch: 20, Index: 350, Loss: 0.0841\n",
      "Epoch: 20, Index: 351, Loss: 0.0492\n",
      "Epoch: 20, Index: 352, Loss: 0.4836\n",
      "Epoch: 20, Index: 353, Loss: 3.1727\n",
      "Epoch: 20, Index: 354, Loss: 0.6878\n",
      "Epoch: 20, Index: 355, Loss: 0.6651\n",
      "Epoch: 20, Index: 356, Loss: 1.4829\n",
      "Epoch: 20, Index: 357, Loss: 1.5772\n",
      "Epoch: 20, Index: 358, Loss: 0.6054\n",
      "Epoch: 20, Index: 359, Loss: 1.5407\n",
      "Epoch: 20, Index: 360, Loss: 3.0735\n",
      "Epoch: 20, Index: 361, Loss: 1.4107\n",
      "Epoch: 20, Index: 362, Loss: 1.3028\n",
      "Epoch: 20, Index: 363, Loss: 1.2208\n",
      "Epoch: 20, Index: 364, Loss: 0.8789\n",
      "Epoch: 20, Index: 365, Loss: 4.3941\n",
      "Epoch: 20, Index: 366, Loss: 0.8449\n",
      "Epoch: 20, Index: 367, Loss: 2.5011\n",
      "Epoch: 20, Index: 368, Loss: 1.0808\n",
      "Epoch: 20, Index: 369, Loss: 0.4276\n",
      "Epoch: 20, Index: 370, Loss: 2.1087\n",
      "Epoch: 20, Index: 371, Loss: 0.3607\n",
      "Epoch: 20, Index: 372, Loss: 0.1291\n",
      "Epoch: 20, Index: 373, Loss: 0.4657\n",
      "Epoch: 20, Index: 374, Loss: 1.9481\n",
      "Epoch: 20, Index: 375, Loss: 4.6016\n",
      "Epoch: 20, Index: 376, Loss: 1.4182\n",
      "Epoch: 20, Index: 377, Loss: 4.6883\n",
      "Epoch: 20, Index: 378, Loss: 4.7124\n",
      "Epoch: 20, Index: 379, Loss: 4.0946\n",
      "Epoch: 20, Index: 380, Loss: 5.3856\n",
      "Epoch: 20, Index: 381, Loss: 0.4709\n",
      "Epoch: 20, Index: 382, Loss: 1.3340\n",
      "Epoch: 20, Index: 383, Loss: 0.3017\n",
      "Epoch: 20, Index: 384, Loss: 3.5638\n",
      "Epoch: 20, Index: 385, Loss: 2.4583\n",
      "Epoch: 20, Index: 386, Loss: 1.0668\n",
      "Epoch: 20, Index: 387, Loss: 1.5718\n",
      "Epoch: 20, Index: 388, Loss: 2.6262\n",
      "Epoch: 20, Index: 389, Loss: 1.9407\n",
      "Epoch: 20, Index: 390, Loss: 0.8611\n",
      "Epoch: 20, Index: 391, Loss: 1.2872\n",
      "Epoch: 20, Index: 392, Loss: 1.3985\n",
      "Epoch: 20, Index: 393, Loss: 1.5521\n",
      "Epoch: 20, Index: 394, Loss: 1.5105\n",
      "Epoch: 20, Index: 395, Loss: 5.5229\n",
      "Epoch: 20, Index: 396, Loss: 0.2954\n",
      "Epoch: 20, Index: 397, Loss: 1.2752\n",
      "Epoch: 20, Index: 398, Loss: 2.9875\n",
      "Epoch: 20, Index: 399, Loss: 0.3000\n",
      "Epoch: 20, Index: 400, Loss: 1.6868\n",
      "Epoch: 20, Index: 401, Loss: 2.6074\n",
      "Epoch: 20, Index: 402, Loss: 2.9640\n",
      "Epoch: 20, Index: 403, Loss: 16.3173\n",
      "Epoch: 20, Index: 404, Loss: 2.2241\n",
      "Epoch: 20, Index: 405, Loss: 5.8054\n",
      "Epoch: 20, Index: 406, Loss: 1.9454\n",
      "Epoch: 20, Index: 407, Loss: 1.0996\n",
      "Epoch: 20, Index: 408, Loss: 1.3590\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model\n",
    "n_total_steps = len(trainloader)\n",
    "avg_loss_over_epochs = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    \n",
    "    for i, (images, labels) in tqdm(enumerate(trainloader), desc=\"Training Progress\", total=len(trainloader)):\n",
    "        # Move images and labels to device\n",
    "        images = torch.stack(images).float()\n",
    "        images = images.permute(1, 0, 2, 3, 4)  # Change shape to [5, 10, 1, 224, 224]\n",
    "        labels = labels.float()\n",
    "\n",
    "        # Forward pass with autograd\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        tqdm.write(f\"Epoch: {epoch+1}, Index: {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        fabric.backward(loss)\n",
    "        optimizer.step()\n",
    "        # Store the loss\n",
    "        train_losses.append(loss.item())\n",
    "    # Store the loss for this epoch\n",
    "    avg_loss_over_epochs.append(sum(train_losses)/len(train_losses))\n",
    "# Plot loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), avg_loss_over_epochs, label='Average Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943767f8e15d424b8d990cfc613888c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss [0.028133157640695572]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461, 1.234619140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461, 1.234619140625, 0.9968831539154053]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461, 1.234619140625, 0.9968831539154053, 5.2996320724487305]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461, 1.234619140625, 0.9968831539154053, 5.2996320724487305, 0.1339855194091797]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461, 1.234619140625, 0.9968831539154053, 5.2996320724487305, 0.1339855194091797, 0.1496105194091797]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461, 1.234619140625, 0.9968831539154053, 5.2996320724487305, 0.1339855194091797, 0.1496105194091797, 0.250244140625]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461, 1.234619140625, 0.9968831539154053, 5.2996320724487305, 0.1339855194091797, 0.1496105194091797, 0.250244140625, 0.6941330432891846]\n",
      "Test loss [0.028133157640695572, 2.6482577323913574, 0.6742894649505615, 1.4357354640960693, 0.47739171981811523, 1.0593831539154053, 0.7801418304443359, 0.6941330432891846, 1.281494140625, 0.6742894649505615, 0.47739171981811523, 0.6742894649505615, 2.447141408920288, 0.3715393543243408, 1.234619140625, 3.0975167751312256, 0.7957668304443359, 4.187744140625, 0.3715393543243408, 1.9854772090911865, 2.6482577323913574, 0.1339855194091797, 4.086111068725586, 0.6681017875671387, 0.7107582092285156, 0.46698546409606934, 1.1131417751312256, 0.3715393543243408, 5.2996320724487305, 1.218994140625, 3.203369140625, 1.218994140625, 0.14439177513122559, 5.620227813720703, 0.250244140625, 0.6742894649505615, 0.1339855194091797, 0.6742894649505615, 1.4357354640960693, 0.250244140625, 0.350726842880249, 6.0818915367126465, 2.3256640434265137, 2.431516408920288, 1.1183605194091797, 0.49301671981811523, 3.4201104640960693, 1.362133264541626, 3.013507843017578, 0.7159769535064697, 0.250244140625, 1.0593831539154053, 0.47739171981811523, 4.101736068725586, 3.9802587032318115, 1.9854772090911865, 0.350726842880249, 0.350726842880249, 0.12876677513122559, 0.6742894649505615, 5.070486068725586, 4.101736068725586, 8.977039337158203, 0.81661057472229, 2.447141408920288, 0.83223557472229, 0.3767580986022949, 0.14439177513122559, 0.03335190936923027, 0.234619140625, 1.281494140625, 0.12876677513122559, 0.1339855194091797, 0.81661057472229, 2.314258575439453, 0.3455080986022949, 10.414892196655273, 1.7201642990112305, 0.12876677513122559, 0.022914407774806023, 0.1339855194091797, 1.6628835201263428, 0.46698546409606934, 13.60238265991211, 0.46698546409606934, 1.39860200881958, 0.022914407774806023, 0.022914407774806023, 22.325361251831055, 4.98225736618042, 0.7801418304443359, 20.220508575439453, 0.3715393543243408, 4.101736068725586, 13.385641098022461, 1.234619140625, 0.9968831539154053, 5.2996320724487305, 0.1339855194091797, 0.1496105194091797, 0.250244140625, 0.6941330432891846, 0.2822265625]\n",
      "Average test loss: 2.1720\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "testloader = fabric.setup_dataloaders(testloader)\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(testloader, desc=\"Testing Progress\"):\n",
    "        images = torch.stack(images).float()\n",
    "        images = images.permute(1, 0, 2, 3, 4)\n",
    "        labels = labels.float()\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        print(\"Test loss\", test_losses)\n",
    "\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "print(f'Average test loss: {avg_test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
