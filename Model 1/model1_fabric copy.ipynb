{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import torchsummary\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('../DataLoader')\n",
    "\n",
    "from dataloader import SunImageDataset\n",
    "\n",
    "from torch.func import stack_module_state\n",
    "from torch.func import vmap\n",
    "\n",
    "from lightning.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "fabric = Fabric(accelerator='cuda', devices=1, precision=\"bf16-mixed\")\n",
    "fabric.launch()\n",
    "print(fabric.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 224*224\n",
    "hidden_size = 166\n",
    "num_epochs = 20\n",
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "dropout = 0.5\n",
    "# dropout = 0.6990787087509548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SunImageDataset(csv_file=\"D:\\\\dataset.csv\", offset=0, transform=transforms.ToTensor())\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# # Without Validation Set\n",
    "# trainset, testset = torch.utils.data.Subset(dataset, range(train_size)), torch.utils.data.Subset(dataset, range(train_size, len(dataset)))\n",
    "# trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "\n",
    "# trainloader = fabric.setup_dataloaders(trainloader)\n",
    "\n",
    "# With Validation Set\n",
    "# Split dataset into training and test sets\n",
    "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, shuffle=False)\n",
    "\n",
    "# Further split training set into training and validation sets\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.25, shuffle=False)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "trainset = torch.utils.data.Subset(dataset, train_indices)\n",
    "valset = torch.utils.data.Subset(dataset, val_indices)\n",
    "testset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "valloader = torch.utils.data.DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "trainloader = fabric.setup_dataloaders(trainloader)\n",
    "valloader = fabric.setup_dataloaders(valloader)\n",
    "\n",
    "\n",
    "# # Get a batch of training data\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "# images = torch.stack(images)\n",
    "# print(images.shape)\n",
    "# print(labels.shape)\n",
    "\n",
    "# print(images)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_FabricModule(\n",
       "  (_forward_module): GmiSwinTransformer(\n",
       "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pretrained_model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers): Sequential(\n",
       "        (0): SwinTransformerStage(\n",
       "          (downsample): Identity()\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.004)\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.013)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.017)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.022)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.026)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.030)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.035)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.039)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.043)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.048)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.052)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.057)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.061)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.065)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.070)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.074)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.078)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.083)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.087)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.096)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Linear(in_features=1024, out_features=166, bias=True)\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Linear(in_features=1660, out_features=166, bias=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=166, out_features=1, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (_original_module): GmiSwinTransformer(\n",
       "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pretrained_model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers): Sequential(\n",
       "        (0): SwinTransformerStage(\n",
       "          (downsample): Identity()\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.004)\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.013)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.017)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.022)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.026)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.030)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.035)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.039)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.043)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.048)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.052)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.057)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.061)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.065)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.070)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.074)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.078)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.083)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.087)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.096)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Linear(in_features=1024, out_features=166, bias=True)\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Linear(in_features=1660, out_features=166, bias=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=166, out_features=1, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class GmiSwinTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(GmiSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Batch normalization for 3 channels\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # Initialize Swin Transformer\n",
    "        self.pretrained_model = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',\n",
    "            pretrained=True,\n",
    "            num_classes=hidden_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size*10, hidden_size),\n",
    "            nn.Dropout(p=dropout),  # Added dropout probability\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Batch should be in format:\n",
    "        {\n",
    "            'images': torch.FloatTensor((10, 1, 224, 224))\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        images = images.reshape(-1, 1, 224, 224)\n",
    "        images = torch.cat([images, images, images], dim=1)\n",
    "        normalized_images = self.bn(images)\n",
    "        features = self.pretrained_model(normalized_images)\n",
    "        image_features = features.view(batch_size, -1)\n",
    "        \n",
    "        output = self.fc(image_features)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GmiSwinTransformer(hidden_size=hidden_size).to(device)\n",
    "model = GmiSwinTransformer(hidden_size=hidden_size)\n",
    "\n",
    "# print(torchsummary.summary(model, (10, 1, 224, 224)))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd4a7644ba643d685d833eafdaea5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Index: 0, Loss: 2.9206\n",
      "Epoch: 1, Index: 1, Loss: 0.2089\n",
      "Epoch: 1, Index: 2, Loss: 62.4478\n",
      "Epoch: 1, Index: 3, Loss: 7.9942\n",
      "Epoch: 1, Index: 4, Loss: 7.0289\n",
      "Epoch: 1, Index: 5, Loss: 5.4620\n",
      "Epoch: 1, Index: 6, Loss: 0.4657\n",
      "Epoch: 1, Index: 7, Loss: 0.7289\n",
      "Epoch: 1, Index: 8, Loss: 2.1132\n",
      "Epoch: 1, Index: 9, Loss: 0.7197\n",
      "Epoch: 1, Index: 10, Loss: 4.2164\n",
      "Epoch: 1, Index: 11, Loss: 5.0883\n",
      "Epoch: 1, Index: 12, Loss: 13.0442\n",
      "Epoch: 1, Index: 13, Loss: 9.5281\n",
      "Epoch: 1, Index: 14, Loss: 6.6037\n",
      "Epoch: 1, Index: 15, Loss: 1.5012\n",
      "Epoch: 1, Index: 16, Loss: 2.2566\n",
      "Epoch: 1, Index: 17, Loss: 2.5071\n",
      "Epoch: 1, Index: 18, Loss: 7.7257\n",
      "Epoch: 1, Index: 19, Loss: 1.7571\n",
      "Epoch: 1, Index: 20, Loss: 0.3626\n",
      "Epoch: 1, Index: 21, Loss: 1.6751\n",
      "Epoch: 1, Index: 22, Loss: 1.9915\n",
      "Epoch: 1, Index: 23, Loss: 0.0313\n",
      "Epoch: 1, Index: 24, Loss: 0.1284\n",
      "Epoch: 1, Index: 25, Loss: 0.7214\n",
      "Epoch: 1, Index: 26, Loss: 3.5963\n",
      "Epoch: 1, Index: 27, Loss: 0.7338\n",
      "Epoch: 1, Index: 28, Loss: 1.7465\n",
      "Epoch: 1, Index: 29, Loss: 4.0029\n",
      "Epoch: 1, Index: 30, Loss: 1.3540\n",
      "Epoch: 1, Index: 31, Loss: 1.6022\n",
      "Epoch: 1, Index: 32, Loss: 0.3853\n",
      "Epoch: 1, Index: 33, Loss: 2.2608\n",
      "Epoch: 1, Index: 34, Loss: 4.6685\n",
      "Epoch: 1, Index: 35, Loss: 0.5158\n",
      "Epoch: 1, Index: 36, Loss: 4.1648\n",
      "Epoch: 1, Index: 37, Loss: 1.7469\n",
      "Epoch: 1, Index: 38, Loss: 3.1417\n",
      "Epoch: 1, Index: 39, Loss: 3.6501\n",
      "Epoch: 1, Index: 40, Loss: 1.3068\n",
      "Epoch: 1, Index: 41, Loss: 0.5799\n",
      "Epoch: 1, Index: 42, Loss: 1.9549\n",
      "Epoch: 1, Index: 43, Loss: 5.6776\n",
      "Epoch: 1, Index: 44, Loss: 1.8635\n",
      "Epoch: 1, Index: 45, Loss: 5.7282\n",
      "Epoch: 1, Index: 46, Loss: 5.5660\n",
      "Epoch: 1, Index: 47, Loss: 0.2595\n",
      "Epoch: 1, Index: 48, Loss: 0.8688\n",
      "Epoch: 1, Index: 49, Loss: 5.0287\n",
      "Epoch: 1, Index: 50, Loss: 0.3561\n",
      "Epoch: 1, Index: 51, Loss: 0.1224\n",
      "Epoch: 1, Index: 52, Loss: 3.6567\n",
      "Epoch: 1, Index: 53, Loss: 12.0260\n",
      "Epoch: 1, Index: 54, Loss: 0.3846\n",
      "Epoch: 1, Index: 55, Loss: 3.5819\n",
      "Epoch: 1, Index: 56, Loss: 1.9822\n",
      "Epoch: 1, Index: 57, Loss: 0.8862\n",
      "Epoch: 1, Index: 58, Loss: 1.0518\n",
      "Epoch: 1, Index: 59, Loss: 10.3883\n",
      "Epoch: 1, Index: 60, Loss: 0.0614\n",
      "Epoch: 1, Index: 61, Loss: 4.3141\n",
      "Epoch: 1, Index: 62, Loss: 5.4074\n",
      "Epoch: 1, Index: 63, Loss: 0.0499\n",
      "Epoch: 1, Index: 64, Loss: 0.9809\n",
      "Epoch: 1, Index: 65, Loss: 0.2268\n",
      "Epoch: 1, Index: 66, Loss: 1.2284\n",
      "Epoch: 1, Index: 67, Loss: 4.1645\n",
      "Epoch: 1, Index: 68, Loss: 0.4368\n",
      "Epoch: 1, Index: 69, Loss: 0.5685\n",
      "Epoch: 1, Index: 70, Loss: 5.7881\n",
      "Epoch: 1, Index: 71, Loss: 1.1323\n",
      "Epoch: 1, Index: 72, Loss: 3.6648\n",
      "Epoch: 1, Index: 73, Loss: 10.1612\n",
      "Epoch: 1, Index: 74, Loss: 0.1164\n",
      "Epoch: 1, Index: 75, Loss: 0.9001\n",
      "Epoch: 1, Index: 76, Loss: 0.9373\n",
      "Epoch: 1, Index: 77, Loss: 2.9245\n",
      "Epoch: 1, Index: 78, Loss: 1.4215\n",
      "Epoch: 1, Index: 79, Loss: 0.1546\n",
      "Epoch: 1, Index: 80, Loss: 14.0879\n",
      "Epoch: 1, Index: 81, Loss: 1.0426\n",
      "Epoch: 1, Index: 82, Loss: 7.5760\n",
      "Epoch: 1, Index: 83, Loss: 7.5959\n",
      "Epoch: 1, Index: 84, Loss: 0.8015\n",
      "Epoch: 1, Index: 85, Loss: 0.5190\n",
      "Epoch: 1, Index: 86, Loss: 9.0799\n",
      "Epoch: 1, Index: 87, Loss: 2.4929\n",
      "Epoch: 1, Index: 88, Loss: 1.6015\n",
      "Epoch: 1, Index: 89, Loss: 2.3795\n",
      "Epoch: 1, Index: 90, Loss: 0.5131\n",
      "Epoch: 1, Index: 91, Loss: 1.8829\n",
      "Epoch: 1, Index: 92, Loss: 2.9848\n",
      "Epoch: 1, Index: 93, Loss: 1.6091\n",
      "Epoch: 1, Index: 94, Loss: 1.4635\n",
      "Epoch: 1, Index: 95, Loss: 7.3633\n",
      "Epoch: 1, Index: 96, Loss: 1.6284\n",
      "Epoch: 1, Index: 97, Loss: 1.6065\n",
      "Epoch: 1, Index: 98, Loss: 0.6910\n",
      "Epoch: 1, Index: 99, Loss: 0.3986\n",
      "Epoch: 1, Index: 100, Loss: 1.8231\n",
      "Epoch: 1, Index: 101, Loss: 3.4054\n",
      "Epoch: 1, Index: 102, Loss: 1.6216\n",
      "Epoch: 1, Index: 103, Loss: 0.9372\n",
      "Epoch: 1, Index: 104, Loss: 2.8071\n",
      "Epoch: 1, Index: 105, Loss: 14.6233\n",
      "Epoch: 1, Index: 106, Loss: 0.0014\n",
      "Epoch: 1, Index: 107, Loss: 1.4144\n",
      "Epoch: 1, Index: 108, Loss: 0.2998\n",
      "Epoch: 1, Index: 109, Loss: 5.9517\n",
      "Epoch: 1, Index: 110, Loss: 2.2802\n",
      "Epoch: 1, Index: 111, Loss: 0.0904\n",
      "Epoch: 1, Index: 112, Loss: 4.9609\n",
      "Epoch: 1, Index: 113, Loss: 0.9762\n",
      "Epoch: 1, Index: 114, Loss: 2.9582\n",
      "Epoch: 1, Index: 115, Loss: 0.9201\n",
      "Epoch: 1, Index: 116, Loss: 1.0042\n",
      "Epoch: 1, Index: 117, Loss: 0.5038\n",
      "Epoch: 1, Index: 118, Loss: 1.7833\n",
      "Epoch: 1, Index: 119, Loss: 2.2223\n",
      "Epoch: 1, Index: 120, Loss: 0.3902\n",
      "Epoch: 1, Index: 121, Loss: 3.8679\n",
      "Epoch: 1, Index: 122, Loss: 1.7773\n",
      "Epoch: 1, Index: 123, Loss: 5.4217\n",
      "Epoch: 1, Index: 124, Loss: 1.1853\n",
      "Epoch: 1, Index: 125, Loss: 3.3445\n",
      "Epoch: 1, Index: 126, Loss: 0.3396\n",
      "Epoch: 1, Index: 127, Loss: 2.0369\n",
      "Epoch: 1, Index: 128, Loss: 1.9118\n",
      "Epoch: 1, Index: 129, Loss: 11.9833\n",
      "Epoch: 1, Index: 130, Loss: 2.4008\n",
      "Epoch: 1, Index: 131, Loss: 0.7656\n",
      "Epoch: 1, Index: 132, Loss: 1.3107\n",
      "Epoch: 1, Index: 133, Loss: 1.5512\n",
      "Epoch: 1, Index: 134, Loss: 3.7777\n",
      "Epoch: 1, Index: 135, Loss: 0.1477\n",
      "Epoch: 1, Index: 136, Loss: 2.8163\n",
      "Epoch: 1, Index: 137, Loss: 4.7216\n",
      "Epoch: 1, Index: 138, Loss: 5.7740\n",
      "Epoch: 1, Index: 139, Loss: 1.9467\n",
      "Epoch: 1, Index: 140, Loss: 1.9482\n",
      "Epoch: 1, Index: 141, Loss: 2.4816\n",
      "Epoch: 1, Index: 142, Loss: 0.1489\n",
      "Epoch: 1, Index: 143, Loss: 4.7397\n",
      "Epoch: 1, Index: 144, Loss: 0.5625\n",
      "Epoch: 1, Index: 145, Loss: 4.2914\n",
      "Epoch: 1, Index: 146, Loss: 0.9085\n",
      "Epoch: 1, Index: 147, Loss: 2.6247\n",
      "Epoch: 1, Index: 148, Loss: 0.1041\n",
      "Epoch: 1, Index: 149, Loss: 11.9798\n",
      "Epoch: 1, Index: 150, Loss: 1.5094\n",
      "Epoch: 1, Index: 151, Loss: 0.1425\n",
      "Epoch: 1, Index: 152, Loss: 10.4715\n",
      "Epoch: 1, Index: 153, Loss: 4.8801\n",
      "Epoch: 1, Index: 154, Loss: 0.3112\n",
      "Epoch: 1, Index: 155, Loss: 0.5374\n",
      "Epoch: 1, Index: 156, Loss: 4.2924\n",
      "Epoch: 1, Index: 157, Loss: 7.1128\n",
      "Epoch: 1, Index: 158, Loss: 1.0824\n",
      "Epoch: 1, Index: 159, Loss: 0.3454\n",
      "Epoch: 1, Index: 160, Loss: 0.9027\n",
      "Epoch: 1, Index: 161, Loss: 10.1832\n",
      "Epoch: 1, Index: 162, Loss: 1.8987\n",
      "Epoch: 1, Index: 163, Loss: 0.6666\n",
      "Epoch: 1, Index: 164, Loss: 1.4438\n",
      "Epoch: 1, Index: 165, Loss: 1.7142\n",
      "Epoch: 1, Index: 166, Loss: 1.6645\n",
      "Epoch: 1, Index: 167, Loss: 0.0002\n",
      "Epoch: 1, Index: 168, Loss: 2.2374\n",
      "Epoch: 1, Index: 169, Loss: 4.2672\n",
      "Epoch: 1, Index: 170, Loss: 0.1126\n",
      "Epoch: 1, Index: 171, Loss: 0.0524\n",
      "Epoch: 1, Index: 172, Loss: 0.9648\n",
      "Epoch: 1, Index: 173, Loss: 4.5508\n",
      "Epoch: 1, Index: 174, Loss: 0.4298\n",
      "Epoch: 1, Index: 175, Loss: 0.4400\n",
      "Epoch: 1, Index: 176, Loss: 0.1205\n",
      "Epoch: 1, Index: 177, Loss: 0.5556\n",
      "Epoch: 1, Index: 178, Loss: 5.9718\n",
      "Epoch: 1, Index: 179, Loss: 4.6174\n",
      "Epoch: 1, Index: 180, Loss: 2.5662\n",
      "Epoch: 1, Index: 181, Loss: 1.8864\n",
      "Epoch: 1, Index: 182, Loss: 0.2598\n",
      "Epoch: 1, Index: 183, Loss: 0.0327\n",
      "Epoch: 1, Index: 184, Loss: 0.5574\n",
      "Epoch: 1, Index: 185, Loss: 0.6373\n",
      "Epoch: 1, Index: 186, Loss: 0.3347\n",
      "Epoch: 1, Index: 187, Loss: 0.0411\n",
      "Epoch: 1, Index: 188, Loss: 0.5226\n",
      "Epoch: 1, Index: 189, Loss: 3.1556\n",
      "Epoch: 1, Index: 190, Loss: 0.1987\n",
      "Epoch: 1, Index: 191, Loss: 1.1133\n",
      "Epoch: 1, Index: 192, Loss: 0.1127\n",
      "Epoch: 1, Index: 193, Loss: 3.0124\n",
      "Epoch: 1, Index: 194, Loss: 2.6733\n",
      "Epoch: 1, Index: 195, Loss: 2.2268\n",
      "Epoch: 1, Index: 196, Loss: 0.0263\n",
      "Epoch: 1, Index: 197, Loss: 2.4357\n",
      "Epoch: 1, Index: 198, Loss: 0.8712\n",
      "Epoch: 1, Index: 199, Loss: 0.9512\n",
      "Epoch: 1, Index: 200, Loss: 7.0141\n",
      "Epoch: 1, Index: 201, Loss: 0.7817\n",
      "Epoch: 1, Index: 202, Loss: 13.4998\n",
      "Epoch: 1, Index: 203, Loss: 1.7302\n",
      "Epoch: 1, Index: 204, Loss: 1.8124\n",
      "Epoch: 1, Index: 205, Loss: 0.2591\n",
      "Epoch: 1, Index: 206, Loss: 1.4730\n",
      "Epoch: 1, Index: 207, Loss: 0.7404\n",
      "Epoch: 1, Index: 208, Loss: 11.1486\n",
      "Epoch: 1, Index: 209, Loss: 0.3557\n",
      "Epoch: 1, Index: 210, Loss: 3.1119\n",
      "Epoch: 1, Index: 211, Loss: 0.8253\n",
      "Epoch: 1, Index: 212, Loss: 2.0241\n",
      "Epoch: 1, Index: 213, Loss: 3.1299\n",
      "Epoch: 1, Index: 214, Loss: 3.3914\n",
      "Epoch: 1, Index: 215, Loss: 2.9542\n",
      "Epoch: 1, Index: 216, Loss: 0.0135\n",
      "Epoch: 1, Index: 217, Loss: 1.0621\n",
      "Epoch: 1, Index: 218, Loss: 2.8362\n",
      "Epoch: 1, Index: 219, Loss: 2.7963\n",
      "Epoch: 1, Index: 220, Loss: 4.9629\n",
      "Epoch: 1, Index: 221, Loss: 6.4625\n",
      "Epoch: 1, Index: 222, Loss: 0.9094\n",
      "Epoch: 1, Index: 223, Loss: 0.0087\n",
      "Epoch: 1, Index: 224, Loss: 1.3214\n",
      "Epoch: 1, Index: 225, Loss: 15.9909\n",
      "Epoch: 1, Index: 226, Loss: 2.0034\n",
      "Epoch: 1, Index: 227, Loss: 3.1657\n",
      "Epoch: 1, Index: 228, Loss: 2.0879\n",
      "Epoch: 1, Index: 229, Loss: 1.7496\n",
      "Epoch: 1, Index: 230, Loss: 4.5535\n",
      "Epoch: 1, Index: 231, Loss: 0.6047\n",
      "Epoch: 1, Index: 232, Loss: 1.5970\n",
      "Epoch: 1, Index: 233, Loss: 5.5434\n",
      "Epoch: 1, Index: 234, Loss: 4.6776\n",
      "Epoch: 1, Index: 235, Loss: 2.1388\n",
      "Epoch: 1, Index: 236, Loss: 0.3276\n",
      "Epoch: 1, Index: 237, Loss: 2.3517\n",
      "Epoch: 1, Index: 238, Loss: 0.2617\n",
      "Epoch: 1, Index: 239, Loss: 0.1607\n",
      "Epoch: 1, Index: 240, Loss: 2.7870\n",
      "Epoch: 1, Index: 241, Loss: 0.4993\n",
      "Epoch: 1, Index: 242, Loss: 1.4453\n",
      "Epoch: 1, Index: 243, Loss: 0.4173\n",
      "Epoch: 1, Index: 244, Loss: 2.2175\n",
      "Epoch: 1, Index: 245, Loss: 0.9909\n",
      "Epoch: 1, Index: 246, Loss: 4.8442\n",
      "Epoch: 1, Index: 247, Loss: 1.0162\n",
      "Epoch: 1, Index: 248, Loss: 0.1130\n",
      "Epoch: 1, Index: 249, Loss: 0.6322\n",
      "Epoch: 1, Index: 250, Loss: 1.6752\n",
      "Epoch: 1, Index: 251, Loss: 1.1017\n",
      "Epoch: 1, Index: 252, Loss: 1.0859\n",
      "Epoch: 1, Index: 253, Loss: 4.1081\n",
      "Epoch: 1, Index: 254, Loss: 7.9350\n",
      "Epoch: 1, Index: 255, Loss: 1.5256\n",
      "Epoch: 1, Index: 256, Loss: 0.1650\n",
      "Epoch: 1, Index: 257, Loss: 1.0055\n",
      "Epoch: 1, Index: 258, Loss: 0.0456\n",
      "Epoch: 1, Index: 259, Loss: 0.6963\n",
      "Epoch: 1, Index: 260, Loss: 3.5756\n",
      "Epoch: 1, Index: 261, Loss: 0.9598\n",
      "Epoch: 1, Index: 262, Loss: 2.2771\n",
      "Epoch: 1, Index: 263, Loss: 7.7381\n",
      "Epoch: 1, Index: 264, Loss: 1.0755\n",
      "Epoch: 1, Index: 265, Loss: 2.6413\n",
      "Epoch: 1, Index: 266, Loss: 4.9245\n",
      "Epoch: 1, Index: 267, Loss: 0.9421\n",
      "Epoch: 1, Index: 268, Loss: 2.3251\n",
      "Epoch: 1, Index: 269, Loss: 0.2225\n",
      "Epoch: 1, Index: 270, Loss: 3.0541\n",
      "Epoch: 1, Index: 271, Loss: 0.7151\n",
      "Epoch: 1, Index: 272, Loss: 0.2331\n",
      "Epoch: 1, Index: 273, Loss: 1.8626\n",
      "Epoch: 1, Index: 274, Loss: 0.5670\n",
      "Epoch: 1, Index: 275, Loss: 0.4358\n",
      "Epoch: 1, Index: 276, Loss: 4.0840\n",
      "Epoch: 1, Index: 277, Loss: 3.2287\n",
      "Epoch: 1, Index: 278, Loss: 21.0658\n",
      "Epoch: 1, Index: 279, Loss: 0.9695\n",
      "Epoch: 1, Index: 280, Loss: 0.6282\n",
      "Epoch: 1, Index: 281, Loss: 1.4157\n",
      "Epoch: 1, Index: 282, Loss: 1.7141\n",
      "Epoch: 1, Index: 283, Loss: 2.7267\n",
      "Epoch: 1, Index: 284, Loss: 3.2969\n",
      "Epoch: 1, Index: 285, Loss: 0.6351\n",
      "Epoch: 1, Index: 286, Loss: 1.0116\n",
      "Epoch: 1, Index: 287, Loss: 6.0298\n",
      "Epoch: 1, Index: 288, Loss: 5.0026\n",
      "Epoch: 1, Index: 289, Loss: 0.5781\n",
      "Epoch: 1, Index: 290, Loss: 2.5247\n",
      "Epoch: 1, Index: 291, Loss: 15.3039\n",
      "Epoch: 1, Index: 292, Loss: 0.0829\n",
      "Epoch: 1, Index: 293, Loss: 0.9312\n",
      "Epoch: 1, Index: 294, Loss: 1.1225\n",
      "Epoch: 1, Index: 295, Loss: 4.1456\n",
      "Epoch: 1, Index: 296, Loss: 3.3060\n",
      "Epoch: 1, Index: 297, Loss: 6.9613\n",
      "Epoch: 1, Index: 298, Loss: 3.3333\n",
      "Epoch: 1, Index: 299, Loss: 0.4283\n",
      "Epoch: 1, Index: 300, Loss: 1.3841\n",
      "Epoch: 1, Index: 301, Loss: 1.1225\n",
      "Epoch: 1, Index: 302, Loss: 4.4489\n",
      "Epoch: 1, Index: 303, Loss: 1.4173\n",
      "Epoch: 1, Index: 304, Loss: 1.8006\n",
      "Epoch: 1, Index: 305, Loss: 3.1024\n",
      "Epoch: 1, Index: 306, Loss: 0.0457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdbd63b544c4e4e8f6e110c51c92ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3577262c154e15bbeefaf0153f64b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Index: 0, Loss: 2.3417\n",
      "Epoch: 2, Index: 1, Loss: 1.6401\n",
      "Epoch: 2, Index: 2, Loss: 2.9971\n",
      "Epoch: 2, Index: 3, Loss: 0.2854\n",
      "Epoch: 2, Index: 4, Loss: 2.7117\n",
      "Epoch: 2, Index: 5, Loss: 0.2868\n",
      "Epoch: 2, Index: 6, Loss: 0.4082\n",
      "Epoch: 2, Index: 7, Loss: 4.3517\n",
      "Epoch: 2, Index: 8, Loss: 1.2372\n",
      "Epoch: 2, Index: 9, Loss: 0.1097\n",
      "Epoch: 2, Index: 10, Loss: 1.5996\n",
      "Epoch: 2, Index: 11, Loss: 1.7532\n",
      "Epoch: 2, Index: 12, Loss: 2.9294\n",
      "Epoch: 2, Index: 13, Loss: 1.5399\n",
      "Epoch: 2, Index: 14, Loss: 3.8688\n",
      "Epoch: 2, Index: 15, Loss: 2.6604\n",
      "Epoch: 2, Index: 16, Loss: 0.1204\n",
      "Epoch: 2, Index: 17, Loss: 0.2820\n",
      "Epoch: 2, Index: 18, Loss: 1.5088\n",
      "Epoch: 2, Index: 19, Loss: 3.9357\n",
      "Epoch: 2, Index: 20, Loss: 1.5324\n",
      "Epoch: 2, Index: 21, Loss: 1.8409\n",
      "Epoch: 2, Index: 22, Loss: 3.8735\n",
      "Epoch: 2, Index: 23, Loss: 2.0441\n",
      "Epoch: 2, Index: 24, Loss: 3.2741\n",
      "Epoch: 2, Index: 25, Loss: 2.0183\n",
      "Epoch: 2, Index: 26, Loss: 5.3922\n",
      "Epoch: 2, Index: 27, Loss: 4.5136\n",
      "Epoch: 2, Index: 28, Loss: 0.4267\n",
      "Epoch: 2, Index: 29, Loss: 1.2704\n",
      "Epoch: 2, Index: 30, Loss: 1.1934\n",
      "Epoch: 2, Index: 31, Loss: 0.9647\n",
      "Epoch: 2, Index: 32, Loss: 0.2232\n",
      "Epoch: 2, Index: 33, Loss: 0.5109\n",
      "Epoch: 2, Index: 34, Loss: 0.3271\n",
      "Epoch: 2, Index: 35, Loss: 0.2446\n",
      "Epoch: 2, Index: 36, Loss: 0.1862\n",
      "Epoch: 2, Index: 37, Loss: 0.0960\n",
      "Epoch: 2, Index: 38, Loss: 2.1753\n",
      "Epoch: 2, Index: 39, Loss: 2.2711\n",
      "Epoch: 2, Index: 40, Loss: 0.2644\n",
      "Epoch: 2, Index: 41, Loss: 4.6161\n",
      "Epoch: 2, Index: 42, Loss: 0.5887\n",
      "Epoch: 2, Index: 43, Loss: 0.7526\n",
      "Epoch: 2, Index: 44, Loss: 0.2568\n",
      "Epoch: 2, Index: 45, Loss: 0.5234\n",
      "Epoch: 2, Index: 46, Loss: 1.2480\n",
      "Epoch: 2, Index: 47, Loss: 2.1414\n",
      "Epoch: 2, Index: 48, Loss: 3.8647\n",
      "Epoch: 2, Index: 49, Loss: 0.4842\n",
      "Epoch: 2, Index: 50, Loss: 2.8332\n",
      "Epoch: 2, Index: 51, Loss: 1.1394\n",
      "Epoch: 2, Index: 52, Loss: 8.5975\n",
      "Epoch: 2, Index: 53, Loss: 4.3354\n",
      "Epoch: 2, Index: 54, Loss: 0.3060\n",
      "Epoch: 2, Index: 55, Loss: 2.2022\n",
      "Epoch: 2, Index: 56, Loss: 0.4450\n",
      "Epoch: 2, Index: 57, Loss: 0.4727\n",
      "Epoch: 2, Index: 58, Loss: 0.5609\n",
      "Epoch: 2, Index: 59, Loss: 0.5686\n",
      "Epoch: 2, Index: 60, Loss: 0.3957\n",
      "Epoch: 2, Index: 61, Loss: 2.5669\n",
      "Epoch: 2, Index: 62, Loss: 0.0673\n",
      "Epoch: 2, Index: 63, Loss: 4.1021\n",
      "Epoch: 2, Index: 64, Loss: 2.1545\n",
      "Epoch: 2, Index: 65, Loss: 1.8583\n",
      "Epoch: 2, Index: 66, Loss: 1.4009\n",
      "Epoch: 2, Index: 67, Loss: 0.2858\n",
      "Epoch: 2, Index: 68, Loss: 2.8829\n",
      "Epoch: 2, Index: 69, Loss: 4.5392\n",
      "Epoch: 2, Index: 70, Loss: 1.1501\n",
      "Epoch: 2, Index: 71, Loss: 1.3010\n",
      "Epoch: 2, Index: 72, Loss: 0.4837\n",
      "Epoch: 2, Index: 73, Loss: 0.4967\n",
      "Epoch: 2, Index: 74, Loss: 0.2882\n",
      "Epoch: 2, Index: 75, Loss: 1.2036\n",
      "Epoch: 2, Index: 76, Loss: 1.6689\n",
      "Epoch: 2, Index: 77, Loss: 1.8797\n",
      "Epoch: 2, Index: 78, Loss: 0.0001\n",
      "Epoch: 2, Index: 79, Loss: 3.1226\n",
      "Epoch: 2, Index: 80, Loss: 2.4959\n",
      "Epoch: 2, Index: 81, Loss: 5.0874\n",
      "Epoch: 2, Index: 82, Loss: 7.9199\n",
      "Epoch: 2, Index: 83, Loss: 0.3425\n",
      "Epoch: 2, Index: 84, Loss: 0.9516\n",
      "Epoch: 2, Index: 85, Loss: 0.8923\n",
      "Epoch: 2, Index: 86, Loss: 1.7207\n",
      "Epoch: 2, Index: 87, Loss: 1.2705\n",
      "Epoch: 2, Index: 88, Loss: 2.0833\n",
      "Epoch: 2, Index: 89, Loss: 0.7497\n",
      "Epoch: 2, Index: 90, Loss: 0.5975\n",
      "Epoch: 2, Index: 91, Loss: 1.5710\n",
      "Epoch: 2, Index: 92, Loss: 0.9197\n",
      "Epoch: 2, Index: 93, Loss: 0.0219\n",
      "Epoch: 2, Index: 94, Loss: 1.1614\n",
      "Epoch: 2, Index: 95, Loss: 2.2056\n",
      "Epoch: 2, Index: 96, Loss: 4.0714\n",
      "Epoch: 2, Index: 97, Loss: 5.6243\n",
      "Epoch: 2, Index: 98, Loss: 0.9568\n",
      "Epoch: 2, Index: 99, Loss: 14.9244\n",
      "Epoch: 2, Index: 100, Loss: 0.7560\n",
      "Epoch: 2, Index: 101, Loss: 2.4500\n",
      "Epoch: 2, Index: 102, Loss: 1.4623\n",
      "Epoch: 2, Index: 103, Loss: 0.0150\n",
      "Epoch: 2, Index: 104, Loss: 0.6316\n",
      "Epoch: 2, Index: 105, Loss: 3.0697\n",
      "Epoch: 2, Index: 106, Loss: 2.8960\n",
      "Epoch: 2, Index: 107, Loss: 5.7043\n",
      "Epoch: 2, Index: 108, Loss: 2.6604\n",
      "Epoch: 2, Index: 109, Loss: 0.4326\n",
      "Epoch: 2, Index: 110, Loss: 4.5223\n",
      "Epoch: 2, Index: 111, Loss: 0.7598\n",
      "Epoch: 2, Index: 112, Loss: 2.5722\n",
      "Epoch: 2, Index: 113, Loss: 3.6965\n",
      "Epoch: 2, Index: 114, Loss: 1.6379\n",
      "Epoch: 2, Index: 115, Loss: 1.6521\n",
      "Epoch: 2, Index: 116, Loss: 3.3050\n",
      "Epoch: 2, Index: 117, Loss: 0.3865\n",
      "Epoch: 2, Index: 118, Loss: 1.7742\n",
      "Epoch: 2, Index: 119, Loss: 2.4663\n",
      "Epoch: 2, Index: 120, Loss: 0.4289\n",
      "Epoch: 2, Index: 121, Loss: 6.8054\n",
      "Epoch: 2, Index: 122, Loss: 2.6991\n",
      "Epoch: 2, Index: 123, Loss: 3.2792\n",
      "Epoch: 2, Index: 124, Loss: 0.0141\n",
      "Epoch: 2, Index: 125, Loss: 14.4115\n",
      "Epoch: 2, Index: 126, Loss: 4.1745\n",
      "Epoch: 2, Index: 127, Loss: 2.5002\n",
      "Epoch: 2, Index: 128, Loss: 1.3427\n",
      "Epoch: 2, Index: 129, Loss: 1.8423\n",
      "Epoch: 2, Index: 130, Loss: 3.7990\n",
      "Epoch: 2, Index: 131, Loss: 0.1589\n",
      "Epoch: 2, Index: 132, Loss: 1.6617\n",
      "Epoch: 2, Index: 133, Loss: 4.1907\n",
      "Epoch: 2, Index: 134, Loss: 3.2504\n",
      "Epoch: 2, Index: 135, Loss: 3.8518\n",
      "Epoch: 2, Index: 136, Loss: 0.9723\n",
      "Epoch: 2, Index: 137, Loss: 2.7649\n",
      "Epoch: 2, Index: 138, Loss: 0.3266\n",
      "Epoch: 2, Index: 139, Loss: 3.1130\n",
      "Epoch: 2, Index: 140, Loss: 0.6474\n",
      "Epoch: 2, Index: 141, Loss: 0.6169\n",
      "Epoch: 2, Index: 142, Loss: 4.7912\n",
      "Epoch: 2, Index: 143, Loss: 1.3802\n",
      "Epoch: 2, Index: 144, Loss: 1.4636\n",
      "Epoch: 2, Index: 145, Loss: 2.0481\n",
      "Epoch: 2, Index: 146, Loss: 0.6252\n",
      "Epoch: 2, Index: 147, Loss: 0.7283\n",
      "Epoch: 2, Index: 148, Loss: 2.9865\n",
      "Epoch: 2, Index: 149, Loss: 2.0996\n",
      "Epoch: 2, Index: 150, Loss: 1.1340\n",
      "Epoch: 2, Index: 151, Loss: 1.5045\n",
      "Epoch: 2, Index: 152, Loss: 2.9088\n",
      "Epoch: 2, Index: 153, Loss: 7.9993\n",
      "Epoch: 2, Index: 154, Loss: 0.1390\n",
      "Epoch: 2, Index: 155, Loss: 0.3939\n",
      "Epoch: 2, Index: 156, Loss: 1.6990\n",
      "Epoch: 2, Index: 157, Loss: 0.0141\n",
      "Epoch: 2, Index: 158, Loss: 5.4879\n",
      "Epoch: 2, Index: 159, Loss: 0.0941\n",
      "Epoch: 2, Index: 160, Loss: 3.0274\n",
      "Epoch: 2, Index: 161, Loss: 1.0718\n",
      "Epoch: 2, Index: 162, Loss: 1.8989\n",
      "Epoch: 2, Index: 163, Loss: 1.9519\n",
      "Epoch: 2, Index: 164, Loss: 4.9700\n",
      "Epoch: 2, Index: 165, Loss: 0.4628\n",
      "Epoch: 2, Index: 166, Loss: 0.9810\n",
      "Epoch: 2, Index: 167, Loss: 2.6440\n",
      "Epoch: 2, Index: 168, Loss: 19.1488\n",
      "Epoch: 2, Index: 169, Loss: 0.7205\n",
      "Epoch: 2, Index: 170, Loss: 1.1924\n",
      "Epoch: 2, Index: 171, Loss: 0.5835\n",
      "Epoch: 2, Index: 172, Loss: 0.8207\n",
      "Epoch: 2, Index: 173, Loss: 1.0365\n",
      "Epoch: 2, Index: 174, Loss: 0.2512\n",
      "Epoch: 2, Index: 175, Loss: 0.1857\n",
      "Epoch: 2, Index: 176, Loss: 0.6598\n",
      "Epoch: 2, Index: 177, Loss: 2.6730\n",
      "Epoch: 2, Index: 178, Loss: 1.8429\n",
      "Epoch: 2, Index: 179, Loss: 0.7985\n",
      "Epoch: 2, Index: 180, Loss: 1.2494\n",
      "Epoch: 2, Index: 181, Loss: 1.1363\n",
      "Epoch: 2, Index: 182, Loss: 1.0651\n",
      "Epoch: 2, Index: 183, Loss: 0.6283\n",
      "Epoch: 2, Index: 184, Loss: 5.0513\n",
      "Epoch: 2, Index: 185, Loss: 0.0887\n",
      "Epoch: 2, Index: 186, Loss: 1.2098\n",
      "Epoch: 2, Index: 187, Loss: 0.8899\n",
      "Epoch: 2, Index: 188, Loss: 2.5136\n",
      "Epoch: 2, Index: 189, Loss: 0.9966\n",
      "Epoch: 2, Index: 190, Loss: 2.6364\n",
      "Epoch: 2, Index: 191, Loss: 0.7279\n",
      "Epoch: 2, Index: 192, Loss: 7.6597\n",
      "Epoch: 2, Index: 193, Loss: 0.4313\n",
      "Epoch: 2, Index: 194, Loss: 1.5146\n",
      "Epoch: 2, Index: 195, Loss: 0.4073\n",
      "Epoch: 2, Index: 196, Loss: 0.0437\n",
      "Epoch: 2, Index: 197, Loss: 1.3427\n",
      "Epoch: 2, Index: 198, Loss: 4.2357\n",
      "Epoch: 2, Index: 199, Loss: 2.3907\n",
      "Epoch: 2, Index: 200, Loss: 0.4180\n",
      "Epoch: 2, Index: 201, Loss: 1.3670\n",
      "Epoch: 2, Index: 202, Loss: 1.3927\n",
      "Epoch: 2, Index: 203, Loss: 0.5409\n",
      "Epoch: 2, Index: 204, Loss: 0.9242\n",
      "Epoch: 2, Index: 205, Loss: 1.5963\n",
      "Epoch: 2, Index: 206, Loss: 4.2948\n",
      "Epoch: 2, Index: 207, Loss: 0.7863\n",
      "Epoch: 2, Index: 208, Loss: 0.8615\n",
      "Epoch: 2, Index: 209, Loss: 8.2411\n",
      "Epoch: 2, Index: 210, Loss: 4.8788\n",
      "Epoch: 2, Index: 211, Loss: 0.8647\n",
      "Epoch: 2, Index: 212, Loss: 0.0388\n",
      "Epoch: 2, Index: 213, Loss: 0.6685\n",
      "Epoch: 2, Index: 214, Loss: 5.7513\n",
      "Epoch: 2, Index: 215, Loss: 7.5702\n",
      "Epoch: 2, Index: 216, Loss: 0.5699\n",
      "Epoch: 2, Index: 217, Loss: 2.5468\n",
      "Epoch: 2, Index: 218, Loss: 2.9404\n",
      "Epoch: 2, Index: 219, Loss: 0.5482\n",
      "Epoch: 2, Index: 220, Loss: 1.4963\n",
      "Epoch: 2, Index: 221, Loss: 4.0899\n",
      "Epoch: 2, Index: 222, Loss: 0.3453\n",
      "Epoch: 2, Index: 223, Loss: 1.5241\n",
      "Epoch: 2, Index: 224, Loss: 1.6478\n",
      "Epoch: 2, Index: 225, Loss: 5.3957\n",
      "Epoch: 2, Index: 226, Loss: 4.5455\n",
      "Epoch: 2, Index: 227, Loss: 0.9101\n",
      "Epoch: 2, Index: 228, Loss: 3.0982\n",
      "Epoch: 2, Index: 229, Loss: 0.1001\n",
      "Epoch: 2, Index: 230, Loss: 4.0441\n",
      "Epoch: 2, Index: 231, Loss: 0.9593\n",
      "Epoch: 2, Index: 232, Loss: 3.0007\n",
      "Epoch: 2, Index: 233, Loss: 4.5418\n",
      "Epoch: 2, Index: 234, Loss: 7.2681\n",
      "Epoch: 2, Index: 235, Loss: 2.6893\n",
      "Epoch: 2, Index: 236, Loss: 3.6271\n",
      "Epoch: 2, Index: 237, Loss: 1.3475\n",
      "Epoch: 2, Index: 238, Loss: 0.1072\n",
      "Epoch: 2, Index: 239, Loss: 0.8623\n",
      "Epoch: 2, Index: 240, Loss: 0.1793\n",
      "Epoch: 2, Index: 241, Loss: 0.0909\n",
      "Epoch: 2, Index: 242, Loss: 5.4895\n",
      "Epoch: 2, Index: 243, Loss: 1.1080\n",
      "Epoch: 2, Index: 244, Loss: 2.7225\n",
      "Epoch: 2, Index: 245, Loss: 1.3159\n",
      "Epoch: 2, Index: 246, Loss: 1.5588\n",
      "Epoch: 2, Index: 247, Loss: 0.7020\n",
      "Epoch: 2, Index: 248, Loss: 1.7617\n",
      "Epoch: 2, Index: 249, Loss: 0.3507\n",
      "Epoch: 2, Index: 250, Loss: 2.2255\n",
      "Epoch: 2, Index: 251, Loss: 1.4380\n",
      "Epoch: 2, Index: 252, Loss: 0.9947\n",
      "Epoch: 2, Index: 253, Loss: 0.7272\n",
      "Epoch: 2, Index: 254, Loss: 2.3043\n",
      "Epoch: 2, Index: 255, Loss: 1.6900\n",
      "Epoch: 2, Index: 256, Loss: 7.1909\n",
      "Epoch: 2, Index: 257, Loss: 2.1513\n",
      "Epoch: 2, Index: 258, Loss: 2.9987\n",
      "Epoch: 2, Index: 259, Loss: 1.3274\n",
      "Epoch: 2, Index: 260, Loss: 0.3477\n",
      "Epoch: 2, Index: 261, Loss: 0.2133\n",
      "Epoch: 2, Index: 262, Loss: 1.3144\n",
      "Epoch: 2, Index: 263, Loss: 1.1989\n",
      "Epoch: 2, Index: 264, Loss: 0.1583\n",
      "Epoch: 2, Index: 265, Loss: 0.0032\n",
      "Epoch: 2, Index: 266, Loss: 0.0862\n",
      "Epoch: 2, Index: 267, Loss: 8.5173\n",
      "Epoch: 2, Index: 268, Loss: 0.7016\n",
      "Epoch: 2, Index: 269, Loss: 0.0276\n",
      "Epoch: 2, Index: 270, Loss: 0.4650\n",
      "Epoch: 2, Index: 271, Loss: 3.4237\n",
      "Epoch: 2, Index: 272, Loss: 4.4434\n",
      "Epoch: 2, Index: 273, Loss: 3.7560\n",
      "Epoch: 2, Index: 274, Loss: 0.4618\n",
      "Epoch: 2, Index: 275, Loss: 3.3578\n",
      "Epoch: 2, Index: 276, Loss: 2.2853\n",
      "Epoch: 2, Index: 277, Loss: 0.4504\n",
      "Epoch: 2, Index: 278, Loss: 6.5000\n",
      "Epoch: 2, Index: 279, Loss: 2.8556\n",
      "Epoch: 2, Index: 280, Loss: 4.5290\n",
      "Epoch: 2, Index: 281, Loss: 0.0194\n",
      "Epoch: 2, Index: 282, Loss: 1.3377\n",
      "Epoch: 2, Index: 283, Loss: 0.0945\n",
      "Epoch: 2, Index: 284, Loss: 0.0094\n",
      "Epoch: 2, Index: 285, Loss: 0.9460\n",
      "Epoch: 2, Index: 286, Loss: 1.9369\n",
      "Epoch: 2, Index: 287, Loss: 1.6290\n",
      "Epoch: 2, Index: 288, Loss: 1.4414\n",
      "Epoch: 2, Index: 289, Loss: 1.0340\n",
      "Epoch: 2, Index: 290, Loss: 5.8458\n",
      "Epoch: 2, Index: 291, Loss: 0.0414\n",
      "Epoch: 2, Index: 292, Loss: 0.4505\n",
      "Epoch: 2, Index: 293, Loss: 0.6690\n",
      "Epoch: 2, Index: 294, Loss: 0.7711\n",
      "Epoch: 2, Index: 295, Loss: 1.3850\n",
      "Epoch: 2, Index: 296, Loss: 0.4768\n",
      "Epoch: 2, Index: 297, Loss: 1.1871\n",
      "Epoch: 2, Index: 298, Loss: 0.2516\n",
      "Epoch: 2, Index: 299, Loss: 1.0467\n",
      "Epoch: 2, Index: 300, Loss: 12.0485\n",
      "Epoch: 2, Index: 301, Loss: 0.7790\n",
      "Epoch: 2, Index: 302, Loss: 5.6581\n",
      "Epoch: 2, Index: 303, Loss: 2.8176\n",
      "Epoch: 2, Index: 304, Loss: 1.5962\n",
      "Epoch: 2, Index: 305, Loss: 1.2328\n",
      "Epoch: 2, Index: 306, Loss: 0.3109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7607af32574580b7569090feb63e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f236c1364a7483487ee06dee17bf92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Index: 0, Loss: 5.1784\n",
      "Epoch: 3, Index: 1, Loss: 0.2419\n",
      "Epoch: 3, Index: 2, Loss: 0.0980\n",
      "Epoch: 3, Index: 3, Loss: 0.7094\n",
      "Epoch: 3, Index: 4, Loss: 0.0331\n",
      "Epoch: 3, Index: 5, Loss: 2.3116\n",
      "Epoch: 3, Index: 6, Loss: 0.3737\n",
      "Epoch: 3, Index: 7, Loss: 1.6654\n",
      "Epoch: 3, Index: 8, Loss: 1.5970\n",
      "Epoch: 3, Index: 9, Loss: 4.6975\n",
      "Epoch: 3, Index: 10, Loss: 0.8623\n",
      "Epoch: 3, Index: 11, Loss: 0.1042\n",
      "Epoch: 3, Index: 12, Loss: 3.1381\n",
      "Epoch: 3, Index: 13, Loss: 0.6921\n",
      "Epoch: 3, Index: 14, Loss: 0.7000\n",
      "Epoch: 3, Index: 15, Loss: 0.7720\n",
      "Epoch: 3, Index: 16, Loss: 3.5910\n",
      "Epoch: 3, Index: 17, Loss: 2.0761\n",
      "Epoch: 3, Index: 18, Loss: 2.0630\n",
      "Epoch: 3, Index: 19, Loss: 0.2008\n",
      "Epoch: 3, Index: 20, Loss: 3.6258\n",
      "Epoch: 3, Index: 21, Loss: 8.7638\n",
      "Epoch: 3, Index: 22, Loss: 2.7267\n",
      "Epoch: 3, Index: 23, Loss: 0.3770\n",
      "Epoch: 3, Index: 24, Loss: 3.1955\n",
      "Epoch: 3, Index: 25, Loss: 2.5672\n",
      "Epoch: 3, Index: 26, Loss: 0.0196\n",
      "Epoch: 3, Index: 27, Loss: 2.4736\n",
      "Epoch: 3, Index: 28, Loss: 5.5082\n",
      "Epoch: 3, Index: 29, Loss: 1.3445\n",
      "Epoch: 3, Index: 30, Loss: 0.9251\n",
      "Epoch: 3, Index: 31, Loss: 0.1728\n",
      "Epoch: 3, Index: 32, Loss: 1.5916\n",
      "Epoch: 3, Index: 33, Loss: 5.3106\n",
      "Epoch: 3, Index: 34, Loss: 1.0547\n",
      "Epoch: 3, Index: 35, Loss: 1.7851\n",
      "Epoch: 3, Index: 36, Loss: 1.8096\n",
      "Epoch: 3, Index: 37, Loss: 1.1323\n",
      "Epoch: 3, Index: 38, Loss: 2.9276\n",
      "Epoch: 3, Index: 39, Loss: 2.1026\n",
      "Epoch: 3, Index: 40, Loss: 9.4944\n",
      "Epoch: 3, Index: 41, Loss: 3.0992\n",
      "Epoch: 3, Index: 42, Loss: 1.0676\n",
      "Epoch: 3, Index: 43, Loss: 1.5649\n",
      "Epoch: 3, Index: 44, Loss: 2.4376\n",
      "Epoch: 3, Index: 45, Loss: 1.2594\n",
      "Epoch: 3, Index: 46, Loss: 1.9606\n",
      "Epoch: 3, Index: 47, Loss: 2.5665\n",
      "Epoch: 3, Index: 48, Loss: 1.6258\n",
      "Epoch: 3, Index: 49, Loss: 0.0700\n",
      "Epoch: 3, Index: 50, Loss: 3.1991\n",
      "Epoch: 3, Index: 51, Loss: 0.4804\n",
      "Epoch: 3, Index: 52, Loss: 2.1809\n",
      "Epoch: 3, Index: 53, Loss: 2.0372\n",
      "Epoch: 3, Index: 54, Loss: 3.8391\n",
      "Epoch: 3, Index: 55, Loss: 7.9804\n",
      "Epoch: 3, Index: 56, Loss: 0.6268\n",
      "Epoch: 3, Index: 57, Loss: 0.0975\n",
      "Epoch: 3, Index: 58, Loss: 0.1584\n",
      "Epoch: 3, Index: 59, Loss: 8.3427\n",
      "Epoch: 3, Index: 60, Loss: 3.4394\n",
      "Epoch: 3, Index: 61, Loss: 0.7360\n",
      "Epoch: 3, Index: 62, Loss: 1.4865\n",
      "Epoch: 3, Index: 63, Loss: 2.0921\n",
      "Epoch: 3, Index: 64, Loss: 1.9224\n",
      "Epoch: 3, Index: 65, Loss: 0.5603\n",
      "Epoch: 3, Index: 66, Loss: 4.3723\n",
      "Epoch: 3, Index: 67, Loss: 0.0743\n",
      "Epoch: 3, Index: 68, Loss: 1.3376\n",
      "Epoch: 3, Index: 69, Loss: 3.2091\n",
      "Epoch: 3, Index: 70, Loss: 1.6347\n",
      "Epoch: 3, Index: 71, Loss: 1.2289\n",
      "Epoch: 3, Index: 72, Loss: 4.0416\n",
      "Epoch: 3, Index: 73, Loss: 1.1672\n",
      "Epoch: 3, Index: 74, Loss: 2.6124\n",
      "Epoch: 3, Index: 75, Loss: 1.8019\n",
      "Epoch: 3, Index: 76, Loss: 1.8561\n",
      "Epoch: 3, Index: 77, Loss: 0.3264\n",
      "Epoch: 3, Index: 78, Loss: 0.6861\n",
      "Epoch: 3, Index: 79, Loss: 2.4944\n",
      "Epoch: 3, Index: 80, Loss: 2.1909\n",
      "Epoch: 3, Index: 81, Loss: 1.2824\n",
      "Epoch: 3, Index: 82, Loss: 3.1959\n",
      "Epoch: 3, Index: 83, Loss: 0.9072\n",
      "Epoch: 3, Index: 84, Loss: 1.2290\n",
      "Epoch: 3, Index: 85, Loss: 1.1723\n",
      "Epoch: 3, Index: 86, Loss: 3.2318\n",
      "Epoch: 3, Index: 87, Loss: 3.9873\n",
      "Epoch: 3, Index: 88, Loss: 0.4396\n",
      "Epoch: 3, Index: 89, Loss: 1.5202\n",
      "Epoch: 3, Index: 90, Loss: 0.0924\n",
      "Epoch: 3, Index: 91, Loss: 12.2107\n",
      "Epoch: 3, Index: 92, Loss: 0.5088\n",
      "Epoch: 3, Index: 93, Loss: 4.0611\n",
      "Epoch: 3, Index: 94, Loss: 16.7445\n",
      "Epoch: 3, Index: 95, Loss: 0.3451\n",
      "Epoch: 3, Index: 96, Loss: 4.4673\n",
      "Epoch: 3, Index: 97, Loss: 0.2083\n",
      "Epoch: 3, Index: 98, Loss: 5.2676\n",
      "Epoch: 3, Index: 99, Loss: 2.9383\n",
      "Epoch: 3, Index: 100, Loss: 4.4270\n",
      "Epoch: 3, Index: 101, Loss: 1.6444\n",
      "Epoch: 3, Index: 102, Loss: 1.5234\n",
      "Epoch: 3, Index: 103, Loss: 3.4826\n",
      "Epoch: 3, Index: 104, Loss: 3.4166\n",
      "Epoch: 3, Index: 105, Loss: 1.8867\n",
      "Epoch: 3, Index: 106, Loss: 0.6463\n",
      "Epoch: 3, Index: 107, Loss: 0.4035\n",
      "Epoch: 3, Index: 108, Loss: 1.1923\n",
      "Epoch: 3, Index: 109, Loss: 2.0756\n",
      "Epoch: 3, Index: 110, Loss: 2.9054\n",
      "Epoch: 3, Index: 111, Loss: 6.6689\n",
      "Epoch: 3, Index: 112, Loss: 8.2535\n",
      "Epoch: 3, Index: 113, Loss: 3.0318\n",
      "Epoch: 3, Index: 114, Loss: 0.3638\n",
      "Epoch: 3, Index: 115, Loss: 0.4693\n",
      "Epoch: 3, Index: 116, Loss: 2.7717\n",
      "Epoch: 3, Index: 117, Loss: 4.2601\n",
      "Epoch: 3, Index: 118, Loss: 0.8173\n",
      "Epoch: 3, Index: 119, Loss: 1.4664\n",
      "Epoch: 3, Index: 120, Loss: 0.4360\n",
      "Epoch: 3, Index: 121, Loss: 1.0447\n",
      "Epoch: 3, Index: 122, Loss: 0.2418\n",
      "Epoch: 3, Index: 123, Loss: 3.1986\n",
      "Epoch: 3, Index: 124, Loss: 0.5994\n",
      "Epoch: 3, Index: 125, Loss: 1.2465\n",
      "Epoch: 3, Index: 126, Loss: 0.0950\n",
      "Epoch: 3, Index: 127, Loss: 8.8940\n",
      "Epoch: 3, Index: 128, Loss: 0.5323\n",
      "Epoch: 3, Index: 129, Loss: 3.4226\n",
      "Epoch: 3, Index: 130, Loss: 0.0017\n",
      "Epoch: 3, Index: 131, Loss: 1.9768\n",
      "Epoch: 3, Index: 132, Loss: 0.2827\n",
      "Epoch: 3, Index: 133, Loss: 1.5757\n",
      "Epoch: 3, Index: 134, Loss: 1.9410\n",
      "Epoch: 3, Index: 135, Loss: 0.0282\n",
      "Epoch: 3, Index: 136, Loss: 0.2239\n",
      "Epoch: 3, Index: 137, Loss: 0.9970\n",
      "Epoch: 3, Index: 138, Loss: 0.9697\n",
      "Epoch: 3, Index: 139, Loss: 5.4990\n",
      "Epoch: 3, Index: 140, Loss: 1.6897\n",
      "Epoch: 3, Index: 141, Loss: 9.7487\n",
      "Epoch: 3, Index: 142, Loss: 0.3307\n",
      "Epoch: 3, Index: 143, Loss: 0.3095\n",
      "Epoch: 3, Index: 144, Loss: 1.7321\n",
      "Epoch: 3, Index: 145, Loss: 0.8405\n",
      "Epoch: 3, Index: 146, Loss: 1.7083\n",
      "Epoch: 3, Index: 147, Loss: 0.5582\n",
      "Epoch: 3, Index: 148, Loss: 0.3821\n",
      "Epoch: 3, Index: 149, Loss: 0.3632\n",
      "Epoch: 3, Index: 150, Loss: 3.0357\n",
      "Epoch: 3, Index: 151, Loss: 0.9399\n",
      "Epoch: 3, Index: 152, Loss: 3.0435\n",
      "Epoch: 3, Index: 153, Loss: 0.1511\n",
      "Epoch: 3, Index: 154, Loss: 1.2477\n",
      "Epoch: 3, Index: 155, Loss: 4.9931\n",
      "Epoch: 3, Index: 156, Loss: 1.8182\n",
      "Epoch: 3, Index: 157, Loss: 1.8836\n",
      "Epoch: 3, Index: 158, Loss: 0.3434\n",
      "Epoch: 3, Index: 159, Loss: 4.3957\n",
      "Epoch: 3, Index: 160, Loss: 1.8480\n",
      "Epoch: 3, Index: 161, Loss: 3.6258\n",
      "Epoch: 3, Index: 162, Loss: 0.3645\n",
      "Epoch: 3, Index: 163, Loss: 7.9635\n",
      "Epoch: 3, Index: 164, Loss: 0.5493\n",
      "Epoch: 3, Index: 165, Loss: 2.0657\n",
      "Epoch: 3, Index: 166, Loss: 1.3316\n",
      "Epoch: 3, Index: 167, Loss: 1.1986\n",
      "Epoch: 3, Index: 168, Loss: 0.8194\n",
      "Epoch: 3, Index: 169, Loss: 0.9827\n",
      "Epoch: 3, Index: 170, Loss: 1.7277\n",
      "Epoch: 3, Index: 171, Loss: 1.2324\n",
      "Epoch: 3, Index: 172, Loss: 1.8883\n",
      "Epoch: 3, Index: 173, Loss: 0.1779\n",
      "Epoch: 3, Index: 174, Loss: 1.4706\n",
      "Epoch: 3, Index: 175, Loss: 0.2268\n",
      "Epoch: 3, Index: 176, Loss: 1.3781\n",
      "Epoch: 3, Index: 177, Loss: 0.3365\n",
      "Epoch: 3, Index: 178, Loss: 0.0984\n",
      "Epoch: 3, Index: 179, Loss: 0.2209\n",
      "Epoch: 3, Index: 180, Loss: 0.4901\n",
      "Epoch: 3, Index: 181, Loss: 0.1204\n",
      "Epoch: 3, Index: 182, Loss: 1.0580\n",
      "Epoch: 3, Index: 183, Loss: 1.4706\n",
      "Epoch: 3, Index: 184, Loss: 1.6128\n",
      "Epoch: 3, Index: 185, Loss: 4.8286\n",
      "Epoch: 3, Index: 186, Loss: 0.8894\n",
      "Epoch: 3, Index: 187, Loss: 0.3028\n",
      "Epoch: 3, Index: 188, Loss: 0.6779\n",
      "Epoch: 3, Index: 189, Loss: 0.7564\n",
      "Epoch: 3, Index: 190, Loss: 0.1076\n",
      "Epoch: 3, Index: 191, Loss: 8.0613\n",
      "Epoch: 3, Index: 192, Loss: 1.5669\n",
      "Epoch: 3, Index: 193, Loss: 3.6494\n",
      "Epoch: 3, Index: 194, Loss: 2.2165\n",
      "Epoch: 3, Index: 195, Loss: 3.3847\n",
      "Epoch: 3, Index: 196, Loss: 2.8748\n",
      "Epoch: 3, Index: 197, Loss: 1.9806\n",
      "Epoch: 3, Index: 198, Loss: 1.0596\n",
      "Epoch: 3, Index: 199, Loss: 0.0506\n",
      "Epoch: 3, Index: 200, Loss: 0.1886\n",
      "Epoch: 3, Index: 201, Loss: 3.7477\n",
      "Epoch: 3, Index: 202, Loss: 0.7779\n",
      "Epoch: 3, Index: 203, Loss: 3.8164\n",
      "Epoch: 3, Index: 204, Loss: 1.1291\n",
      "Epoch: 3, Index: 205, Loss: 2.1261\n",
      "Epoch: 3, Index: 206, Loss: 2.3346\n",
      "Epoch: 3, Index: 207, Loss: 0.1522\n",
      "Epoch: 3, Index: 208, Loss: 0.4302\n",
      "Epoch: 3, Index: 209, Loss: 1.4855\n",
      "Epoch: 3, Index: 210, Loss: 10.6685\n",
      "Epoch: 3, Index: 211, Loss: 0.6385\n",
      "Epoch: 3, Index: 212, Loss: 0.6256\n",
      "Epoch: 3, Index: 213, Loss: 4.3901\n",
      "Epoch: 3, Index: 214, Loss: 12.8980\n",
      "Epoch: 3, Index: 215, Loss: 4.7590\n",
      "Epoch: 3, Index: 216, Loss: 0.3406\n",
      "Epoch: 3, Index: 217, Loss: 2.1411\n",
      "Epoch: 3, Index: 218, Loss: 1.8658\n",
      "Epoch: 3, Index: 219, Loss: 1.8486\n",
      "Epoch: 3, Index: 220, Loss: 0.7184\n",
      "Epoch: 3, Index: 221, Loss: 0.3426\n",
      "Epoch: 3, Index: 222, Loss: 0.0244\n",
      "Epoch: 3, Index: 223, Loss: 0.3171\n",
      "Epoch: 3, Index: 224, Loss: 1.2970\n",
      "Epoch: 3, Index: 225, Loss: 1.7071\n",
      "Epoch: 3, Index: 226, Loss: 1.6010\n",
      "Epoch: 3, Index: 227, Loss: 3.1234\n",
      "Epoch: 3, Index: 228, Loss: 2.4002\n",
      "Epoch: 3, Index: 229, Loss: 0.4020\n",
      "Epoch: 3, Index: 230, Loss: 0.5285\n",
      "Epoch: 3, Index: 231, Loss: 2.2167\n",
      "Epoch: 3, Index: 232, Loss: 0.1573\n",
      "Epoch: 3, Index: 233, Loss: 0.1835\n",
      "Epoch: 3, Index: 234, Loss: 2.4592\n",
      "Epoch: 3, Index: 235, Loss: 0.5777\n",
      "Epoch: 3, Index: 236, Loss: 0.5241\n",
      "Epoch: 3, Index: 237, Loss: 0.9800\n",
      "Epoch: 3, Index: 238, Loss: 1.5380\n",
      "Epoch: 3, Index: 239, Loss: 3.1399\n",
      "Epoch: 3, Index: 240, Loss: 1.7763\n",
      "Epoch: 3, Index: 241, Loss: 4.8105\n",
      "Epoch: 3, Index: 242, Loss: 1.1852\n",
      "Epoch: 3, Index: 243, Loss: 2.6408\n",
      "Epoch: 3, Index: 244, Loss: 1.5059\n",
      "Epoch: 3, Index: 245, Loss: 0.3346\n",
      "Epoch: 3, Index: 246, Loss: 1.7048\n",
      "Epoch: 3, Index: 247, Loss: 0.7835\n",
      "Epoch: 3, Index: 248, Loss: 2.3932\n",
      "Epoch: 3, Index: 249, Loss: 0.4492\n",
      "Epoch: 3, Index: 250, Loss: 3.1353\n",
      "Epoch: 3, Index: 251, Loss: 1.2789\n",
      "Epoch: 3, Index: 252, Loss: 1.7230\n",
      "Epoch: 3, Index: 253, Loss: 1.4465\n",
      "Epoch: 3, Index: 254, Loss: 1.3313\n",
      "Epoch: 3, Index: 255, Loss: 1.5035\n",
      "Epoch: 3, Index: 256, Loss: 1.0833\n",
      "Epoch: 3, Index: 257, Loss: 4.7727\n",
      "Epoch: 3, Index: 258, Loss: 2.3942\n",
      "Epoch: 3, Index: 259, Loss: 1.2965\n",
      "Epoch: 3, Index: 260, Loss: 6.7773\n",
      "Epoch: 3, Index: 261, Loss: 2.3346\n",
      "Epoch: 3, Index: 262, Loss: 13.3026\n",
      "Epoch: 3, Index: 263, Loss: 0.4731\n",
      "Epoch: 3, Index: 264, Loss: 2.0462\n",
      "Epoch: 3, Index: 265, Loss: 1.8687\n",
      "Epoch: 3, Index: 266, Loss: 0.6419\n",
      "Epoch: 3, Index: 267, Loss: 2.1015\n",
      "Epoch: 3, Index: 268, Loss: 0.7388\n",
      "Epoch: 3, Index: 269, Loss: 3.4296\n",
      "Epoch: 3, Index: 270, Loss: 3.3622\n",
      "Epoch: 3, Index: 271, Loss: 2.9388\n",
      "Epoch: 3, Index: 272, Loss: 5.3020\n",
      "Epoch: 3, Index: 273, Loss: 1.5132\n",
      "Epoch: 3, Index: 274, Loss: 2.1766\n",
      "Epoch: 3, Index: 275, Loss: 4.3848\n",
      "Epoch: 3, Index: 276, Loss: 3.1294\n",
      "Epoch: 3, Index: 277, Loss: 2.9500\n",
      "Epoch: 3, Index: 278, Loss: 2.1583\n",
      "Epoch: 3, Index: 279, Loss: 2.9540\n",
      "Epoch: 3, Index: 280, Loss: 0.0621\n",
      "Epoch: 3, Index: 281, Loss: 0.2426\n",
      "Epoch: 3, Index: 282, Loss: 0.0714\n",
      "Epoch: 3, Index: 283, Loss: 7.6710\n",
      "Epoch: 3, Index: 284, Loss: 5.4447\n",
      "Epoch: 3, Index: 285, Loss: 1.8575\n",
      "Epoch: 3, Index: 286, Loss: 3.3924\n",
      "Epoch: 3, Index: 287, Loss: 1.1399\n",
      "Epoch: 3, Index: 288, Loss: 0.4591\n",
      "Epoch: 3, Index: 289, Loss: 0.9919\n",
      "Epoch: 3, Index: 290, Loss: 18.0936\n",
      "Epoch: 3, Index: 291, Loss: 3.5182\n",
      "Epoch: 3, Index: 292, Loss: 2.1638\n",
      "Epoch: 3, Index: 293, Loss: 0.8099\n",
      "Epoch: 3, Index: 294, Loss: 0.5524\n",
      "Epoch: 3, Index: 295, Loss: 3.2615\n",
      "Epoch: 3, Index: 296, Loss: 1.7480\n",
      "Epoch: 3, Index: 297, Loss: 5.2887\n",
      "Epoch: 3, Index: 298, Loss: 0.9134\n",
      "Epoch: 3, Index: 299, Loss: 0.4251\n",
      "Epoch: 3, Index: 300, Loss: 0.6458\n",
      "Epoch: 3, Index: 301, Loss: 0.5871\n",
      "Epoch: 3, Index: 302, Loss: 6.7848\n",
      "Epoch: 3, Index: 303, Loss: 1.4090\n",
      "Epoch: 3, Index: 304, Loss: 1.5025\n",
      "Epoch: 3, Index: 305, Loss: 0.3983\n",
      "Epoch: 3, Index: 306, Loss: 0.1821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90322cbefe724661a7afa053ea032a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0e44915be3482890fec1a05c6ad7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Index: 0, Loss: 1.6948\n",
      "Epoch: 4, Index: 1, Loss: 3.7188\n",
      "Epoch: 4, Index: 2, Loss: 0.0054\n",
      "Epoch: 4, Index: 3, Loss: 0.0287\n",
      "Epoch: 4, Index: 4, Loss: 1.6843\n",
      "Epoch: 4, Index: 5, Loss: 4.7751\n",
      "Epoch: 4, Index: 6, Loss: 6.6367\n",
      "Epoch: 4, Index: 7, Loss: 2.6002\n",
      "Epoch: 4, Index: 8, Loss: 2.0439\n",
      "Epoch: 4, Index: 9, Loss: 3.2442\n",
      "Epoch: 4, Index: 10, Loss: 1.2725\n",
      "Epoch: 4, Index: 11, Loss: 3.6556\n",
      "Epoch: 4, Index: 12, Loss: 8.8363\n",
      "Epoch: 4, Index: 13, Loss: 0.4889\n",
      "Epoch: 4, Index: 14, Loss: 0.9268\n",
      "Epoch: 4, Index: 15, Loss: 8.2804\n",
      "Epoch: 4, Index: 16, Loss: 0.4838\n",
      "Epoch: 4, Index: 17, Loss: 0.6868\n",
      "Epoch: 4, Index: 18, Loss: 5.1850\n",
      "Epoch: 4, Index: 19, Loss: 0.0539\n",
      "Epoch: 4, Index: 20, Loss: 4.1885\n",
      "Epoch: 4, Index: 21, Loss: 6.2979\n",
      "Epoch: 4, Index: 22, Loss: 1.1214\n",
      "Epoch: 4, Index: 23, Loss: 2.3709\n",
      "Epoch: 4, Index: 24, Loss: 5.1689\n",
      "Epoch: 4, Index: 25, Loss: 0.6014\n",
      "Epoch: 4, Index: 26, Loss: 0.0369\n",
      "Epoch: 4, Index: 27, Loss: 0.8073\n",
      "Epoch: 4, Index: 28, Loss: 3.3417\n",
      "Epoch: 4, Index: 29, Loss: 6.0719\n",
      "Epoch: 4, Index: 30, Loss: 2.8789\n",
      "Epoch: 4, Index: 31, Loss: 2.7004\n",
      "Epoch: 4, Index: 32, Loss: 1.2984\n",
      "Epoch: 4, Index: 33, Loss: 0.9298\n",
      "Epoch: 4, Index: 34, Loss: 5.9720\n",
      "Epoch: 4, Index: 35, Loss: 0.3333\n",
      "Epoch: 4, Index: 36, Loss: 1.1531\n",
      "Epoch: 4, Index: 37, Loss: 1.6743\n",
      "Epoch: 4, Index: 38, Loss: 0.1101\n",
      "Epoch: 4, Index: 39, Loss: 2.4993\n",
      "Epoch: 4, Index: 40, Loss: 0.1336\n",
      "Epoch: 4, Index: 41, Loss: 2.2089\n",
      "Epoch: 4, Index: 42, Loss: 2.2836\n",
      "Epoch: 4, Index: 43, Loss: 0.8278\n",
      "Epoch: 4, Index: 44, Loss: 0.5163\n",
      "Epoch: 4, Index: 45, Loss: 3.4935\n",
      "Epoch: 4, Index: 46, Loss: 0.3819\n",
      "Epoch: 4, Index: 47, Loss: 0.1274\n",
      "Epoch: 4, Index: 48, Loss: 3.1774\n",
      "Epoch: 4, Index: 49, Loss: 0.5748\n",
      "Epoch: 4, Index: 50, Loss: 0.3662\n",
      "Epoch: 4, Index: 51, Loss: 3.3596\n",
      "Epoch: 4, Index: 52, Loss: 11.5796\n",
      "Epoch: 4, Index: 53, Loss: 0.9438\n",
      "Epoch: 4, Index: 54, Loss: 0.9415\n",
      "Epoch: 4, Index: 55, Loss: 3.5591\n",
      "Epoch: 4, Index: 56, Loss: 1.3721\n",
      "Epoch: 4, Index: 57, Loss: 5.3117\n",
      "Epoch: 4, Index: 58, Loss: 0.4923\n",
      "Epoch: 4, Index: 59, Loss: 0.6458\n",
      "Epoch: 4, Index: 60, Loss: 1.4224\n",
      "Epoch: 4, Index: 61, Loss: 1.0574\n",
      "Epoch: 4, Index: 62, Loss: 1.9037\n",
      "Epoch: 4, Index: 63, Loss: 0.5699\n",
      "Epoch: 4, Index: 64, Loss: 2.5701\n",
      "Epoch: 4, Index: 65, Loss: 2.7949\n",
      "Epoch: 4, Index: 66, Loss: 3.4412\n",
      "Epoch: 4, Index: 67, Loss: 2.6256\n",
      "Epoch: 4, Index: 68, Loss: 0.1669\n",
      "Epoch: 4, Index: 69, Loss: 3.8395\n",
      "Epoch: 4, Index: 70, Loss: 2.4346\n",
      "Epoch: 4, Index: 71, Loss: 1.0648\n",
      "Epoch: 4, Index: 72, Loss: 0.1481\n",
      "Epoch: 4, Index: 73, Loss: 4.3948\n",
      "Epoch: 4, Index: 74, Loss: 1.1898\n",
      "Epoch: 4, Index: 75, Loss: 2.3848\n",
      "Epoch: 4, Index: 76, Loss: 0.8993\n",
      "Epoch: 4, Index: 77, Loss: 0.9900\n",
      "Epoch: 4, Index: 78, Loss: 0.5171\n",
      "Epoch: 4, Index: 79, Loss: 2.2939\n",
      "Epoch: 4, Index: 80, Loss: 0.3520\n",
      "Epoch: 4, Index: 81, Loss: 0.4912\n",
      "Epoch: 4, Index: 82, Loss: 6.1663\n",
      "Epoch: 4, Index: 83, Loss: 0.2015\n",
      "Epoch: 4, Index: 84, Loss: 1.6095\n",
      "Epoch: 4, Index: 85, Loss: 2.8144\n",
      "Epoch: 4, Index: 86, Loss: 7.3632\n",
      "Epoch: 4, Index: 87, Loss: 2.1736\n",
      "Epoch: 4, Index: 88, Loss: 0.6115\n",
      "Epoch: 4, Index: 89, Loss: 0.8694\n",
      "Epoch: 4, Index: 90, Loss: 2.3235\n",
      "Epoch: 4, Index: 91, Loss: 0.1893\n",
      "Epoch: 4, Index: 92, Loss: 0.1409\n",
      "Epoch: 4, Index: 93, Loss: 0.5516\n",
      "Epoch: 4, Index: 94, Loss: 0.4542\n",
      "Epoch: 4, Index: 95, Loss: 3.0627\n",
      "Epoch: 4, Index: 96, Loss: 3.7240\n",
      "Epoch: 4, Index: 97, Loss: 10.0293\n",
      "Epoch: 4, Index: 98, Loss: 3.9683\n",
      "Epoch: 4, Index: 99, Loss: 0.5298\n",
      "Epoch: 4, Index: 100, Loss: 0.3884\n",
      "Epoch: 4, Index: 101, Loss: 0.7581\n",
      "Epoch: 4, Index: 102, Loss: 2.8048\n",
      "Epoch: 4, Index: 103, Loss: 1.3551\n",
      "Epoch: 4, Index: 104, Loss: 1.7941\n",
      "Epoch: 4, Index: 105, Loss: 4.7247\n",
      "Epoch: 4, Index: 106, Loss: 0.0280\n",
      "Epoch: 4, Index: 107, Loss: 0.2775\n",
      "Epoch: 4, Index: 108, Loss: 4.0486\n",
      "Epoch: 4, Index: 109, Loss: 0.3464\n",
      "Epoch: 4, Index: 110, Loss: 0.4336\n",
      "Epoch: 4, Index: 111, Loss: 1.7782\n",
      "Epoch: 4, Index: 112, Loss: 4.0582\n",
      "Epoch: 4, Index: 113, Loss: 0.5743\n",
      "Epoch: 4, Index: 114, Loss: 0.5593\n",
      "Epoch: 4, Index: 115, Loss: 7.7100\n",
      "Epoch: 4, Index: 116, Loss: 0.4014\n",
      "Epoch: 4, Index: 117, Loss: 0.2756\n",
      "Epoch: 4, Index: 118, Loss: 0.9277\n",
      "Epoch: 4, Index: 119, Loss: 0.7939\n",
      "Epoch: 4, Index: 120, Loss: 0.5853\n",
      "Epoch: 4, Index: 121, Loss: 0.9136\n",
      "Epoch: 4, Index: 122, Loss: 0.7878\n",
      "Epoch: 4, Index: 123, Loss: 8.3492\n",
      "Epoch: 4, Index: 124, Loss: 5.1907\n",
      "Epoch: 4, Index: 125, Loss: 0.6294\n",
      "Epoch: 4, Index: 126, Loss: 2.6782\n",
      "Epoch: 4, Index: 127, Loss: 7.0986\n",
      "Epoch: 4, Index: 128, Loss: 0.3457\n",
      "Epoch: 4, Index: 129, Loss: 3.7384\n",
      "Epoch: 4, Index: 130, Loss: 0.2927\n",
      "Epoch: 4, Index: 131, Loss: 0.6269\n",
      "Epoch: 4, Index: 132, Loss: 9.1437\n",
      "Epoch: 4, Index: 133, Loss: 3.1438\n",
      "Epoch: 4, Index: 134, Loss: 2.2019\n",
      "Epoch: 4, Index: 135, Loss: 4.2581\n",
      "Epoch: 4, Index: 136, Loss: 3.6300\n",
      "Epoch: 4, Index: 137, Loss: 0.6898\n",
      "Epoch: 4, Index: 138, Loss: 0.2505\n",
      "Epoch: 4, Index: 139, Loss: 0.2313\n",
      "Epoch: 4, Index: 140, Loss: 0.7881\n",
      "Epoch: 4, Index: 141, Loss: 3.5034\n",
      "Epoch: 4, Index: 142, Loss: 5.0865\n",
      "Epoch: 4, Index: 143, Loss: 0.6274\n",
      "Epoch: 4, Index: 144, Loss: 0.1501\n",
      "Epoch: 4, Index: 145, Loss: 0.5823\n",
      "Epoch: 4, Index: 146, Loss: 6.7243\n",
      "Epoch: 4, Index: 147, Loss: 1.1994\n",
      "Epoch: 4, Index: 148, Loss: 3.9977\n",
      "Epoch: 4, Index: 149, Loss: 0.4994\n",
      "Epoch: 4, Index: 150, Loss: 0.5317\n",
      "Epoch: 4, Index: 151, Loss: 1.2808\n",
      "Epoch: 4, Index: 152, Loss: 2.5047\n",
      "Epoch: 4, Index: 153, Loss: 1.7142\n",
      "Epoch: 4, Index: 154, Loss: 1.8247\n",
      "Epoch: 4, Index: 155, Loss: 2.0371\n",
      "Epoch: 4, Index: 156, Loss: 0.3277\n",
      "Epoch: 4, Index: 157, Loss: 13.4426\n",
      "Epoch: 4, Index: 158, Loss: 2.5111\n",
      "Epoch: 4, Index: 159, Loss: 0.7561\n",
      "Epoch: 4, Index: 160, Loss: 1.6357\n",
      "Epoch: 4, Index: 161, Loss: 2.5195\n",
      "Epoch: 4, Index: 162, Loss: 3.3033\n",
      "Epoch: 4, Index: 163, Loss: 1.3785\n",
      "Epoch: 4, Index: 164, Loss: 0.5853\n",
      "Epoch: 4, Index: 165, Loss: 1.8894\n",
      "Epoch: 4, Index: 166, Loss: 7.9737\n",
      "Epoch: 4, Index: 167, Loss: 0.8722\n",
      "Epoch: 4, Index: 168, Loss: 4.1031\n",
      "Epoch: 4, Index: 169, Loss: 0.0236\n",
      "Epoch: 4, Index: 170, Loss: 2.0196\n",
      "Epoch: 4, Index: 171, Loss: 0.2627\n",
      "Epoch: 4, Index: 172, Loss: 0.9095\n",
      "Epoch: 4, Index: 173, Loss: 2.1578\n",
      "Epoch: 4, Index: 174, Loss: 1.8457\n",
      "Epoch: 4, Index: 175, Loss: 1.6119\n",
      "Epoch: 4, Index: 176, Loss: 0.2189\n",
      "Epoch: 4, Index: 177, Loss: 2.5423\n",
      "Epoch: 4, Index: 178, Loss: 2.8800\n",
      "Epoch: 4, Index: 179, Loss: 0.3377\n",
      "Epoch: 4, Index: 180, Loss: 0.2740\n",
      "Epoch: 4, Index: 181, Loss: 1.3410\n",
      "Epoch: 4, Index: 182, Loss: 3.3848\n",
      "Epoch: 4, Index: 183, Loss: 1.4504\n",
      "Epoch: 4, Index: 184, Loss: 6.2423\n",
      "Epoch: 4, Index: 185, Loss: 1.8114\n",
      "Epoch: 4, Index: 186, Loss: 0.1070\n",
      "Epoch: 4, Index: 187, Loss: 0.7471\n",
      "Epoch: 4, Index: 188, Loss: 3.3716\n",
      "Epoch: 4, Index: 189, Loss: 0.1458\n",
      "Epoch: 4, Index: 190, Loss: 0.4799\n",
      "Epoch: 4, Index: 191, Loss: 0.4363\n",
      "Epoch: 4, Index: 192, Loss: 1.0726\n",
      "Epoch: 4, Index: 193, Loss: 3.6808\n",
      "Epoch: 4, Index: 194, Loss: 2.4382\n",
      "Epoch: 4, Index: 195, Loss: 0.6350\n",
      "Epoch: 4, Index: 196, Loss: 1.3257\n",
      "Epoch: 4, Index: 197, Loss: 1.2737\n",
      "Epoch: 4, Index: 198, Loss: 1.3066\n",
      "Epoch: 4, Index: 199, Loss: 2.6994\n",
      "Epoch: 4, Index: 200, Loss: 1.4801\n",
      "Epoch: 4, Index: 201, Loss: 3.1720\n",
      "Epoch: 4, Index: 202, Loss: 2.0385\n",
      "Epoch: 4, Index: 203, Loss: 2.6830\n",
      "Epoch: 4, Index: 204, Loss: 2.8962\n",
      "Epoch: 4, Index: 205, Loss: 0.7894\n",
      "Epoch: 4, Index: 206, Loss: 0.1527\n",
      "Epoch: 4, Index: 207, Loss: 2.8098\n",
      "Epoch: 4, Index: 208, Loss: 0.4812\n",
      "Epoch: 4, Index: 209, Loss: 1.4288\n",
      "Epoch: 4, Index: 210, Loss: 1.5690\n",
      "Epoch: 4, Index: 211, Loss: 7.9986\n",
      "Epoch: 4, Index: 212, Loss: 0.0556\n",
      "Epoch: 4, Index: 213, Loss: 0.6716\n",
      "Epoch: 4, Index: 214, Loss: 5.0072\n",
      "Epoch: 4, Index: 215, Loss: 0.0769\n",
      "Epoch: 4, Index: 216, Loss: 2.8251\n",
      "Epoch: 4, Index: 217, Loss: 1.1250\n",
      "Epoch: 4, Index: 218, Loss: 4.5984\n",
      "Epoch: 4, Index: 219, Loss: 0.6513\n",
      "Epoch: 4, Index: 220, Loss: 0.3163\n",
      "Epoch: 4, Index: 221, Loss: 1.0288\n",
      "Epoch: 4, Index: 222, Loss: 0.9606\n",
      "Epoch: 4, Index: 223, Loss: 10.4848\n",
      "Epoch: 4, Index: 224, Loss: 3.2746\n",
      "Epoch: 4, Index: 225, Loss: 4.3446\n",
      "Epoch: 4, Index: 226, Loss: 0.5769\n",
      "Epoch: 4, Index: 227, Loss: 1.1868\n",
      "Epoch: 4, Index: 228, Loss: 3.0991\n",
      "Epoch: 4, Index: 229, Loss: 6.9360\n",
      "Epoch: 4, Index: 230, Loss: 3.2747\n",
      "Epoch: 4, Index: 231, Loss: 1.6531\n",
      "Epoch: 4, Index: 232, Loss: 0.4463\n",
      "Epoch: 4, Index: 233, Loss: 3.1067\n",
      "Epoch: 4, Index: 234, Loss: 0.3741\n",
      "Epoch: 4, Index: 235, Loss: 0.3640\n",
      "Epoch: 4, Index: 236, Loss: 0.5278\n",
      "Epoch: 4, Index: 237, Loss: 2.0512\n",
      "Epoch: 4, Index: 238, Loss: 2.4483\n",
      "Epoch: 4, Index: 239, Loss: 1.8545\n",
      "Epoch: 4, Index: 240, Loss: 2.2035\n",
      "Epoch: 4, Index: 241, Loss: 2.6894\n",
      "Epoch: 4, Index: 242, Loss: 0.2241\n",
      "Epoch: 4, Index: 243, Loss: 21.1313\n",
      "Epoch: 4, Index: 244, Loss: 0.2318\n",
      "Epoch: 4, Index: 245, Loss: 0.1549\n",
      "Epoch: 4, Index: 246, Loss: 0.7183\n",
      "Epoch: 4, Index: 247, Loss: 0.3870\n",
      "Epoch: 4, Index: 248, Loss: 1.4576\n",
      "Epoch: 4, Index: 249, Loss: 0.8261\n",
      "Epoch: 4, Index: 250, Loss: 0.4223\n",
      "Epoch: 4, Index: 251, Loss: 1.9749\n",
      "Epoch: 4, Index: 252, Loss: 2.2527\n",
      "Epoch: 4, Index: 253, Loss: 2.7660\n",
      "Epoch: 4, Index: 254, Loss: 4.4663\n",
      "Epoch: 4, Index: 255, Loss: 0.2597\n",
      "Epoch: 4, Index: 256, Loss: 0.6567\n",
      "Epoch: 4, Index: 257, Loss: 0.5897\n",
      "Epoch: 4, Index: 258, Loss: 0.5941\n",
      "Epoch: 4, Index: 259, Loss: 2.6267\n",
      "Epoch: 4, Index: 260, Loss: 0.2403\n",
      "Epoch: 4, Index: 261, Loss: 2.0448\n",
      "Epoch: 4, Index: 262, Loss: 2.1351\n",
      "Epoch: 4, Index: 263, Loss: 0.3219\n",
      "Epoch: 4, Index: 264, Loss: 2.4458\n",
      "Epoch: 4, Index: 265, Loss: 0.6610\n",
      "Epoch: 4, Index: 266, Loss: 0.7059\n",
      "Epoch: 4, Index: 267, Loss: 0.4669\n",
      "Epoch: 4, Index: 268, Loss: 0.5382\n",
      "Epoch: 4, Index: 269, Loss: 0.6232\n",
      "Epoch: 4, Index: 270, Loss: 0.3936\n",
      "Epoch: 4, Index: 271, Loss: 1.9225\n",
      "Epoch: 4, Index: 272, Loss: 1.6910\n",
      "Epoch: 4, Index: 273, Loss: 0.3026\n",
      "Epoch: 4, Index: 274, Loss: 0.3451\n",
      "Epoch: 4, Index: 275, Loss: 3.1044\n",
      "Epoch: 4, Index: 276, Loss: 0.5193\n",
      "Epoch: 4, Index: 277, Loss: 1.1188\n",
      "Epoch: 4, Index: 278, Loss: 0.8166\n",
      "Epoch: 4, Index: 279, Loss: 0.2390\n",
      "Epoch: 4, Index: 280, Loss: 0.1057\n",
      "Epoch: 4, Index: 281, Loss: 3.9346\n",
      "Epoch: 4, Index: 282, Loss: 4.7153\n",
      "Epoch: 4, Index: 283, Loss: 1.0647\n",
      "Epoch: 4, Index: 284, Loss: 0.7485\n",
      "Epoch: 4, Index: 285, Loss: 1.0877\n",
      "Epoch: 4, Index: 286, Loss: 3.7678\n",
      "Epoch: 4, Index: 287, Loss: 0.1669\n",
      "Epoch: 4, Index: 288, Loss: 1.3268\n",
      "Epoch: 4, Index: 289, Loss: 0.3326\n",
      "Epoch: 4, Index: 290, Loss: 5.1189\n",
      "Epoch: 4, Index: 291, Loss: 3.3972\n",
      "Epoch: 4, Index: 292, Loss: 4.9323\n",
      "Epoch: 4, Index: 293, Loss: 2.8510\n",
      "Epoch: 4, Index: 294, Loss: 1.1219\n",
      "Epoch: 4, Index: 295, Loss: 0.7377\n",
      "Epoch: 4, Index: 296, Loss: 3.8386\n",
      "Epoch: 4, Index: 297, Loss: 0.1882\n",
      "Epoch: 4, Index: 298, Loss: 1.1718\n",
      "Epoch: 4, Index: 299, Loss: 2.2810\n",
      "Epoch: 4, Index: 300, Loss: 2.7274\n",
      "Epoch: 4, Index: 301, Loss: 1.9879\n",
      "Epoch: 4, Index: 302, Loss: 1.5736\n",
      "Epoch: 4, Index: 303, Loss: 0.2183\n",
      "Epoch: 4, Index: 304, Loss: 0.1678\n",
      "Epoch: 4, Index: 305, Loss: 1.3138\n",
      "Epoch: 4, Index: 306, Loss: 0.0244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683ac5508a6b4651a78a8cd711185f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a457a14a2714e818e15e0ee9473ab30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Index: 0, Loss: 1.6971\n",
      "Epoch: 5, Index: 1, Loss: 1.8371\n",
      "Epoch: 5, Index: 2, Loss: 6.9075\n",
      "Epoch: 5, Index: 3, Loss: 6.5866\n",
      "Epoch: 5, Index: 4, Loss: 0.1578\n",
      "Epoch: 5, Index: 5, Loss: 0.7530\n",
      "Epoch: 5, Index: 6, Loss: 2.7142\n",
      "Epoch: 5, Index: 7, Loss: 0.4849\n",
      "Epoch: 5, Index: 8, Loss: 0.3609\n",
      "Epoch: 5, Index: 9, Loss: 2.6755\n",
      "Epoch: 5, Index: 10, Loss: 1.6379\n",
      "Epoch: 5, Index: 11, Loss: 2.9836\n",
      "Epoch: 5, Index: 12, Loss: 3.4302\n",
      "Epoch: 5, Index: 13, Loss: 0.1156\n",
      "Epoch: 5, Index: 14, Loss: 3.3738\n",
      "Epoch: 5, Index: 15, Loss: 0.3347\n",
      "Epoch: 5, Index: 16, Loss: 0.4339\n",
      "Epoch: 5, Index: 17, Loss: 1.1016\n",
      "Epoch: 5, Index: 18, Loss: 2.3961\n",
      "Epoch: 5, Index: 19, Loss: 0.0066\n",
      "Epoch: 5, Index: 20, Loss: 6.9380\n",
      "Epoch: 5, Index: 21, Loss: 0.6217\n",
      "Epoch: 5, Index: 22, Loss: 2.0806\n",
      "Epoch: 5, Index: 23, Loss: 1.0089\n",
      "Epoch: 5, Index: 24, Loss: 4.0221\n",
      "Epoch: 5, Index: 25, Loss: 0.8730\n",
      "Epoch: 5, Index: 26, Loss: 2.3777\n",
      "Epoch: 5, Index: 27, Loss: 1.9983\n",
      "Epoch: 5, Index: 28, Loss: 5.3189\n",
      "Epoch: 5, Index: 29, Loss: 1.2453\n",
      "Epoch: 5, Index: 30, Loss: 2.0331\n",
      "Epoch: 5, Index: 31, Loss: 5.4275\n",
      "Epoch: 5, Index: 32, Loss: 8.5240\n",
      "Epoch: 5, Index: 33, Loss: 1.3203\n",
      "Epoch: 5, Index: 34, Loss: 0.1184\n",
      "Epoch: 5, Index: 35, Loss: 0.6463\n",
      "Epoch: 5, Index: 36, Loss: 0.3552\n",
      "Epoch: 5, Index: 37, Loss: 1.3188\n",
      "Epoch: 5, Index: 38, Loss: 0.3876\n",
      "Epoch: 5, Index: 39, Loss: 7.9283\n",
      "Epoch: 5, Index: 40, Loss: 11.1774\n",
      "Epoch: 5, Index: 41, Loss: 0.6083\n",
      "Epoch: 5, Index: 42, Loss: 8.7498\n",
      "Epoch: 5, Index: 43, Loss: 1.5351\n",
      "Epoch: 5, Index: 44, Loss: 0.7755\n",
      "Epoch: 5, Index: 45, Loss: 6.0133\n",
      "Epoch: 5, Index: 46, Loss: 1.0960\n",
      "Epoch: 5, Index: 47, Loss: 4.8411\n",
      "Epoch: 5, Index: 48, Loss: 0.5663\n",
      "Epoch: 5, Index: 49, Loss: 0.7701\n",
      "Epoch: 5, Index: 50, Loss: 1.9077\n",
      "Epoch: 5, Index: 51, Loss: 0.2462\n",
      "Epoch: 5, Index: 52, Loss: 0.9224\n",
      "Epoch: 5, Index: 53, Loss: 2.7300\n",
      "Epoch: 5, Index: 54, Loss: 0.3740\n",
      "Epoch: 5, Index: 55, Loss: 1.2276\n",
      "Epoch: 5, Index: 56, Loss: 0.0010\n",
      "Epoch: 5, Index: 57, Loss: 3.9065\n",
      "Epoch: 5, Index: 58, Loss: 0.4211\n",
      "Epoch: 5, Index: 59, Loss: 0.6613\n",
      "Epoch: 5, Index: 60, Loss: 2.9987\n",
      "Epoch: 5, Index: 61, Loss: 2.7008\n",
      "Epoch: 5, Index: 62, Loss: 5.6751\n",
      "Epoch: 5, Index: 63, Loss: 1.5969\n",
      "Epoch: 5, Index: 64, Loss: 1.8647\n",
      "Epoch: 5, Index: 65, Loss: 1.1319\n",
      "Epoch: 5, Index: 66, Loss: 1.2486\n",
      "Epoch: 5, Index: 67, Loss: 4.9477\n",
      "Epoch: 5, Index: 68, Loss: 1.4965\n",
      "Epoch: 5, Index: 69, Loss: 1.6093\n",
      "Epoch: 5, Index: 70, Loss: 6.6192\n",
      "Epoch: 5, Index: 71, Loss: 0.6176\n",
      "Epoch: 5, Index: 72, Loss: 0.3448\n",
      "Epoch: 5, Index: 73, Loss: 5.9983\n",
      "Epoch: 5, Index: 74, Loss: 1.8895\n",
      "Epoch: 5, Index: 75, Loss: 3.5862\n",
      "Epoch: 5, Index: 76, Loss: 0.8427\n",
      "Epoch: 5, Index: 77, Loss: 0.5864\n",
      "Epoch: 5, Index: 78, Loss: 0.4189\n",
      "Epoch: 5, Index: 79, Loss: 0.4036\n",
      "Epoch: 5, Index: 80, Loss: 0.3885\n",
      "Epoch: 5, Index: 81, Loss: 0.9484\n",
      "Epoch: 5, Index: 82, Loss: 0.6677\n",
      "Epoch: 5, Index: 83, Loss: 2.1915\n",
      "Epoch: 5, Index: 84, Loss: 0.3874\n",
      "Epoch: 5, Index: 85, Loss: 1.6955\n",
      "Epoch: 5, Index: 86, Loss: 1.0391\n",
      "Epoch: 5, Index: 87, Loss: 1.2005\n",
      "Epoch: 5, Index: 88, Loss: 3.2090\n",
      "Epoch: 5, Index: 89, Loss: 3.6434\n",
      "Epoch: 5, Index: 90, Loss: 0.0545\n",
      "Epoch: 5, Index: 91, Loss: 2.1160\n",
      "Epoch: 5, Index: 92, Loss: 0.7415\n",
      "Epoch: 5, Index: 93, Loss: 3.5340\n",
      "Epoch: 5, Index: 94, Loss: 0.7059\n",
      "Epoch: 5, Index: 95, Loss: 2.3915\n",
      "Epoch: 5, Index: 96, Loss: 2.4962\n",
      "Epoch: 5, Index: 97, Loss: 0.3756\n",
      "Epoch: 5, Index: 98, Loss: 3.4030\n",
      "Epoch: 5, Index: 99, Loss: 2.3265\n",
      "Epoch: 5, Index: 100, Loss: 0.3484\n",
      "Epoch: 5, Index: 101, Loss: 0.8039\n",
      "Epoch: 5, Index: 102, Loss: 0.4160\n",
      "Epoch: 5, Index: 103, Loss: 5.9081\n",
      "Epoch: 5, Index: 104, Loss: 0.5351\n",
      "Epoch: 5, Index: 105, Loss: 0.8537\n",
      "Epoch: 5, Index: 106, Loss: 2.5909\n",
      "Epoch: 5, Index: 107, Loss: 1.9761\n",
      "Epoch: 5, Index: 108, Loss: 6.0750\n",
      "Epoch: 5, Index: 109, Loss: 16.7817\n",
      "Epoch: 5, Index: 110, Loss: 0.4889\n",
      "Epoch: 5, Index: 111, Loss: 0.7891\n",
      "Epoch: 5, Index: 112, Loss: 1.4027\n",
      "Epoch: 5, Index: 113, Loss: 1.7072\n",
      "Epoch: 5, Index: 114, Loss: 1.9374\n",
      "Epoch: 5, Index: 115, Loss: 1.5188\n",
      "Epoch: 5, Index: 116, Loss: 0.7542\n",
      "Epoch: 5, Index: 117, Loss: 1.7562\n",
      "Epoch: 5, Index: 118, Loss: 1.5315\n",
      "Epoch: 5, Index: 119, Loss: 0.7030\n",
      "Epoch: 5, Index: 120, Loss: 0.0192\n",
      "Epoch: 5, Index: 121, Loss: 0.1318\n",
      "Epoch: 5, Index: 122, Loss: 0.0715\n",
      "Epoch: 5, Index: 123, Loss: 1.2930\n",
      "Epoch: 5, Index: 124, Loss: 0.1492\n",
      "Epoch: 5, Index: 125, Loss: 1.4635\n",
      "Epoch: 5, Index: 126, Loss: 0.4677\n",
      "Epoch: 5, Index: 127, Loss: 0.3377\n",
      "Epoch: 5, Index: 128, Loss: 5.7858\n",
      "Epoch: 5, Index: 129, Loss: 5.6780\n",
      "Epoch: 5, Index: 130, Loss: 2.9049\n",
      "Epoch: 5, Index: 131, Loss: 1.1760\n",
      "Epoch: 5, Index: 132, Loss: 0.2360\n",
      "Epoch: 5, Index: 133, Loss: 0.0822\n",
      "Epoch: 5, Index: 134, Loss: 1.0776\n",
      "Epoch: 5, Index: 135, Loss: 0.9842\n",
      "Epoch: 5, Index: 136, Loss: 1.2552\n",
      "Epoch: 5, Index: 137, Loss: 0.6747\n",
      "Epoch: 5, Index: 138, Loss: 0.6964\n",
      "Epoch: 5, Index: 139, Loss: 0.3632\n",
      "Epoch: 5, Index: 140, Loss: 1.5150\n",
      "Epoch: 5, Index: 141, Loss: 1.7058\n",
      "Epoch: 5, Index: 142, Loss: 6.4349\n",
      "Epoch: 5, Index: 143, Loss: 2.4414\n",
      "Epoch: 5, Index: 144, Loss: 17.5390\n",
      "Epoch: 5, Index: 145, Loss: 0.0583\n",
      "Epoch: 5, Index: 146, Loss: 7.1087\n",
      "Epoch: 5, Index: 147, Loss: 2.4162\n",
      "Epoch: 5, Index: 148, Loss: 0.2827\n",
      "Epoch: 5, Index: 149, Loss: 3.1984\n",
      "Epoch: 5, Index: 150, Loss: 1.2214\n",
      "Epoch: 5, Index: 151, Loss: 0.4119\n",
      "Epoch: 5, Index: 152, Loss: 3.3722\n",
      "Epoch: 5, Index: 153, Loss: 0.1504\n",
      "Epoch: 5, Index: 154, Loss: 0.0757\n",
      "Epoch: 5, Index: 155, Loss: 4.8719\n",
      "Epoch: 5, Index: 156, Loss: 1.0883\n",
      "Epoch: 5, Index: 157, Loss: 1.3195\n",
      "Epoch: 5, Index: 158, Loss: 2.0206\n",
      "Epoch: 5, Index: 159, Loss: 0.1338\n",
      "Epoch: 5, Index: 160, Loss: 0.0379\n",
      "Epoch: 5, Index: 161, Loss: 0.8238\n",
      "Epoch: 5, Index: 162, Loss: 0.1852\n",
      "Epoch: 5, Index: 163, Loss: 0.3898\n",
      "Epoch: 5, Index: 164, Loss: 2.7876\n",
      "Epoch: 5, Index: 165, Loss: 0.2200\n",
      "Epoch: 5, Index: 166, Loss: 0.8636\n",
      "Epoch: 5, Index: 167, Loss: 1.0159\n",
      "Epoch: 5, Index: 168, Loss: 2.4124\n",
      "Epoch: 5, Index: 169, Loss: 3.4033\n",
      "Epoch: 5, Index: 170, Loss: 4.3782\n",
      "Epoch: 5, Index: 171, Loss: 0.2495\n",
      "Epoch: 5, Index: 172, Loss: 1.0169\n",
      "Epoch: 5, Index: 173, Loss: 3.3893\n",
      "Epoch: 5, Index: 174, Loss: 0.2927\n",
      "Epoch: 5, Index: 175, Loss: 0.0200\n",
      "Epoch: 5, Index: 176, Loss: 0.9321\n",
      "Epoch: 5, Index: 177, Loss: 1.7381\n",
      "Epoch: 5, Index: 178, Loss: 2.6884\n",
      "Epoch: 5, Index: 179, Loss: 0.6268\n",
      "Epoch: 5, Index: 180, Loss: 1.6262\n",
      "Epoch: 5, Index: 181, Loss: 4.2249\n",
      "Epoch: 5, Index: 182, Loss: 3.2838\n",
      "Epoch: 5, Index: 183, Loss: 4.3671\n",
      "Epoch: 5, Index: 184, Loss: 0.1161\n",
      "Epoch: 5, Index: 185, Loss: 3.2504\n",
      "Epoch: 5, Index: 186, Loss: 2.1577\n",
      "Epoch: 5, Index: 187, Loss: 0.9138\n",
      "Epoch: 5, Index: 188, Loss: 0.6211\n",
      "Epoch: 5, Index: 189, Loss: 1.5386\n",
      "Epoch: 5, Index: 190, Loss: 0.6024\n",
      "Epoch: 5, Index: 191, Loss: 0.1086\n",
      "Epoch: 5, Index: 192, Loss: 2.4082\n",
      "Epoch: 5, Index: 193, Loss: 3.2446\n",
      "Epoch: 5, Index: 194, Loss: 0.3922\n",
      "Epoch: 5, Index: 195, Loss: 3.7261\n",
      "Epoch: 5, Index: 196, Loss: 7.0027\n",
      "Epoch: 5, Index: 197, Loss: 0.7438\n",
      "Epoch: 5, Index: 198, Loss: 0.7636\n",
      "Epoch: 5, Index: 199, Loss: 0.8176\n",
      "Epoch: 5, Index: 200, Loss: 0.9000\n",
      "Epoch: 5, Index: 201, Loss: 0.8259\n",
      "Epoch: 5, Index: 202, Loss: 1.6266\n",
      "Epoch: 5, Index: 203, Loss: 0.0511\n",
      "Epoch: 5, Index: 204, Loss: 0.8482\n",
      "Epoch: 5, Index: 205, Loss: 2.2294\n",
      "Epoch: 5, Index: 206, Loss: 0.1840\n",
      "Epoch: 5, Index: 207, Loss: 0.5136\n",
      "Epoch: 5, Index: 208, Loss: 4.2458\n",
      "Epoch: 5, Index: 209, Loss: 3.6212\n",
      "Epoch: 5, Index: 210, Loss: 4.3649\n",
      "Epoch: 5, Index: 211, Loss: 3.4356\n",
      "Epoch: 5, Index: 212, Loss: 0.0131\n",
      "Epoch: 5, Index: 213, Loss: 1.6996\n",
      "Epoch: 5, Index: 214, Loss: 1.6641\n",
      "Epoch: 5, Index: 215, Loss: 3.5445\n",
      "Epoch: 5, Index: 216, Loss: 0.2087\n",
      "Epoch: 5, Index: 217, Loss: 0.5014\n",
      "Epoch: 5, Index: 218, Loss: 2.8523\n",
      "Epoch: 5, Index: 219, Loss: 1.9439\n",
      "Epoch: 5, Index: 220, Loss: 0.5470\n",
      "Epoch: 5, Index: 221, Loss: 3.6378\n",
      "Epoch: 5, Index: 222, Loss: 5.2856\n",
      "Epoch: 5, Index: 223, Loss: 6.5294\n",
      "Epoch: 5, Index: 224, Loss: 1.9930\n",
      "Epoch: 5, Index: 225, Loss: 5.5227\n",
      "Epoch: 5, Index: 226, Loss: 0.9515\n",
      "Epoch: 5, Index: 227, Loss: 0.6654\n",
      "Epoch: 5, Index: 228, Loss: 1.1039\n",
      "Epoch: 5, Index: 229, Loss: 4.2688\n",
      "Epoch: 5, Index: 230, Loss: 0.6128\n",
      "Epoch: 5, Index: 231, Loss: 1.6502\n",
      "Epoch: 5, Index: 232, Loss: 2.7196\n",
      "Epoch: 5, Index: 233, Loss: 0.0037\n",
      "Epoch: 5, Index: 234, Loss: 0.4703\n",
      "Epoch: 5, Index: 235, Loss: 0.1298\n",
      "Epoch: 5, Index: 236, Loss: 0.0653\n",
      "Epoch: 5, Index: 237, Loss: 5.2508\n",
      "Epoch: 5, Index: 238, Loss: 2.9342\n",
      "Epoch: 5, Index: 239, Loss: 0.3376\n",
      "Epoch: 5, Index: 240, Loss: 0.1028\n",
      "Epoch: 5, Index: 241, Loss: 2.5302\n",
      "Epoch: 5, Index: 242, Loss: 0.8000\n",
      "Epoch: 5, Index: 243, Loss: 0.7582\n",
      "Epoch: 5, Index: 244, Loss: 1.4163\n",
      "Epoch: 5, Index: 245, Loss: 0.6160\n",
      "Epoch: 5, Index: 246, Loss: 0.3958\n",
      "Epoch: 5, Index: 247, Loss: 0.3137\n",
      "Epoch: 5, Index: 248, Loss: 0.0659\n",
      "Epoch: 5, Index: 249, Loss: 2.9472\n",
      "Epoch: 5, Index: 250, Loss: 1.3316\n",
      "Epoch: 5, Index: 251, Loss: 3.1269\n",
      "Epoch: 5, Index: 252, Loss: 0.8562\n",
      "Epoch: 5, Index: 253, Loss: 2.0179\n",
      "Epoch: 5, Index: 254, Loss: 0.7436\n",
      "Epoch: 5, Index: 255, Loss: 3.1512\n",
      "Epoch: 5, Index: 256, Loss: 4.0319\n",
      "Epoch: 5, Index: 257, Loss: 2.5470\n",
      "Epoch: 5, Index: 258, Loss: 4.0339\n",
      "Epoch: 5, Index: 259, Loss: 1.0734\n",
      "Epoch: 5, Index: 260, Loss: 0.1389\n",
      "Epoch: 5, Index: 261, Loss: 2.3567\n",
      "Epoch: 5, Index: 262, Loss: 0.7881\n",
      "Epoch: 5, Index: 263, Loss: 3.9498\n",
      "Epoch: 5, Index: 264, Loss: 4.4064\n",
      "Epoch: 5, Index: 265, Loss: 2.0288\n",
      "Epoch: 5, Index: 266, Loss: 1.9722\n",
      "Epoch: 5, Index: 267, Loss: 3.6748\n",
      "Epoch: 5, Index: 268, Loss: 2.1951\n",
      "Epoch: 5, Index: 269, Loss: 2.9928\n",
      "Epoch: 5, Index: 270, Loss: 1.4476\n",
      "Epoch: 5, Index: 271, Loss: 2.3248\n",
      "Epoch: 5, Index: 272, Loss: 2.0314\n",
      "Epoch: 5, Index: 273, Loss: 1.3502\n",
      "Epoch: 5, Index: 274, Loss: 0.8428\n",
      "Epoch: 5, Index: 275, Loss: 0.2893\n",
      "Epoch: 5, Index: 276, Loss: 1.6747\n",
      "Epoch: 5, Index: 277, Loss: 5.2722\n",
      "Epoch: 5, Index: 278, Loss: 1.3752\n",
      "Epoch: 5, Index: 279, Loss: 0.1749\n",
      "Epoch: 5, Index: 280, Loss: 1.2225\n",
      "Epoch: 5, Index: 281, Loss: 1.2008\n",
      "Epoch: 5, Index: 282, Loss: 0.5847\n",
      "Epoch: 5, Index: 283, Loss: 0.5956\n",
      "Epoch: 5, Index: 284, Loss: 1.3806\n",
      "Epoch: 5, Index: 285, Loss: 6.3467\n",
      "Epoch: 5, Index: 286, Loss: 1.0249\n",
      "Epoch: 5, Index: 287, Loss: 4.3821\n",
      "Epoch: 5, Index: 288, Loss: 2.1817\n",
      "Epoch: 5, Index: 289, Loss: 3.5689\n",
      "Epoch: 5, Index: 290, Loss: 2.6001\n",
      "Epoch: 5, Index: 291, Loss: 2.8240\n",
      "Epoch: 5, Index: 292, Loss: 5.0822\n",
      "Epoch: 5, Index: 293, Loss: 0.1211\n",
      "Epoch: 5, Index: 294, Loss: 4.6948\n",
      "Epoch: 5, Index: 295, Loss: 2.5093\n",
      "Epoch: 5, Index: 296, Loss: 1.6203\n",
      "Epoch: 5, Index: 297, Loss: 1.3362\n",
      "Epoch: 5, Index: 298, Loss: 3.2248\n",
      "Epoch: 5, Index: 299, Loss: 2.8729\n",
      "Epoch: 5, Index: 300, Loss: 0.8437\n",
      "Epoch: 5, Index: 301, Loss: 1.2736\n",
      "Epoch: 5, Index: 302, Loss: 4.7264\n",
      "Epoch: 5, Index: 303, Loss: 1.7106\n",
      "Epoch: 5, Index: 304, Loss: 2.9838\n",
      "Epoch: 5, Index: 305, Loss: 0.1102\n",
      "Epoch: 5, Index: 306, Loss: 1.3482\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820bed33b1ab4b8ea806957fcc211f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db94feba54348298ce69f9f22129f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Index: 0, Loss: 11.6248\n",
      "Epoch: 6, Index: 1, Loss: 0.5223\n",
      "Epoch: 6, Index: 2, Loss: 0.1906\n",
      "Epoch: 6, Index: 3, Loss: 5.1794\n",
      "Epoch: 6, Index: 4, Loss: 0.9500\n",
      "Epoch: 6, Index: 5, Loss: 1.1112\n",
      "Epoch: 6, Index: 6, Loss: 8.2098\n",
      "Epoch: 6, Index: 7, Loss: 0.8993\n",
      "Epoch: 6, Index: 8, Loss: 2.9895\n",
      "Epoch: 6, Index: 9, Loss: 4.3113\n",
      "Epoch: 6, Index: 10, Loss: 0.2907\n",
      "Epoch: 6, Index: 11, Loss: 1.0113\n",
      "Epoch: 6, Index: 12, Loss: 0.3373\n",
      "Epoch: 6, Index: 13, Loss: 4.8199\n",
      "Epoch: 6, Index: 14, Loss: 0.6498\n",
      "Epoch: 6, Index: 15, Loss: 0.0421\n",
      "Epoch: 6, Index: 16, Loss: 1.6266\n",
      "Epoch: 6, Index: 17, Loss: 0.0629\n",
      "Epoch: 6, Index: 18, Loss: 3.2322\n",
      "Epoch: 6, Index: 19, Loss: 9.4700\n",
      "Epoch: 6, Index: 20, Loss: 1.4298\n",
      "Epoch: 6, Index: 21, Loss: 1.5257\n",
      "Epoch: 6, Index: 22, Loss: 0.6000\n",
      "Epoch: 6, Index: 23, Loss: 5.3228\n",
      "Epoch: 6, Index: 24, Loss: 0.3837\n",
      "Epoch: 6, Index: 25, Loss: 2.0518\n",
      "Epoch: 6, Index: 26, Loss: 0.9053\n",
      "Epoch: 6, Index: 27, Loss: 3.3880\n",
      "Epoch: 6, Index: 28, Loss: 2.7427\n",
      "Epoch: 6, Index: 29, Loss: 0.0426\n",
      "Epoch: 6, Index: 30, Loss: 0.7330\n",
      "Epoch: 6, Index: 31, Loss: 3.3075\n",
      "Epoch: 6, Index: 32, Loss: 0.5751\n",
      "Epoch: 6, Index: 33, Loss: 1.1271\n",
      "Epoch: 6, Index: 34, Loss: 0.5855\n",
      "Epoch: 6, Index: 35, Loss: 2.4653\n",
      "Epoch: 6, Index: 36, Loss: 2.5376\n",
      "Epoch: 6, Index: 37, Loss: 7.0391\n",
      "Epoch: 6, Index: 38, Loss: 1.7980\n",
      "Epoch: 6, Index: 39, Loss: 1.0480\n",
      "Epoch: 6, Index: 40, Loss: 2.4694\n",
      "Epoch: 6, Index: 41, Loss: 0.6988\n",
      "Epoch: 6, Index: 42, Loss: 1.5030\n",
      "Epoch: 6, Index: 43, Loss: 2.2083\n",
      "Epoch: 6, Index: 44, Loss: 1.8914\n",
      "Epoch: 6, Index: 45, Loss: 0.9680\n",
      "Epoch: 6, Index: 46, Loss: 0.3396\n",
      "Epoch: 6, Index: 47, Loss: 4.7623\n",
      "Epoch: 6, Index: 48, Loss: 3.6150\n",
      "Epoch: 6, Index: 49, Loss: 5.6699\n",
      "Epoch: 6, Index: 50, Loss: 0.0007\n",
      "Epoch: 6, Index: 51, Loss: 4.9850\n",
      "Epoch: 6, Index: 52, Loss: 0.5490\n",
      "Epoch: 6, Index: 53, Loss: 3.8986\n",
      "Epoch: 6, Index: 54, Loss: 0.8907\n",
      "Epoch: 6, Index: 55, Loss: 0.3567\n",
      "Epoch: 6, Index: 56, Loss: 2.3232\n",
      "Epoch: 6, Index: 57, Loss: 1.6224\n",
      "Epoch: 6, Index: 58, Loss: 2.9526\n",
      "Epoch: 6, Index: 59, Loss: 7.0524\n",
      "Epoch: 6, Index: 60, Loss: 2.8947\n",
      "Epoch: 6, Index: 61, Loss: 3.7005\n",
      "Epoch: 6, Index: 62, Loss: 4.5805\n",
      "Epoch: 6, Index: 63, Loss: 1.0071\n",
      "Epoch: 6, Index: 64, Loss: 0.4956\n",
      "Epoch: 6, Index: 65, Loss: 1.1812\n",
      "Epoch: 6, Index: 66, Loss: 5.6690\n",
      "Epoch: 6, Index: 67, Loss: 2.9512\n",
      "Epoch: 6, Index: 68, Loss: 2.2179\n",
      "Epoch: 6, Index: 69, Loss: 8.3398\n",
      "Epoch: 6, Index: 70, Loss: 9.0072\n",
      "Epoch: 6, Index: 71, Loss: 2.8226\n",
      "Epoch: 6, Index: 72, Loss: 1.1567\n",
      "Epoch: 6, Index: 73, Loss: 1.3787\n",
      "Epoch: 6, Index: 74, Loss: 0.0761\n",
      "Epoch: 6, Index: 75, Loss: 1.2815\n",
      "Epoch: 6, Index: 76, Loss: 0.8287\n",
      "Epoch: 6, Index: 77, Loss: 2.9000\n",
      "Epoch: 6, Index: 78, Loss: 5.5354\n",
      "Epoch: 6, Index: 79, Loss: 1.3634\n",
      "Epoch: 6, Index: 80, Loss: 2.4402\n",
      "Epoch: 6, Index: 81, Loss: 2.2644\n",
      "Epoch: 6, Index: 82, Loss: 0.3373\n",
      "Epoch: 6, Index: 83, Loss: 1.7099\n",
      "Epoch: 6, Index: 84, Loss: 1.1380\n",
      "Epoch: 6, Index: 85, Loss: 4.2242\n",
      "Epoch: 6, Index: 86, Loss: 1.6867\n",
      "Epoch: 6, Index: 87, Loss: 1.9407\n",
      "Epoch: 6, Index: 88, Loss: 0.5213\n",
      "Epoch: 6, Index: 89, Loss: 1.8122\n",
      "Epoch: 6, Index: 90, Loss: 0.5021\n",
      "Epoch: 6, Index: 91, Loss: 0.1719\n",
      "Epoch: 6, Index: 92, Loss: 2.2554\n",
      "Epoch: 6, Index: 93, Loss: 3.4858\n",
      "Epoch: 6, Index: 94, Loss: 0.2747\n",
      "Epoch: 6, Index: 95, Loss: 0.0923\n",
      "Epoch: 6, Index: 96, Loss: 1.8763\n",
      "Epoch: 6, Index: 97, Loss: 1.8561\n",
      "Epoch: 6, Index: 98, Loss: 0.2176\n",
      "Epoch: 6, Index: 99, Loss: 0.4309\n",
      "Epoch: 6, Index: 100, Loss: 3.9940\n",
      "Epoch: 6, Index: 101, Loss: 0.5694\n",
      "Epoch: 6, Index: 102, Loss: 1.2830\n",
      "Epoch: 6, Index: 103, Loss: 2.8171\n",
      "Epoch: 6, Index: 104, Loss: 0.2745\n",
      "Epoch: 6, Index: 105, Loss: 3.1896\n",
      "Epoch: 6, Index: 106, Loss: 1.0969\n",
      "Epoch: 6, Index: 107, Loss: 1.7751\n",
      "Epoch: 6, Index: 108, Loss: 2.2951\n",
      "Epoch: 6, Index: 109, Loss: 0.4056\n",
      "Epoch: 6, Index: 110, Loss: 3.0530\n",
      "Epoch: 6, Index: 111, Loss: 4.0866\n",
      "Epoch: 6, Index: 112, Loss: 0.8893\n",
      "Epoch: 6, Index: 113, Loss: 5.4231\n",
      "Epoch: 6, Index: 114, Loss: 0.2011\n",
      "Epoch: 6, Index: 115, Loss: 1.2131\n",
      "Epoch: 6, Index: 116, Loss: 0.9060\n",
      "Epoch: 6, Index: 117, Loss: 2.8479\n",
      "Epoch: 6, Index: 118, Loss: 0.6511\n",
      "Epoch: 6, Index: 119, Loss: 0.5875\n",
      "Epoch: 6, Index: 120, Loss: 8.1550\n",
      "Epoch: 6, Index: 121, Loss: 1.0331\n",
      "Epoch: 6, Index: 122, Loss: 1.4483\n",
      "Epoch: 6, Index: 123, Loss: 1.0571\n",
      "Epoch: 6, Index: 124, Loss: 6.4420\n",
      "Epoch: 6, Index: 125, Loss: 0.8636\n",
      "Epoch: 6, Index: 126, Loss: 1.2110\n",
      "Epoch: 6, Index: 127, Loss: 1.6035\n",
      "Epoch: 6, Index: 128, Loss: 0.9590\n",
      "Epoch: 6, Index: 129, Loss: 0.7487\n",
      "Epoch: 6, Index: 130, Loss: 0.0880\n",
      "Epoch: 6, Index: 131, Loss: 0.3277\n",
      "Epoch: 6, Index: 132, Loss: 0.5876\n",
      "Epoch: 6, Index: 133, Loss: 3.4584\n",
      "Epoch: 6, Index: 134, Loss: 0.2622\n",
      "Epoch: 6, Index: 135, Loss: 4.0051\n",
      "Epoch: 6, Index: 136, Loss: 2.1544\n",
      "Epoch: 6, Index: 137, Loss: 2.8989\n",
      "Epoch: 6, Index: 138, Loss: 1.5408\n",
      "Epoch: 6, Index: 139, Loss: 0.2537\n",
      "Epoch: 6, Index: 140, Loss: 3.8903\n",
      "Epoch: 6, Index: 141, Loss: 0.8228\n",
      "Epoch: 6, Index: 142, Loss: 1.2459\n",
      "Epoch: 6, Index: 143, Loss: 0.1232\n",
      "Epoch: 6, Index: 144, Loss: 0.8070\n",
      "Epoch: 6, Index: 145, Loss: 0.4904\n",
      "Epoch: 6, Index: 146, Loss: 2.2364\n",
      "Epoch: 6, Index: 147, Loss: 0.7253\n",
      "Epoch: 6, Index: 148, Loss: 0.7849\n",
      "Epoch: 6, Index: 149, Loss: 0.5558\n",
      "Epoch: 6, Index: 150, Loss: 1.8781\n",
      "Epoch: 6, Index: 151, Loss: 7.1258\n",
      "Epoch: 6, Index: 152, Loss: 0.1929\n",
      "Epoch: 6, Index: 153, Loss: 12.1668\n",
      "Epoch: 6, Index: 154, Loss: 1.1149\n",
      "Epoch: 6, Index: 155, Loss: 1.1043\n",
      "Epoch: 6, Index: 156, Loss: 5.9126\n",
      "Epoch: 6, Index: 157, Loss: 0.7217\n",
      "Epoch: 6, Index: 158, Loss: 1.3562\n",
      "Epoch: 6, Index: 159, Loss: 0.2698\n",
      "Epoch: 6, Index: 160, Loss: 2.2875\n",
      "Epoch: 6, Index: 161, Loss: 1.2150\n",
      "Epoch: 6, Index: 162, Loss: 0.9904\n",
      "Epoch: 6, Index: 163, Loss: 0.8916\n",
      "Epoch: 6, Index: 164, Loss: 0.3896\n",
      "Epoch: 6, Index: 165, Loss: 5.6088\n",
      "Epoch: 6, Index: 166, Loss: 4.6405\n",
      "Epoch: 6, Index: 167, Loss: 1.9290\n",
      "Epoch: 6, Index: 168, Loss: 1.5042\n",
      "Epoch: 6, Index: 169, Loss: 3.3521\n",
      "Epoch: 6, Index: 170, Loss: 0.8016\n",
      "Epoch: 6, Index: 171, Loss: 2.8680\n",
      "Epoch: 6, Index: 172, Loss: 0.3924\n",
      "Epoch: 6, Index: 173, Loss: 0.2435\n",
      "Epoch: 6, Index: 174, Loss: 1.9959\n",
      "Epoch: 6, Index: 175, Loss: 0.0587\n",
      "Epoch: 6, Index: 176, Loss: 7.2606\n",
      "Epoch: 6, Index: 177, Loss: 0.2776\n",
      "Epoch: 6, Index: 178, Loss: 4.7497\n",
      "Epoch: 6, Index: 179, Loss: 0.4884\n",
      "Epoch: 6, Index: 180, Loss: 0.2448\n",
      "Epoch: 6, Index: 181, Loss: 0.5036\n",
      "Epoch: 6, Index: 182, Loss: 4.8610\n",
      "Epoch: 6, Index: 183, Loss: 1.5864\n",
      "Epoch: 6, Index: 184, Loss: 5.8443\n",
      "Epoch: 6, Index: 185, Loss: 0.3838\n",
      "Epoch: 6, Index: 186, Loss: 1.2923\n",
      "Epoch: 6, Index: 187, Loss: 0.0015\n",
      "Epoch: 6, Index: 188, Loss: 19.8872\n",
      "Epoch: 6, Index: 189, Loss: 1.4618\n",
      "Epoch: 6, Index: 190, Loss: 6.2056\n",
      "Epoch: 6, Index: 191, Loss: 0.5665\n",
      "Epoch: 6, Index: 192, Loss: 3.5600\n",
      "Epoch: 6, Index: 193, Loss: 1.3608\n",
      "Epoch: 6, Index: 194, Loss: 0.8418\n",
      "Epoch: 6, Index: 195, Loss: 0.6648\n",
      "Epoch: 6, Index: 196, Loss: 12.3361\n",
      "Epoch: 6, Index: 197, Loss: 0.0259\n",
      "Epoch: 6, Index: 198, Loss: 0.5849\n",
      "Epoch: 6, Index: 199, Loss: 5.2740\n",
      "Epoch: 6, Index: 200, Loss: 0.9537\n",
      "Epoch: 6, Index: 201, Loss: 1.4771\n",
      "Epoch: 6, Index: 202, Loss: 0.3522\n",
      "Epoch: 6, Index: 203, Loss: 7.9900\n",
      "Epoch: 6, Index: 204, Loss: 0.9571\n",
      "Epoch: 6, Index: 205, Loss: 2.7272\n",
      "Epoch: 6, Index: 206, Loss: 0.9325\n",
      "Epoch: 6, Index: 207, Loss: 0.8992\n",
      "Epoch: 6, Index: 208, Loss: 0.6808\n",
      "Epoch: 6, Index: 209, Loss: 2.2451\n",
      "Epoch: 6, Index: 210, Loss: 0.5575\n",
      "Epoch: 6, Index: 211, Loss: 2.0467\n",
      "Epoch: 6, Index: 212, Loss: 1.8904\n",
      "Epoch: 6, Index: 213, Loss: 0.9626\n",
      "Epoch: 6, Index: 214, Loss: 2.0820\n",
      "Epoch: 6, Index: 215, Loss: 0.9474\n",
      "Epoch: 6, Index: 216, Loss: 3.5034\n",
      "Epoch: 6, Index: 217, Loss: 6.7885\n",
      "Epoch: 6, Index: 218, Loss: 3.3069\n",
      "Epoch: 6, Index: 219, Loss: 0.7014\n",
      "Epoch: 6, Index: 220, Loss: 1.5387\n",
      "Epoch: 6, Index: 221, Loss: 2.4283\n",
      "Epoch: 6, Index: 222, Loss: 1.1470\n",
      "Epoch: 6, Index: 223, Loss: 0.1257\n",
      "Epoch: 6, Index: 224, Loss: 1.1763\n",
      "Epoch: 6, Index: 225, Loss: 2.0143\n",
      "Epoch: 6, Index: 226, Loss: 0.9202\n",
      "Epoch: 6, Index: 227, Loss: 3.4861\n",
      "Epoch: 6, Index: 228, Loss: 2.3386\n",
      "Epoch: 6, Index: 229, Loss: 0.4713\n",
      "Epoch: 6, Index: 230, Loss: 5.6383\n",
      "Epoch: 6, Index: 231, Loss: 8.5602\n",
      "Epoch: 6, Index: 232, Loss: 1.6846\n",
      "Epoch: 6, Index: 233, Loss: 4.2215\n",
      "Epoch: 6, Index: 234, Loss: 0.8362\n",
      "Epoch: 6, Index: 235, Loss: 0.9383\n",
      "Epoch: 6, Index: 236, Loss: 0.9830\n",
      "Epoch: 6, Index: 237, Loss: 0.2253\n",
      "Epoch: 6, Index: 238, Loss: 4.3794\n",
      "Epoch: 6, Index: 239, Loss: 1.5993\n",
      "Epoch: 6, Index: 240, Loss: 1.7621\n",
      "Epoch: 6, Index: 241, Loss: 0.5060\n",
      "Epoch: 6, Index: 242, Loss: 1.1274\n",
      "Epoch: 6, Index: 243, Loss: 4.9099\n",
      "Epoch: 6, Index: 244, Loss: 2.0324\n",
      "Epoch: 6, Index: 245, Loss: 0.4229\n",
      "Epoch: 6, Index: 246, Loss: 1.1576\n",
      "Epoch: 6, Index: 247, Loss: 2.4165\n",
      "Epoch: 6, Index: 248, Loss: 3.0994\n",
      "Epoch: 6, Index: 249, Loss: 1.4052\n",
      "Epoch: 6, Index: 250, Loss: 0.5260\n",
      "Epoch: 6, Index: 251, Loss: 2.0435\n",
      "Epoch: 6, Index: 252, Loss: 0.2988\n",
      "Epoch: 6, Index: 253, Loss: 1.5999\n",
      "Epoch: 6, Index: 254, Loss: 0.5136\n",
      "Epoch: 6, Index: 255, Loss: 3.7751\n",
      "Epoch: 6, Index: 256, Loss: 1.0513\n",
      "Epoch: 6, Index: 257, Loss: 2.9345\n",
      "Epoch: 6, Index: 258, Loss: 3.0817\n",
      "Epoch: 6, Index: 259, Loss: 0.3818\n",
      "Epoch: 6, Index: 260, Loss: 1.0745\n",
      "Epoch: 6, Index: 261, Loss: 3.5605\n",
      "Epoch: 6, Index: 262, Loss: 1.5316\n",
      "Epoch: 6, Index: 263, Loss: 1.6352\n",
      "Epoch: 6, Index: 264, Loss: 4.2438\n",
      "Epoch: 6, Index: 265, Loss: 2.0925\n",
      "Epoch: 6, Index: 266, Loss: 3.8113\n",
      "Epoch: 6, Index: 267, Loss: 3.2521\n",
      "Epoch: 6, Index: 268, Loss: 2.2583\n",
      "Epoch: 6, Index: 269, Loss: 0.0549\n",
      "Epoch: 6, Index: 270, Loss: 0.9971\n",
      "Epoch: 6, Index: 271, Loss: 2.8132\n",
      "Epoch: 6, Index: 272, Loss: 0.3684\n",
      "Epoch: 6, Index: 273, Loss: 0.8692\n",
      "Epoch: 6, Index: 274, Loss: 1.9787\n",
      "Epoch: 6, Index: 275, Loss: 0.2933\n",
      "Epoch: 6, Index: 276, Loss: 0.3601\n",
      "Epoch: 6, Index: 277, Loss: 1.0510\n",
      "Epoch: 6, Index: 278, Loss: 0.2878\n",
      "Epoch: 6, Index: 279, Loss: 0.7793\n",
      "Epoch: 6, Index: 280, Loss: 0.3987\n",
      "Epoch: 6, Index: 281, Loss: 1.3088\n",
      "Epoch: 6, Index: 282, Loss: 2.2027\n",
      "Epoch: 6, Index: 283, Loss: 0.4056\n",
      "Epoch: 6, Index: 284, Loss: 1.2018\n",
      "Epoch: 6, Index: 285, Loss: 0.5490\n",
      "Epoch: 6, Index: 286, Loss: 0.2135\n",
      "Epoch: 6, Index: 287, Loss: 0.3603\n",
      "Epoch: 6, Index: 288, Loss: 0.2125\n",
      "Epoch: 6, Index: 289, Loss: 0.4361\n",
      "Epoch: 6, Index: 290, Loss: 0.7631\n",
      "Epoch: 6, Index: 291, Loss: 2.6298\n",
      "Epoch: 6, Index: 292, Loss: 0.0024\n",
      "Epoch: 6, Index: 293, Loss: 2.7633\n",
      "Epoch: 6, Index: 294, Loss: 1.1039\n",
      "Epoch: 6, Index: 295, Loss: 5.7568\n",
      "Epoch: 6, Index: 296, Loss: 0.2299\n",
      "Epoch: 6, Index: 297, Loss: 1.0947\n",
      "Epoch: 6, Index: 298, Loss: 2.4232\n",
      "Epoch: 6, Index: 299, Loss: 2.2210\n",
      "Epoch: 6, Index: 300, Loss: 2.2025\n",
      "Epoch: 6, Index: 301, Loss: 0.6991\n",
      "Epoch: 6, Index: 302, Loss: 0.0187\n",
      "Epoch: 6, Index: 303, Loss: 0.8186\n",
      "Epoch: 6, Index: 304, Loss: 3.6690\n",
      "Epoch: 6, Index: 305, Loss: 1.9266\n",
      "Epoch: 6, Index: 306, Loss: 0.5552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47537a07751d457c9465453ce0bd6c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8412ef1e2b8463a84068ebc72a8634e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Index: 0, Loss: 0.1933\n",
      "Epoch: 7, Index: 1, Loss: 0.0273\n",
      "Epoch: 7, Index: 2, Loss: 0.6413\n",
      "Epoch: 7, Index: 3, Loss: 1.5105\n",
      "Epoch: 7, Index: 4, Loss: 0.0505\n",
      "Epoch: 7, Index: 5, Loss: 3.5033\n",
      "Epoch: 7, Index: 6, Loss: 1.2255\n",
      "Epoch: 7, Index: 7, Loss: 2.9277\n",
      "Epoch: 7, Index: 8, Loss: 8.9506\n",
      "Epoch: 7, Index: 9, Loss: 1.4321\n",
      "Epoch: 7, Index: 10, Loss: 3.0192\n",
      "Epoch: 7, Index: 11, Loss: 1.0162\n",
      "Epoch: 7, Index: 12, Loss: 0.6929\n",
      "Epoch: 7, Index: 13, Loss: 4.9030\n",
      "Epoch: 7, Index: 14, Loss: 1.0826\n",
      "Epoch: 7, Index: 15, Loss: 1.6206\n",
      "Epoch: 7, Index: 16, Loss: 1.3593\n",
      "Epoch: 7, Index: 17, Loss: 0.2608\n",
      "Epoch: 7, Index: 18, Loss: 1.9328\n",
      "Epoch: 7, Index: 19, Loss: 1.3511\n",
      "Epoch: 7, Index: 20, Loss: 0.4600\n",
      "Epoch: 7, Index: 21, Loss: 0.3714\n",
      "Epoch: 7, Index: 22, Loss: 8.7972\n",
      "Epoch: 7, Index: 23, Loss: 2.2158\n",
      "Epoch: 7, Index: 24, Loss: 2.2256\n",
      "Epoch: 7, Index: 25, Loss: 2.7156\n",
      "Epoch: 7, Index: 26, Loss: 4.7522\n",
      "Epoch: 7, Index: 27, Loss: 4.3483\n",
      "Epoch: 7, Index: 28, Loss: 0.1716\n",
      "Epoch: 7, Index: 29, Loss: 1.7669\n",
      "Epoch: 7, Index: 30, Loss: 1.3028\n",
      "Epoch: 7, Index: 31, Loss: 1.2289\n",
      "Epoch: 7, Index: 32, Loss: 1.2297\n",
      "Epoch: 7, Index: 33, Loss: 0.0846\n",
      "Epoch: 7, Index: 34, Loss: 1.3054\n",
      "Epoch: 7, Index: 35, Loss: 0.1503\n",
      "Epoch: 7, Index: 36, Loss: 2.7194\n",
      "Epoch: 7, Index: 37, Loss: 0.0279\n",
      "Epoch: 7, Index: 38, Loss: 0.2028\n",
      "Epoch: 7, Index: 39, Loss: 0.4149\n",
      "Epoch: 7, Index: 40, Loss: 1.8024\n",
      "Epoch: 7, Index: 41, Loss: 0.8363\n",
      "Epoch: 7, Index: 42, Loss: 0.8022\n",
      "Epoch: 7, Index: 43, Loss: 1.0487\n",
      "Epoch: 7, Index: 44, Loss: 0.4151\n",
      "Epoch: 7, Index: 45, Loss: 0.1630\n",
      "Epoch: 7, Index: 46, Loss: 0.1727\n",
      "Epoch: 7, Index: 47, Loss: 0.3081\n",
      "Epoch: 7, Index: 48, Loss: 0.1358\n",
      "Epoch: 7, Index: 49, Loss: 8.3326\n",
      "Epoch: 7, Index: 50, Loss: 3.0250\n",
      "Epoch: 7, Index: 51, Loss: 5.8974\n",
      "Epoch: 7, Index: 52, Loss: 2.2590\n",
      "Epoch: 7, Index: 53, Loss: 5.0873\n",
      "Epoch: 7, Index: 54, Loss: 4.4620\n",
      "Epoch: 7, Index: 55, Loss: 0.0807\n",
      "Epoch: 7, Index: 56, Loss: 0.5822\n",
      "Epoch: 7, Index: 57, Loss: 1.1536\n",
      "Epoch: 7, Index: 58, Loss: 0.6104\n",
      "Epoch: 7, Index: 59, Loss: 1.3057\n",
      "Epoch: 7, Index: 60, Loss: 2.5720\n",
      "Epoch: 7, Index: 61, Loss: 0.1932\n",
      "Epoch: 7, Index: 62, Loss: 2.5433\n",
      "Epoch: 7, Index: 63, Loss: 3.1414\n",
      "Epoch: 7, Index: 64, Loss: 2.5367\n",
      "Epoch: 7, Index: 65, Loss: 1.1438\n",
      "Epoch: 7, Index: 66, Loss: 5.5643\n",
      "Epoch: 7, Index: 67, Loss: 0.2462\n",
      "Epoch: 7, Index: 68, Loss: 0.2231\n",
      "Epoch: 7, Index: 69, Loss: 4.7438\n",
      "Epoch: 7, Index: 70, Loss: 0.3942\n",
      "Epoch: 7, Index: 71, Loss: 1.5305\n",
      "Epoch: 7, Index: 72, Loss: 2.2399\n",
      "Epoch: 7, Index: 73, Loss: 2.5814\n",
      "Epoch: 7, Index: 74, Loss: 2.0814\n",
      "Epoch: 7, Index: 75, Loss: 2.1055\n",
      "Epoch: 7, Index: 76, Loss: 2.5357\n",
      "Epoch: 7, Index: 77, Loss: 5.2575\n",
      "Epoch: 7, Index: 78, Loss: 1.1050\n",
      "Epoch: 7, Index: 79, Loss: 0.2719\n",
      "Epoch: 7, Index: 80, Loss: 1.2604\n",
      "Epoch: 7, Index: 81, Loss: 0.9943\n",
      "Epoch: 7, Index: 82, Loss: 0.5866\n",
      "Epoch: 7, Index: 83, Loss: 1.6315\n",
      "Epoch: 7, Index: 84, Loss: 4.2661\n",
      "Epoch: 7, Index: 85, Loss: 0.3088\n",
      "Epoch: 7, Index: 86, Loss: 3.9441\n",
      "Epoch: 7, Index: 87, Loss: 1.7495\n",
      "Epoch: 7, Index: 88, Loss: 1.3212\n",
      "Epoch: 7, Index: 89, Loss: 1.0839\n",
      "Epoch: 7, Index: 90, Loss: 0.8814\n",
      "Epoch: 7, Index: 91, Loss: 2.4784\n",
      "Epoch: 7, Index: 92, Loss: 1.2259\n",
      "Epoch: 7, Index: 93, Loss: 6.0334\n",
      "Epoch: 7, Index: 94, Loss: 0.4200\n",
      "Epoch: 7, Index: 95, Loss: 0.4390\n",
      "Epoch: 7, Index: 96, Loss: 1.8045\n",
      "Epoch: 7, Index: 97, Loss: 2.2541\n",
      "Epoch: 7, Index: 98, Loss: 0.1675\n",
      "Epoch: 7, Index: 99, Loss: 0.9068\n",
      "Epoch: 7, Index: 100, Loss: 1.9235\n",
      "Epoch: 7, Index: 101, Loss: 0.3055\n",
      "Epoch: 7, Index: 102, Loss: 0.0915\n",
      "Epoch: 7, Index: 103, Loss: 0.6528\n",
      "Epoch: 7, Index: 104, Loss: 0.2542\n",
      "Epoch: 7, Index: 105, Loss: 5.9564\n",
      "Epoch: 7, Index: 106, Loss: 2.7165\n",
      "Epoch: 7, Index: 107, Loss: 1.0913\n",
      "Epoch: 7, Index: 108, Loss: 0.6876\n",
      "Epoch: 7, Index: 109, Loss: 0.9879\n",
      "Epoch: 7, Index: 110, Loss: 2.5365\n",
      "Epoch: 7, Index: 111, Loss: 1.3838\n",
      "Epoch: 7, Index: 112, Loss: 4.0393\n",
      "Epoch: 7, Index: 113, Loss: 1.6781\n",
      "Epoch: 7, Index: 114, Loss: 2.1921\n",
      "Epoch: 7, Index: 115, Loss: 2.0320\n",
      "Epoch: 7, Index: 116, Loss: 0.3492\n",
      "Epoch: 7, Index: 117, Loss: 1.6983\n",
      "Epoch: 7, Index: 118, Loss: 0.1515\n",
      "Epoch: 7, Index: 119, Loss: 2.2103\n",
      "Epoch: 7, Index: 120, Loss: 0.0768\n",
      "Epoch: 7, Index: 121, Loss: 1.2671\n",
      "Epoch: 7, Index: 122, Loss: 0.4449\n",
      "Epoch: 7, Index: 123, Loss: 1.0728\n",
      "Epoch: 7, Index: 124, Loss: 1.5845\n",
      "Epoch: 7, Index: 125, Loss: 1.2453\n",
      "Epoch: 7, Index: 126, Loss: 3.5129\n",
      "Epoch: 7, Index: 127, Loss: 0.6837\n",
      "Epoch: 7, Index: 128, Loss: 0.8357\n",
      "Epoch: 7, Index: 129, Loss: 1.1078\n",
      "Epoch: 7, Index: 130, Loss: 1.0643\n",
      "Epoch: 7, Index: 131, Loss: 3.7201\n",
      "Epoch: 7, Index: 132, Loss: 1.3353\n",
      "Epoch: 7, Index: 133, Loss: 0.7564\n",
      "Epoch: 7, Index: 134, Loss: 1.6722\n",
      "Epoch: 7, Index: 135, Loss: 0.4245\n",
      "Epoch: 7, Index: 136, Loss: 0.7764\n",
      "Epoch: 7, Index: 137, Loss: 2.0054\n",
      "Epoch: 7, Index: 138, Loss: 0.8322\n",
      "Epoch: 7, Index: 139, Loss: 0.4975\n",
      "Epoch: 7, Index: 140, Loss: 3.9617\n",
      "Epoch: 7, Index: 141, Loss: 0.9189\n",
      "Epoch: 7, Index: 142, Loss: 0.2394\n",
      "Epoch: 7, Index: 143, Loss: 2.1904\n",
      "Epoch: 7, Index: 144, Loss: 1.7699\n",
      "Epoch: 7, Index: 145, Loss: 0.2472\n",
      "Epoch: 7, Index: 146, Loss: 4.2333\n",
      "Epoch: 7, Index: 147, Loss: 4.6419\n",
      "Epoch: 7, Index: 148, Loss: 1.2908\n",
      "Epoch: 7, Index: 149, Loss: 7.4492\n",
      "Epoch: 7, Index: 150, Loss: 0.8193\n",
      "Epoch: 7, Index: 151, Loss: 1.5748\n",
      "Epoch: 7, Index: 152, Loss: 2.2828\n",
      "Epoch: 7, Index: 153, Loss: 4.5009\n",
      "Epoch: 7, Index: 154, Loss: 1.6998\n",
      "Epoch: 7, Index: 155, Loss: 1.0150\n",
      "Epoch: 7, Index: 156, Loss: 0.1180\n",
      "Epoch: 7, Index: 157, Loss: 2.4860\n",
      "Epoch: 7, Index: 158, Loss: 0.0003\n",
      "Epoch: 7, Index: 159, Loss: 6.9690\n",
      "Epoch: 7, Index: 160, Loss: 6.5234\n",
      "Epoch: 7, Index: 161, Loss: 0.0176\n",
      "Epoch: 7, Index: 162, Loss: 0.8453\n",
      "Epoch: 7, Index: 163, Loss: 0.2161\n",
      "Epoch: 7, Index: 164, Loss: 4.7646\n",
      "Epoch: 7, Index: 165, Loss: 0.8545\n",
      "Epoch: 7, Index: 166, Loss: 2.3853\n",
      "Epoch: 7, Index: 167, Loss: 0.2866\n",
      "Epoch: 7, Index: 168, Loss: 0.9755\n",
      "Epoch: 7, Index: 169, Loss: 1.9118\n",
      "Epoch: 7, Index: 170, Loss: 0.2532\n",
      "Epoch: 7, Index: 171, Loss: 0.2777\n",
      "Epoch: 7, Index: 172, Loss: 1.6987\n",
      "Epoch: 7, Index: 173, Loss: 2.4525\n",
      "Epoch: 7, Index: 174, Loss: 2.7449\n",
      "Epoch: 7, Index: 175, Loss: 9.2450\n",
      "Epoch: 7, Index: 176, Loss: 2.6928\n",
      "Epoch: 7, Index: 177, Loss: 1.5482\n",
      "Epoch: 7, Index: 178, Loss: 0.3840\n",
      "Epoch: 7, Index: 179, Loss: 4.4085\n",
      "Epoch: 7, Index: 180, Loss: 6.2909\n",
      "Epoch: 7, Index: 181, Loss: 9.5581\n",
      "Epoch: 7, Index: 182, Loss: 2.4213\n",
      "Epoch: 7, Index: 183, Loss: 1.8102\n",
      "Epoch: 7, Index: 184, Loss: 3.3102\n",
      "Epoch: 7, Index: 185, Loss: 2.9516\n",
      "Epoch: 7, Index: 186, Loss: 0.3586\n",
      "Epoch: 7, Index: 187, Loss: 0.4884\n",
      "Epoch: 7, Index: 188, Loss: 0.5180\n",
      "Epoch: 7, Index: 189, Loss: 2.4637\n",
      "Epoch: 7, Index: 190, Loss: 1.7243\n",
      "Epoch: 7, Index: 191, Loss: 0.6842\n",
      "Epoch: 7, Index: 192, Loss: 3.6668\n",
      "Epoch: 7, Index: 193, Loss: 6.0304\n",
      "Epoch: 7, Index: 194, Loss: 2.7797\n",
      "Epoch: 7, Index: 195, Loss: 4.1587\n",
      "Epoch: 7, Index: 196, Loss: 1.2886\n",
      "Epoch: 7, Index: 197, Loss: 0.4597\n",
      "Epoch: 7, Index: 198, Loss: 0.6584\n",
      "Epoch: 7, Index: 199, Loss: 1.9997\n",
      "Epoch: 7, Index: 200, Loss: 0.2608\n",
      "Epoch: 7, Index: 201, Loss: 0.7679\n",
      "Epoch: 7, Index: 202, Loss: 2.8564\n",
      "Epoch: 7, Index: 203, Loss: 0.4655\n",
      "Epoch: 7, Index: 204, Loss: 1.3677\n",
      "Epoch: 7, Index: 205, Loss: 0.0822\n",
      "Epoch: 7, Index: 206, Loss: 1.7719\n",
      "Epoch: 7, Index: 207, Loss: 1.0454\n",
      "Epoch: 7, Index: 208, Loss: 0.2882\n",
      "Epoch: 7, Index: 209, Loss: 3.5213\n",
      "Epoch: 7, Index: 210, Loss: 0.3090\n",
      "Epoch: 7, Index: 211, Loss: 0.1306\n",
      "Epoch: 7, Index: 212, Loss: 4.2753\n",
      "Epoch: 7, Index: 213, Loss: 16.6079\n",
      "Epoch: 7, Index: 214, Loss: 5.6776\n",
      "Epoch: 7, Index: 215, Loss: 0.7008\n",
      "Epoch: 7, Index: 216, Loss: 1.4606\n",
      "Epoch: 7, Index: 217, Loss: 2.4966\n",
      "Epoch: 7, Index: 218, Loss: 1.2563\n",
      "Epoch: 7, Index: 219, Loss: 0.1699\n",
      "Epoch: 7, Index: 220, Loss: 2.2941\n",
      "Epoch: 7, Index: 221, Loss: 0.2774\n",
      "Epoch: 7, Index: 222, Loss: 1.5751\n",
      "Epoch: 7, Index: 223, Loss: 0.0334\n",
      "Epoch: 7, Index: 224, Loss: 0.7554\n",
      "Epoch: 7, Index: 225, Loss: 0.4745\n",
      "Epoch: 7, Index: 226, Loss: 0.2853\n",
      "Epoch: 7, Index: 227, Loss: 8.2999\n",
      "Epoch: 7, Index: 228, Loss: 0.4224\n",
      "Epoch: 7, Index: 229, Loss: 1.2952\n",
      "Epoch: 7, Index: 230, Loss: 1.9878\n",
      "Epoch: 7, Index: 231, Loss: 1.3613\n",
      "Epoch: 7, Index: 232, Loss: 2.1743\n",
      "Epoch: 7, Index: 233, Loss: 2.0020\n",
      "Epoch: 7, Index: 234, Loss: 2.2239\n",
      "Epoch: 7, Index: 235, Loss: 1.5604\n",
      "Epoch: 7, Index: 236, Loss: 1.0642\n",
      "Epoch: 7, Index: 237, Loss: 2.0488\n",
      "Epoch: 7, Index: 238, Loss: 1.3513\n",
      "Epoch: 7, Index: 239, Loss: 4.0308\n",
      "Epoch: 7, Index: 240, Loss: 10.1884\n",
      "Epoch: 7, Index: 241, Loss: 0.1948\n",
      "Epoch: 7, Index: 242, Loss: 0.8273\n",
      "Epoch: 7, Index: 243, Loss: 2.9282\n",
      "Epoch: 7, Index: 244, Loss: 0.2559\n",
      "Epoch: 7, Index: 245, Loss: 0.3360\n",
      "Epoch: 7, Index: 246, Loss: 0.1719\n",
      "Epoch: 7, Index: 247, Loss: 1.7443\n",
      "Epoch: 7, Index: 248, Loss: 0.3121\n",
      "Epoch: 7, Index: 249, Loss: 3.0414\n",
      "Epoch: 7, Index: 250, Loss: 1.5580\n",
      "Epoch: 7, Index: 251, Loss: 0.7070\n",
      "Epoch: 7, Index: 252, Loss: 0.3068\n",
      "Epoch: 7, Index: 253, Loss: 0.5613\n",
      "Epoch: 7, Index: 254, Loss: 3.7065\n",
      "Epoch: 7, Index: 255, Loss: 1.8924\n",
      "Epoch: 7, Index: 256, Loss: 0.5559\n",
      "Epoch: 7, Index: 257, Loss: 1.1108\n",
      "Epoch: 7, Index: 258, Loss: 0.4001\n",
      "Epoch: 7, Index: 259, Loss: 0.9156\n",
      "Epoch: 7, Index: 260, Loss: 0.1670\n",
      "Epoch: 7, Index: 261, Loss: 3.6064\n",
      "Epoch: 7, Index: 262, Loss: 0.5990\n",
      "Epoch: 7, Index: 263, Loss: 0.8900\n",
      "Epoch: 7, Index: 264, Loss: 0.5458\n",
      "Epoch: 7, Index: 265, Loss: 1.9974\n",
      "Epoch: 7, Index: 266, Loss: 4.1440\n",
      "Epoch: 7, Index: 267, Loss: 5.4028\n",
      "Epoch: 7, Index: 268, Loss: 0.5054\n",
      "Epoch: 7, Index: 269, Loss: 4.9909\n",
      "Epoch: 7, Index: 270, Loss: 2.3240\n",
      "Epoch: 7, Index: 271, Loss: 2.9576\n",
      "Epoch: 7, Index: 272, Loss: 0.3390\n",
      "Epoch: 7, Index: 273, Loss: 5.0209\n",
      "Epoch: 7, Index: 274, Loss: 0.1027\n",
      "Epoch: 7, Index: 275, Loss: 3.1990\n",
      "Epoch: 7, Index: 276, Loss: 0.7150\n",
      "Epoch: 7, Index: 277, Loss: 0.2542\n",
      "Epoch: 7, Index: 278, Loss: 2.9399\n",
      "Epoch: 7, Index: 279, Loss: 0.8756\n",
      "Epoch: 7, Index: 280, Loss: 3.8815\n",
      "Epoch: 7, Index: 281, Loss: 1.5960\n",
      "Epoch: 7, Index: 282, Loss: 1.6464\n",
      "Epoch: 7, Index: 283, Loss: 4.5758\n",
      "Epoch: 7, Index: 284, Loss: 2.1549\n",
      "Epoch: 7, Index: 285, Loss: 0.8568\n",
      "Epoch: 7, Index: 286, Loss: 0.0372\n",
      "Epoch: 7, Index: 287, Loss: 0.7634\n",
      "Epoch: 7, Index: 288, Loss: 0.5813\n",
      "Epoch: 7, Index: 289, Loss: 3.0130\n",
      "Epoch: 7, Index: 290, Loss: 0.4888\n",
      "Epoch: 7, Index: 291, Loss: 0.6991\n",
      "Epoch: 7, Index: 292, Loss: 2.4587\n",
      "Epoch: 7, Index: 293, Loss: 1.8554\n",
      "Epoch: 7, Index: 294, Loss: 3.2170\n",
      "Epoch: 7, Index: 295, Loss: 0.9875\n",
      "Epoch: 7, Index: 296, Loss: 2.4137\n",
      "Epoch: 7, Index: 297, Loss: 1.2875\n",
      "Epoch: 7, Index: 298, Loss: 15.8321\n",
      "Epoch: 7, Index: 299, Loss: 0.6679\n",
      "Epoch: 7, Index: 300, Loss: 4.3133\n",
      "Epoch: 7, Index: 301, Loss: 2.5714\n",
      "Epoch: 7, Index: 302, Loss: 5.9121\n",
      "Epoch: 7, Index: 303, Loss: 1.6331\n",
      "Epoch: 7, Index: 304, Loss: 1.5959\n",
      "Epoch: 7, Index: 305, Loss: 0.6676\n",
      "Epoch: 7, Index: 306, Loss: 0.0706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6115173a7eea48d4b46248814bd1f2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fc96a8e3b14d6f80abb9b341bcdc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Index: 0, Loss: 3.5552\n",
      "Epoch: 8, Index: 1, Loss: 0.0135\n",
      "Epoch: 8, Index: 2, Loss: 3.9035\n",
      "Epoch: 8, Index: 3, Loss: 0.9293\n",
      "Epoch: 8, Index: 4, Loss: 0.2636\n",
      "Epoch: 8, Index: 5, Loss: 1.1982\n",
      "Epoch: 8, Index: 6, Loss: 0.0637\n",
      "Epoch: 8, Index: 7, Loss: 2.2893\n",
      "Epoch: 8, Index: 8, Loss: 0.1531\n",
      "Epoch: 8, Index: 9, Loss: 1.0965\n",
      "Epoch: 8, Index: 10, Loss: 1.6870\n",
      "Epoch: 8, Index: 11, Loss: 1.7338\n",
      "Epoch: 8, Index: 12, Loss: 0.5416\n",
      "Epoch: 8, Index: 13, Loss: 6.2569\n",
      "Epoch: 8, Index: 14, Loss: 3.5696\n",
      "Epoch: 8, Index: 15, Loss: 3.7975\n",
      "Epoch: 8, Index: 16, Loss: 2.2557\n",
      "Epoch: 8, Index: 17, Loss: 1.8187\n",
      "Epoch: 8, Index: 18, Loss: 2.0146\n",
      "Epoch: 8, Index: 19, Loss: 2.7413\n",
      "Epoch: 8, Index: 20, Loss: 0.4814\n",
      "Epoch: 8, Index: 21, Loss: 4.9832\n",
      "Epoch: 8, Index: 22, Loss: 1.2779\n",
      "Epoch: 8, Index: 23, Loss: 12.2485\n",
      "Epoch: 8, Index: 24, Loss: 0.5613\n",
      "Epoch: 8, Index: 25, Loss: 5.3556\n",
      "Epoch: 8, Index: 26, Loss: 0.1004\n",
      "Epoch: 8, Index: 27, Loss: 7.3297\n",
      "Epoch: 8, Index: 28, Loss: 1.1380\n",
      "Epoch: 8, Index: 29, Loss: 0.1797\n",
      "Epoch: 8, Index: 30, Loss: 3.3989\n",
      "Epoch: 8, Index: 31, Loss: 2.2527\n",
      "Epoch: 8, Index: 32, Loss: 8.3726\n",
      "Epoch: 8, Index: 33, Loss: 0.2610\n",
      "Epoch: 8, Index: 34, Loss: 2.3020\n",
      "Epoch: 8, Index: 35, Loss: 1.4368\n",
      "Epoch: 8, Index: 36, Loss: 1.1789\n",
      "Epoch: 8, Index: 37, Loss: 0.8551\n",
      "Epoch: 8, Index: 38, Loss: 14.1769\n",
      "Epoch: 8, Index: 39, Loss: 1.9477\n",
      "Epoch: 8, Index: 40, Loss: 0.2337\n",
      "Epoch: 8, Index: 41, Loss: 2.7350\n",
      "Epoch: 8, Index: 42, Loss: 1.2217\n",
      "Epoch: 8, Index: 43, Loss: 5.0125\n",
      "Epoch: 8, Index: 44, Loss: 3.4446\n",
      "Epoch: 8, Index: 45, Loss: 4.4456\n",
      "Epoch: 8, Index: 46, Loss: 0.3292\n",
      "Epoch: 8, Index: 47, Loss: 0.7823\n",
      "Epoch: 8, Index: 48, Loss: 1.9467\n",
      "Epoch: 8, Index: 49, Loss: 2.5488\n",
      "Epoch: 8, Index: 50, Loss: 1.0276\n",
      "Epoch: 8, Index: 51, Loss: 1.7601\n",
      "Epoch: 8, Index: 52, Loss: 1.5935\n",
      "Epoch: 8, Index: 53, Loss: 0.4062\n",
      "Epoch: 8, Index: 54, Loss: 0.0582\n",
      "Epoch: 8, Index: 55, Loss: 0.1681\n",
      "Epoch: 8, Index: 56, Loss: 2.0361\n",
      "Epoch: 8, Index: 57, Loss: 2.7164\n",
      "Epoch: 8, Index: 58, Loss: 2.9024\n",
      "Epoch: 8, Index: 59, Loss: 0.6801\n",
      "Epoch: 8, Index: 60, Loss: 2.4134\n",
      "Epoch: 8, Index: 61, Loss: 4.2174\n",
      "Epoch: 8, Index: 62, Loss: 0.6455\n",
      "Epoch: 8, Index: 63, Loss: 1.4985\n",
      "Epoch: 8, Index: 64, Loss: 8.2132\n",
      "Epoch: 8, Index: 65, Loss: 0.4770\n",
      "Epoch: 8, Index: 66, Loss: 0.9739\n",
      "Epoch: 8, Index: 67, Loss: 0.5162\n",
      "Epoch: 8, Index: 68, Loss: 0.1742\n",
      "Epoch: 8, Index: 69, Loss: 0.2139\n",
      "Epoch: 8, Index: 70, Loss: 1.7558\n",
      "Epoch: 8, Index: 71, Loss: 0.2009\n",
      "Epoch: 8, Index: 72, Loss: 4.0891\n",
      "Epoch: 8, Index: 73, Loss: 6.4829\n",
      "Epoch: 8, Index: 74, Loss: 3.0803\n",
      "Epoch: 8, Index: 75, Loss: 0.5959\n",
      "Epoch: 8, Index: 76, Loss: 1.5328\n",
      "Epoch: 8, Index: 77, Loss: 0.7734\n",
      "Epoch: 8, Index: 78, Loss: 0.5042\n",
      "Epoch: 8, Index: 79, Loss: 0.0592\n",
      "Epoch: 8, Index: 80, Loss: 0.3891\n",
      "Epoch: 8, Index: 81, Loss: 0.7504\n",
      "Epoch: 8, Index: 82, Loss: 2.4815\n",
      "Epoch: 8, Index: 83, Loss: 0.0192\n",
      "Epoch: 8, Index: 84, Loss: 2.0728\n",
      "Epoch: 8, Index: 85, Loss: 0.8886\n",
      "Epoch: 8, Index: 86, Loss: 0.1990\n",
      "Epoch: 8, Index: 87, Loss: 1.4929\n",
      "Epoch: 8, Index: 88, Loss: 1.6910\n",
      "Epoch: 8, Index: 89, Loss: 3.1425\n",
      "Epoch: 8, Index: 90, Loss: 0.7564\n",
      "Epoch: 8, Index: 91, Loss: 0.5732\n",
      "Epoch: 8, Index: 92, Loss: 0.1953\n",
      "Epoch: 8, Index: 93, Loss: 0.9118\n",
      "Epoch: 8, Index: 94, Loss: 1.5588\n",
      "Epoch: 8, Index: 95, Loss: 2.6726\n",
      "Epoch: 8, Index: 96, Loss: 0.2724\n",
      "Epoch: 8, Index: 97, Loss: 2.1697\n",
      "Epoch: 8, Index: 98, Loss: 6.5735\n",
      "Epoch: 8, Index: 99, Loss: 1.3255\n",
      "Epoch: 8, Index: 100, Loss: 1.6375\n",
      "Epoch: 8, Index: 101, Loss: 0.1422\n",
      "Epoch: 8, Index: 102, Loss: 1.4980\n",
      "Epoch: 8, Index: 103, Loss: 4.0816\n",
      "Epoch: 8, Index: 104, Loss: 2.7892\n",
      "Epoch: 8, Index: 105, Loss: 0.7248\n",
      "Epoch: 8, Index: 106, Loss: 5.1945\n",
      "Epoch: 8, Index: 107, Loss: 4.6155\n",
      "Epoch: 8, Index: 108, Loss: 3.0794\n",
      "Epoch: 8, Index: 109, Loss: 0.2819\n",
      "Epoch: 8, Index: 110, Loss: 0.4260\n",
      "Epoch: 8, Index: 111, Loss: 2.7307\n",
      "Epoch: 8, Index: 112, Loss: 2.0860\n",
      "Epoch: 8, Index: 113, Loss: 1.5925\n",
      "Epoch: 8, Index: 114, Loss: 2.2066\n",
      "Epoch: 8, Index: 115, Loss: 1.8579\n",
      "Epoch: 8, Index: 116, Loss: 0.8563\n",
      "Epoch: 8, Index: 117, Loss: 1.2067\n",
      "Epoch: 8, Index: 118, Loss: 1.3835\n",
      "Epoch: 8, Index: 119, Loss: 2.6940\n",
      "Epoch: 8, Index: 120, Loss: 4.6781\n",
      "Epoch: 8, Index: 121, Loss: 3.1024\n",
      "Epoch: 8, Index: 122, Loss: 0.1452\n",
      "Epoch: 8, Index: 123, Loss: 1.6991\n",
      "Epoch: 8, Index: 124, Loss: 2.1703\n",
      "Epoch: 8, Index: 125, Loss: 0.0221\n",
      "Epoch: 8, Index: 126, Loss: 1.0109\n",
      "Epoch: 8, Index: 127, Loss: 1.0556\n",
      "Epoch: 8, Index: 128, Loss: 7.3486\n",
      "Epoch: 8, Index: 129, Loss: 0.4476\n",
      "Epoch: 8, Index: 130, Loss: 2.4449\n",
      "Epoch: 8, Index: 131, Loss: 0.6700\n",
      "Epoch: 8, Index: 132, Loss: 0.0861\n",
      "Epoch: 8, Index: 133, Loss: 0.3389\n",
      "Epoch: 8, Index: 134, Loss: 0.6417\n",
      "Epoch: 8, Index: 135, Loss: 0.5148\n",
      "Epoch: 8, Index: 136, Loss: 3.5558\n",
      "Epoch: 8, Index: 137, Loss: 0.3590\n",
      "Epoch: 8, Index: 138, Loss: 1.8762\n",
      "Epoch: 8, Index: 139, Loss: 0.1668\n",
      "Epoch: 8, Index: 140, Loss: 1.7769\n",
      "Epoch: 8, Index: 141, Loss: 1.0310\n",
      "Epoch: 8, Index: 142, Loss: 0.0476\n",
      "Epoch: 8, Index: 143, Loss: 2.4578\n",
      "Epoch: 8, Index: 144, Loss: 1.6792\n",
      "Epoch: 8, Index: 145, Loss: 3.5902\n",
      "Epoch: 8, Index: 146, Loss: 0.4117\n",
      "Epoch: 8, Index: 147, Loss: 0.9682\n",
      "Epoch: 8, Index: 148, Loss: 1.7234\n",
      "Epoch: 8, Index: 149, Loss: 2.1149\n",
      "Epoch: 8, Index: 150, Loss: 0.2081\n",
      "Epoch: 8, Index: 151, Loss: 0.3079\n",
      "Epoch: 8, Index: 152, Loss: 1.6792\n",
      "Epoch: 8, Index: 153, Loss: 2.2565\n",
      "Epoch: 8, Index: 154, Loss: 0.5516\n",
      "Epoch: 8, Index: 155, Loss: 0.0869\n",
      "Epoch: 8, Index: 156, Loss: 1.4119\n",
      "Epoch: 8, Index: 157, Loss: 4.6294\n",
      "Epoch: 8, Index: 158, Loss: 3.4747\n",
      "Epoch: 8, Index: 159, Loss: 1.0992\n",
      "Epoch: 8, Index: 160, Loss: 0.0267\n",
      "Epoch: 8, Index: 161, Loss: 9.4325\n",
      "Epoch: 8, Index: 162, Loss: 0.8752\n",
      "Epoch: 8, Index: 163, Loss: 3.0594\n",
      "Epoch: 8, Index: 164, Loss: 5.7056\n",
      "Epoch: 8, Index: 165, Loss: 0.0693\n",
      "Epoch: 8, Index: 166, Loss: 4.1724\n",
      "Epoch: 8, Index: 167, Loss: 1.4106\n",
      "Epoch: 8, Index: 168, Loss: 1.1868\n",
      "Epoch: 8, Index: 169, Loss: 2.0629\n",
      "Epoch: 8, Index: 170, Loss: 3.2892\n",
      "Epoch: 8, Index: 171, Loss: 0.8423\n",
      "Epoch: 8, Index: 172, Loss: 1.4923\n",
      "Epoch: 8, Index: 173, Loss: 0.1444\n",
      "Epoch: 8, Index: 174, Loss: 0.4780\n",
      "Epoch: 8, Index: 175, Loss: 3.2946\n",
      "Epoch: 8, Index: 176, Loss: 0.3411\n",
      "Epoch: 8, Index: 177, Loss: 0.7160\n",
      "Epoch: 8, Index: 178, Loss: 2.5981\n",
      "Epoch: 8, Index: 179, Loss: 0.3420\n",
      "Epoch: 8, Index: 180, Loss: 3.0982\n",
      "Epoch: 8, Index: 181, Loss: 4.1329\n",
      "Epoch: 8, Index: 182, Loss: 1.4925\n",
      "Epoch: 8, Index: 183, Loss: 0.2911\n",
      "Epoch: 8, Index: 184, Loss: 0.1370\n",
      "Epoch: 8, Index: 185, Loss: 2.1964\n",
      "Epoch: 8, Index: 186, Loss: 4.5794\n",
      "Epoch: 8, Index: 187, Loss: 0.2074\n",
      "Epoch: 8, Index: 188, Loss: 0.2075\n",
      "Epoch: 8, Index: 189, Loss: 0.3126\n",
      "Epoch: 8, Index: 190, Loss: 1.0615\n",
      "Epoch: 8, Index: 191, Loss: 5.4896\n",
      "Epoch: 8, Index: 192, Loss: 0.8636\n",
      "Epoch: 8, Index: 193, Loss: 7.7761\n",
      "Epoch: 8, Index: 194, Loss: 0.1876\n",
      "Epoch: 8, Index: 195, Loss: 0.5623\n",
      "Epoch: 8, Index: 196, Loss: 2.3425\n",
      "Epoch: 8, Index: 197, Loss: 0.6586\n",
      "Epoch: 8, Index: 198, Loss: 1.7700\n",
      "Epoch: 8, Index: 199, Loss: 2.4072\n",
      "Epoch: 8, Index: 200, Loss: 0.6742\n",
      "Epoch: 8, Index: 201, Loss: 2.7063\n",
      "Epoch: 8, Index: 202, Loss: 0.0893\n",
      "Epoch: 8, Index: 203, Loss: 0.6856\n",
      "Epoch: 8, Index: 204, Loss: 0.0122\n",
      "Epoch: 8, Index: 205, Loss: 1.8686\n",
      "Epoch: 8, Index: 206, Loss: 0.6171\n",
      "Epoch: 8, Index: 207, Loss: 1.7483\n",
      "Epoch: 8, Index: 208, Loss: 0.7776\n",
      "Epoch: 8, Index: 209, Loss: 8.4929\n",
      "Epoch: 8, Index: 210, Loss: 3.1905\n",
      "Epoch: 8, Index: 211, Loss: 3.3522\n",
      "Epoch: 8, Index: 212, Loss: 0.8350\n",
      "Epoch: 8, Index: 213, Loss: 1.4994\n",
      "Epoch: 8, Index: 214, Loss: 1.5071\n",
      "Epoch: 8, Index: 215, Loss: 0.3904\n",
      "Epoch: 8, Index: 216, Loss: 1.8444\n",
      "Epoch: 8, Index: 217, Loss: 1.2628\n",
      "Epoch: 8, Index: 218, Loss: 0.4532\n",
      "Epoch: 8, Index: 219, Loss: 2.2963\n",
      "Epoch: 8, Index: 220, Loss: 1.3834\n",
      "Epoch: 8, Index: 221, Loss: 0.2826\n",
      "Epoch: 8, Index: 222, Loss: 1.3307\n",
      "Epoch: 8, Index: 223, Loss: 0.7581\n",
      "Epoch: 8, Index: 224, Loss: 3.8212\n",
      "Epoch: 8, Index: 225, Loss: 0.0557\n",
      "Epoch: 8, Index: 226, Loss: 4.8099\n",
      "Epoch: 8, Index: 227, Loss: 2.7208\n",
      "Epoch: 8, Index: 228, Loss: 0.7157\n",
      "Epoch: 8, Index: 229, Loss: 1.5641\n",
      "Epoch: 8, Index: 230, Loss: 4.4032\n",
      "Epoch: 8, Index: 231, Loss: 1.7803\n",
      "Epoch: 8, Index: 232, Loss: 1.4953\n",
      "Epoch: 8, Index: 233, Loss: 1.1894\n",
      "Epoch: 8, Index: 234, Loss: 0.8183\n",
      "Epoch: 8, Index: 235, Loss: 2.1045\n",
      "Epoch: 8, Index: 236, Loss: 8.9663\n",
      "Epoch: 8, Index: 237, Loss: 1.4697\n",
      "Epoch: 8, Index: 238, Loss: 2.5332\n",
      "Epoch: 8, Index: 239, Loss: 1.3610\n",
      "Epoch: 8, Index: 240, Loss: 1.1135\n",
      "Epoch: 8, Index: 241, Loss: 0.7935\n",
      "Epoch: 8, Index: 242, Loss: 0.7487\n",
      "Epoch: 8, Index: 243, Loss: 1.7666\n",
      "Epoch: 8, Index: 244, Loss: 2.4427\n",
      "Epoch: 8, Index: 245, Loss: 0.4685\n",
      "Epoch: 8, Index: 246, Loss: 1.1207\n",
      "Epoch: 8, Index: 247, Loss: 0.7938\n",
      "Epoch: 8, Index: 248, Loss: 1.7935\n",
      "Epoch: 8, Index: 249, Loss: 2.6894\n",
      "Epoch: 8, Index: 250, Loss: 1.7012\n",
      "Epoch: 8, Index: 251, Loss: 6.9530\n",
      "Epoch: 8, Index: 252, Loss: 1.7497\n",
      "Epoch: 8, Index: 253, Loss: 2.8921\n",
      "Epoch: 8, Index: 254, Loss: 4.1183\n",
      "Epoch: 8, Index: 255, Loss: 1.7656\n",
      "Epoch: 8, Index: 256, Loss: 1.9958\n",
      "Epoch: 8, Index: 257, Loss: 0.4646\n",
      "Epoch: 8, Index: 258, Loss: 0.2702\n",
      "Epoch: 8, Index: 259, Loss: 0.4026\n",
      "Epoch: 8, Index: 260, Loss: 1.1971\n",
      "Epoch: 8, Index: 261, Loss: 0.2799\n",
      "Epoch: 8, Index: 262, Loss: 2.1299\n",
      "Epoch: 8, Index: 263, Loss: 0.9202\n",
      "Epoch: 8, Index: 264, Loss: 0.6732\n",
      "Epoch: 8, Index: 265, Loss: 4.0833\n",
      "Epoch: 8, Index: 266, Loss: 0.4021\n",
      "Epoch: 8, Index: 267, Loss: 0.5620\n",
      "Epoch: 8, Index: 268, Loss: 2.5324\n",
      "Epoch: 8, Index: 269, Loss: 0.2803\n",
      "Epoch: 8, Index: 270, Loss: 6.4464\n",
      "Epoch: 8, Index: 271, Loss: 3.0914\n",
      "Epoch: 8, Index: 272, Loss: 0.1779\n",
      "Epoch: 8, Index: 273, Loss: 1.5669\n",
      "Epoch: 8, Index: 274, Loss: 0.3271\n",
      "Epoch: 8, Index: 275, Loss: 1.8071\n",
      "Epoch: 8, Index: 276, Loss: 1.5711\n",
      "Epoch: 8, Index: 277, Loss: 0.8853\n",
      "Epoch: 8, Index: 278, Loss: 0.1671\n",
      "Epoch: 8, Index: 279, Loss: 3.0999\n",
      "Epoch: 8, Index: 280, Loss: 0.1991\n",
      "Epoch: 8, Index: 281, Loss: 0.8171\n",
      "Epoch: 8, Index: 282, Loss: 0.1782\n",
      "Epoch: 8, Index: 283, Loss: 1.3064\n",
      "Epoch: 8, Index: 284, Loss: 0.3473\n",
      "Epoch: 8, Index: 285, Loss: 10.6595\n",
      "Epoch: 8, Index: 286, Loss: 0.3823\n",
      "Epoch: 8, Index: 287, Loss: 2.0023\n",
      "Epoch: 8, Index: 288, Loss: 1.0191\n",
      "Epoch: 8, Index: 289, Loss: 1.3677\n",
      "Epoch: 8, Index: 290, Loss: 0.3823\n",
      "Epoch: 8, Index: 291, Loss: 0.0103\n",
      "Epoch: 8, Index: 292, Loss: 0.3255\n",
      "Epoch: 8, Index: 293, Loss: 2.2638\n",
      "Epoch: 8, Index: 294, Loss: 5.7992\n",
      "Epoch: 8, Index: 295, Loss: 3.3646\n",
      "Epoch: 8, Index: 296, Loss: 0.9624\n",
      "Epoch: 8, Index: 297, Loss: 5.3235\n",
      "Epoch: 8, Index: 298, Loss: 1.0027\n",
      "Epoch: 8, Index: 299, Loss: 0.4398\n",
      "Epoch: 8, Index: 300, Loss: 1.5981\n",
      "Epoch: 8, Index: 301, Loss: 2.2161\n",
      "Epoch: 8, Index: 302, Loss: 1.6478\n",
      "Epoch: 8, Index: 303, Loss: 0.5482\n",
      "Epoch: 8, Index: 304, Loss: 2.1437\n",
      "Epoch: 8, Index: 305, Loss: 0.9968\n",
      "Epoch: 8, Index: 306, Loss: 0.4243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3a555a1f56418fb9da451e817febd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebbd31f380b4c108e8b48d62abe4688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Index: 0, Loss: 1.4561\n",
      "Epoch: 9, Index: 1, Loss: 1.0739\n",
      "Epoch: 9, Index: 2, Loss: 3.3407\n",
      "Epoch: 9, Index: 3, Loss: 0.3828\n",
      "Epoch: 9, Index: 4, Loss: 5.9343\n",
      "Epoch: 9, Index: 5, Loss: 0.6112\n",
      "Epoch: 9, Index: 6, Loss: 4.1691\n",
      "Epoch: 9, Index: 7, Loss: 0.1783\n",
      "Epoch: 9, Index: 8, Loss: 0.0359\n",
      "Epoch: 9, Index: 9, Loss: 1.1282\n",
      "Epoch: 9, Index: 10, Loss: 0.6554\n",
      "Epoch: 9, Index: 11, Loss: 4.1333\n",
      "Epoch: 9, Index: 12, Loss: 0.1173\n",
      "Epoch: 9, Index: 13, Loss: 0.1071\n",
      "Epoch: 9, Index: 14, Loss: 0.3042\n",
      "Epoch: 9, Index: 15, Loss: 0.2783\n",
      "Epoch: 9, Index: 16, Loss: 5.0992\n",
      "Epoch: 9, Index: 17, Loss: 1.4588\n",
      "Epoch: 9, Index: 18, Loss: 0.1195\n",
      "Epoch: 9, Index: 19, Loss: 0.7658\n",
      "Epoch: 9, Index: 20, Loss: 1.5260\n",
      "Epoch: 9, Index: 21, Loss: 1.1013\n",
      "Epoch: 9, Index: 22, Loss: 0.7585\n",
      "Epoch: 9, Index: 23, Loss: 1.7943\n",
      "Epoch: 9, Index: 24, Loss: 0.6666\n",
      "Epoch: 9, Index: 25, Loss: 3.7988\n",
      "Epoch: 9, Index: 26, Loss: 0.2462\n",
      "Epoch: 9, Index: 27, Loss: 3.9702\n",
      "Epoch: 9, Index: 28, Loss: 1.5302\n",
      "Epoch: 9, Index: 29, Loss: 0.7163\n",
      "Epoch: 9, Index: 30, Loss: 0.2906\n",
      "Epoch: 9, Index: 31, Loss: 6.7914\n",
      "Epoch: 9, Index: 32, Loss: 1.1109\n",
      "Epoch: 9, Index: 33, Loss: 1.4339\n",
      "Epoch: 9, Index: 34, Loss: 0.0811\n",
      "Epoch: 9, Index: 35, Loss: 0.9981\n",
      "Epoch: 9, Index: 36, Loss: 1.7169\n",
      "Epoch: 9, Index: 37, Loss: 2.3448\n",
      "Epoch: 9, Index: 38, Loss: 4.2739\n",
      "Epoch: 9, Index: 39, Loss: 0.6544\n",
      "Epoch: 9, Index: 40, Loss: 1.2215\n",
      "Epoch: 9, Index: 41, Loss: 0.2291\n",
      "Epoch: 9, Index: 42, Loss: 1.4979\n",
      "Epoch: 9, Index: 43, Loss: 1.2294\n",
      "Epoch: 9, Index: 44, Loss: 2.6183\n",
      "Epoch: 9, Index: 45, Loss: 1.2879\n",
      "Epoch: 9, Index: 46, Loss: 6.2913\n",
      "Epoch: 9, Index: 47, Loss: 0.4745\n",
      "Epoch: 9, Index: 48, Loss: 0.8160\n",
      "Epoch: 9, Index: 49, Loss: 1.1018\n",
      "Epoch: 9, Index: 50, Loss: 1.6102\n",
      "Epoch: 9, Index: 51, Loss: 2.5119\n",
      "Epoch: 9, Index: 52, Loss: 2.2204\n",
      "Epoch: 9, Index: 53, Loss: 0.5356\n",
      "Epoch: 9, Index: 54, Loss: 0.2073\n",
      "Epoch: 9, Index: 55, Loss: 0.7438\n",
      "Epoch: 9, Index: 56, Loss: 0.4869\n",
      "Epoch: 9, Index: 57, Loss: 10.7721\n",
      "Epoch: 9, Index: 58, Loss: 2.6998\n",
      "Epoch: 9, Index: 59, Loss: 3.8827\n",
      "Epoch: 9, Index: 60, Loss: 5.3019\n",
      "Epoch: 9, Index: 61, Loss: 1.0652\n",
      "Epoch: 9, Index: 62, Loss: 2.1608\n",
      "Epoch: 9, Index: 63, Loss: 3.5560\n",
      "Epoch: 9, Index: 64, Loss: 0.8852\n",
      "Epoch: 9, Index: 65, Loss: 0.0804\n",
      "Epoch: 9, Index: 66, Loss: 2.2340\n",
      "Epoch: 9, Index: 67, Loss: 1.0162\n",
      "Epoch: 9, Index: 68, Loss: 0.5376\n",
      "Epoch: 9, Index: 69, Loss: 0.8000\n",
      "Epoch: 9, Index: 70, Loss: 1.4111\n",
      "Epoch: 9, Index: 71, Loss: 3.4520\n",
      "Epoch: 9, Index: 72, Loss: 1.5900\n",
      "Epoch: 9, Index: 73, Loss: 0.7054\n",
      "Epoch: 9, Index: 74, Loss: 5.1482\n",
      "Epoch: 9, Index: 75, Loss: 1.1900\n",
      "Epoch: 9, Index: 76, Loss: 5.0896\n",
      "Epoch: 9, Index: 77, Loss: 1.2323\n",
      "Epoch: 9, Index: 78, Loss: 0.3658\n",
      "Epoch: 9, Index: 79, Loss: 0.7339\n",
      "Epoch: 9, Index: 80, Loss: 3.1378\n",
      "Epoch: 9, Index: 81, Loss: 2.7709\n",
      "Epoch: 9, Index: 82, Loss: 1.3226\n",
      "Epoch: 9, Index: 83, Loss: 1.0403\n",
      "Epoch: 9, Index: 84, Loss: 1.5447\n",
      "Epoch: 9, Index: 85, Loss: 0.0205\n",
      "Epoch: 9, Index: 86, Loss: 1.0319\n",
      "Epoch: 9, Index: 87, Loss: 2.0817\n",
      "Epoch: 9, Index: 88, Loss: 0.2545\n",
      "Epoch: 9, Index: 89, Loss: 0.5093\n",
      "Epoch: 9, Index: 90, Loss: 3.1680\n",
      "Epoch: 9, Index: 91, Loss: 3.6892\n",
      "Epoch: 9, Index: 92, Loss: 0.2966\n",
      "Epoch: 9, Index: 93, Loss: 4.0943\n",
      "Epoch: 9, Index: 94, Loss: 5.2617\n",
      "Epoch: 9, Index: 95, Loss: 2.8722\n",
      "Epoch: 9, Index: 96, Loss: 8.2702\n",
      "Epoch: 9, Index: 97, Loss: 1.9241\n",
      "Epoch: 9, Index: 98, Loss: 1.7021\n",
      "Epoch: 9, Index: 99, Loss: 5.6530\n",
      "Epoch: 9, Index: 100, Loss: 0.6720\n",
      "Epoch: 9, Index: 101, Loss: 0.4197\n",
      "Epoch: 9, Index: 102, Loss: 0.3372\n",
      "Epoch: 9, Index: 103, Loss: 6.7386\n",
      "Epoch: 9, Index: 104, Loss: 0.6393\n",
      "Epoch: 9, Index: 105, Loss: 1.0488\n",
      "Epoch: 9, Index: 106, Loss: 18.6196\n",
      "Epoch: 9, Index: 107, Loss: 3.5745\n",
      "Epoch: 9, Index: 108, Loss: 1.4215\n",
      "Epoch: 9, Index: 109, Loss: 3.0455\n",
      "Epoch: 9, Index: 110, Loss: 1.9957\n",
      "Epoch: 9, Index: 111, Loss: 0.6001\n",
      "Epoch: 9, Index: 112, Loss: 0.4397\n",
      "Epoch: 9, Index: 113, Loss: 3.1141\n",
      "Epoch: 9, Index: 114, Loss: 1.3028\n",
      "Epoch: 9, Index: 115, Loss: 2.6451\n",
      "Epoch: 9, Index: 116, Loss: 0.9084\n",
      "Epoch: 9, Index: 117, Loss: 1.5232\n",
      "Epoch: 9, Index: 118, Loss: 0.2881\n",
      "Epoch: 9, Index: 119, Loss: 0.0212\n",
      "Epoch: 9, Index: 120, Loss: 0.7655\n",
      "Epoch: 9, Index: 121, Loss: 3.4256\n",
      "Epoch: 9, Index: 122, Loss: 0.1635\n",
      "Epoch: 9, Index: 123, Loss: 7.6590\n",
      "Epoch: 9, Index: 124, Loss: 3.7183\n",
      "Epoch: 9, Index: 125, Loss: 0.3044\n",
      "Epoch: 9, Index: 126, Loss: 1.6053\n",
      "Epoch: 9, Index: 127, Loss: 7.8813\n",
      "Epoch: 9, Index: 128, Loss: 0.2984\n",
      "Epoch: 9, Index: 129, Loss: 3.1175\n",
      "Epoch: 9, Index: 130, Loss: 3.7872\n",
      "Epoch: 9, Index: 131, Loss: 1.6026\n",
      "Epoch: 9, Index: 132, Loss: 1.1401\n",
      "Epoch: 9, Index: 133, Loss: 0.3558\n",
      "Epoch: 9, Index: 134, Loss: 2.1312\n",
      "Epoch: 9, Index: 135, Loss: 5.4543\n",
      "Epoch: 9, Index: 136, Loss: 0.5120\n",
      "Epoch: 9, Index: 137, Loss: 2.4816\n",
      "Epoch: 9, Index: 138, Loss: 3.1389\n",
      "Epoch: 9, Index: 139, Loss: 1.6946\n",
      "Epoch: 9, Index: 140, Loss: 0.5155\n",
      "Epoch: 9, Index: 141, Loss: 3.4291\n",
      "Epoch: 9, Index: 142, Loss: 3.6650\n",
      "Epoch: 9, Index: 143, Loss: 0.5869\n",
      "Epoch: 9, Index: 144, Loss: 1.1658\n",
      "Epoch: 9, Index: 145, Loss: 0.3443\n",
      "Epoch: 9, Index: 146, Loss: 0.1067\n",
      "Epoch: 9, Index: 147, Loss: 1.1933\n",
      "Epoch: 9, Index: 148, Loss: 3.5014\n",
      "Epoch: 9, Index: 149, Loss: 0.9283\n",
      "Epoch: 9, Index: 150, Loss: 2.0140\n",
      "Epoch: 9, Index: 151, Loss: 0.1445\n",
      "Epoch: 9, Index: 152, Loss: 1.6141\n",
      "Epoch: 9, Index: 153, Loss: 0.1190\n",
      "Epoch: 9, Index: 154, Loss: 1.1205\n",
      "Epoch: 9, Index: 155, Loss: 2.7677\n",
      "Epoch: 9, Index: 156, Loss: 0.2490\n",
      "Epoch: 9, Index: 157, Loss: 1.5083\n",
      "Epoch: 9, Index: 158, Loss: 0.1226\n",
      "Epoch: 9, Index: 159, Loss: 7.6361\n",
      "Epoch: 9, Index: 160, Loss: 1.2635\n",
      "Epoch: 9, Index: 161, Loss: 2.9303\n",
      "Epoch: 9, Index: 162, Loss: 1.2139\n",
      "Epoch: 9, Index: 163, Loss: 0.0418\n",
      "Epoch: 9, Index: 164, Loss: 0.9295\n",
      "Epoch: 9, Index: 165, Loss: 2.5033\n",
      "Epoch: 9, Index: 166, Loss: 1.9547\n",
      "Epoch: 9, Index: 167, Loss: 3.7070\n",
      "Epoch: 9, Index: 168, Loss: 0.8048\n",
      "Epoch: 9, Index: 169, Loss: 2.6459\n",
      "Epoch: 9, Index: 170, Loss: 2.4699\n",
      "Epoch: 9, Index: 171, Loss: 0.3016\n",
      "Epoch: 9, Index: 172, Loss: 0.2716\n",
      "Epoch: 9, Index: 173, Loss: 0.9289\n",
      "Epoch: 9, Index: 174, Loss: 4.9246\n",
      "Epoch: 9, Index: 175, Loss: 0.3538\n",
      "Epoch: 9, Index: 176, Loss: 1.0877\n",
      "Epoch: 9, Index: 177, Loss: 0.3767\n",
      "Epoch: 9, Index: 178, Loss: 1.1425\n",
      "Epoch: 9, Index: 179, Loss: 0.0099\n",
      "Epoch: 9, Index: 180, Loss: 0.6227\n",
      "Epoch: 9, Index: 181, Loss: 0.7133\n",
      "Epoch: 9, Index: 182, Loss: 1.8982\n",
      "Epoch: 9, Index: 183, Loss: 0.7780\n",
      "Epoch: 9, Index: 184, Loss: 5.1230\n",
      "Epoch: 9, Index: 185, Loss: 0.0673\n",
      "Epoch: 9, Index: 186, Loss: 0.8552\n",
      "Epoch: 9, Index: 187, Loss: 3.6852\n",
      "Epoch: 9, Index: 188, Loss: 3.8363\n",
      "Epoch: 9, Index: 189, Loss: 2.2641\n",
      "Epoch: 9, Index: 190, Loss: 0.6282\n",
      "Epoch: 9, Index: 191, Loss: 3.9192\n",
      "Epoch: 9, Index: 192, Loss: 1.0795\n",
      "Epoch: 9, Index: 193, Loss: 2.9190\n",
      "Epoch: 9, Index: 194, Loss: 2.1286\n",
      "Epoch: 9, Index: 195, Loss: 0.7321\n",
      "Epoch: 9, Index: 196, Loss: 2.4132\n",
      "Epoch: 9, Index: 197, Loss: 0.7522\n",
      "Epoch: 9, Index: 198, Loss: 0.1363\n",
      "Epoch: 9, Index: 199, Loss: 0.3855\n",
      "Epoch: 9, Index: 200, Loss: 0.2866\n",
      "Epoch: 9, Index: 201, Loss: 0.5699\n",
      "Epoch: 9, Index: 202, Loss: 8.9661\n",
      "Epoch: 9, Index: 203, Loss: 0.6470\n",
      "Epoch: 9, Index: 204, Loss: 1.8335\n",
      "Epoch: 9, Index: 205, Loss: 0.1627\n",
      "Epoch: 9, Index: 206, Loss: 2.8328\n",
      "Epoch: 9, Index: 207, Loss: 3.9531\n",
      "Epoch: 9, Index: 208, Loss: 0.3782\n",
      "Epoch: 9, Index: 209, Loss: 0.6339\n",
      "Epoch: 9, Index: 210, Loss: 0.3206\n",
      "Epoch: 9, Index: 211, Loss: 6.0734\n",
      "Epoch: 9, Index: 212, Loss: 0.0205\n",
      "Epoch: 9, Index: 213, Loss: 2.7693\n",
      "Epoch: 9, Index: 214, Loss: 2.0384\n",
      "Epoch: 9, Index: 215, Loss: 0.3615\n",
      "Epoch: 9, Index: 216, Loss: 4.0431\n",
      "Epoch: 9, Index: 217, Loss: 1.5077\n",
      "Epoch: 9, Index: 218, Loss: 0.7366\n",
      "Epoch: 9, Index: 219, Loss: 1.4548\n",
      "Epoch: 9, Index: 220, Loss: 2.9871\n",
      "Epoch: 9, Index: 221, Loss: 1.3257\n",
      "Epoch: 9, Index: 222, Loss: 2.3609\n",
      "Epoch: 9, Index: 223, Loss: 0.9186\n",
      "Epoch: 9, Index: 224, Loss: 1.2720\n",
      "Epoch: 9, Index: 225, Loss: 2.6271\n",
      "Epoch: 9, Index: 226, Loss: 0.0359\n",
      "Epoch: 9, Index: 227, Loss: 0.1926\n",
      "Epoch: 9, Index: 228, Loss: 1.6541\n",
      "Epoch: 9, Index: 229, Loss: 3.8791\n",
      "Epoch: 9, Index: 230, Loss: 1.5572\n",
      "Epoch: 9, Index: 231, Loss: 0.1921\n",
      "Epoch: 9, Index: 232, Loss: 0.3433\n",
      "Epoch: 9, Index: 233, Loss: 3.5559\n",
      "Epoch: 9, Index: 234, Loss: 0.3934\n",
      "Epoch: 9, Index: 235, Loss: 0.2289\n",
      "Epoch: 9, Index: 236, Loss: 1.4223\n",
      "Epoch: 9, Index: 237, Loss: 6.3379\n",
      "Epoch: 9, Index: 238, Loss: 2.1776\n",
      "Epoch: 9, Index: 239, Loss: 0.8515\n",
      "Epoch: 9, Index: 240, Loss: 2.8722\n",
      "Epoch: 9, Index: 241, Loss: 0.5744\n",
      "Epoch: 9, Index: 242, Loss: 0.4029\n",
      "Epoch: 9, Index: 243, Loss: 0.3872\n",
      "Epoch: 9, Index: 244, Loss: 0.2803\n",
      "Epoch: 9, Index: 245, Loss: 1.1675\n",
      "Epoch: 9, Index: 246, Loss: 0.0096\n",
      "Epoch: 9, Index: 247, Loss: 1.6811\n",
      "Epoch: 9, Index: 248, Loss: 1.5930\n",
      "Epoch: 9, Index: 249, Loss: 1.3121\n",
      "Epoch: 9, Index: 250, Loss: 0.2086\n",
      "Epoch: 9, Index: 251, Loss: 0.8616\n",
      "Epoch: 9, Index: 252, Loss: 0.6000\n",
      "Epoch: 9, Index: 253, Loss: 1.4870\n",
      "Epoch: 9, Index: 254, Loss: 0.0829\n",
      "Epoch: 9, Index: 255, Loss: 0.0164\n",
      "Epoch: 9, Index: 256, Loss: 2.7900\n",
      "Epoch: 9, Index: 257, Loss: 0.1251\n",
      "Epoch: 9, Index: 258, Loss: 1.2981\n",
      "Epoch: 9, Index: 259, Loss: 0.9583\n",
      "Epoch: 9, Index: 260, Loss: 1.4267\n",
      "Epoch: 9, Index: 261, Loss: 1.5482\n",
      "Epoch: 9, Index: 262, Loss: 2.6498\n",
      "Epoch: 9, Index: 263, Loss: 0.5867\n",
      "Epoch: 9, Index: 264, Loss: 1.8209\n",
      "Epoch: 9, Index: 265, Loss: 1.7626\n",
      "Epoch: 9, Index: 266, Loss: 0.5012\n",
      "Epoch: 9, Index: 267, Loss: 2.6021\n",
      "Epoch: 9, Index: 268, Loss: 19.7557\n",
      "Epoch: 9, Index: 269, Loss: 0.6119\n",
      "Epoch: 9, Index: 270, Loss: 2.1764\n",
      "Epoch: 9, Index: 271, Loss: 0.6446\n",
      "Epoch: 9, Index: 272, Loss: 1.4551\n",
      "Epoch: 9, Index: 273, Loss: 3.3178\n",
      "Epoch: 9, Index: 274, Loss: 4.9156\n",
      "Epoch: 9, Index: 275, Loss: 0.1918\n",
      "Epoch: 9, Index: 276, Loss: 1.0693\n",
      "Epoch: 9, Index: 277, Loss: 0.9629\n",
      "Epoch: 9, Index: 278, Loss: 0.1372\n",
      "Epoch: 9, Index: 279, Loss: 1.7118\n",
      "Epoch: 9, Index: 280, Loss: 1.3071\n",
      "Epoch: 9, Index: 281, Loss: 0.8115\n",
      "Epoch: 9, Index: 282, Loss: 2.4631\n",
      "Epoch: 9, Index: 283, Loss: 1.5799\n",
      "Epoch: 9, Index: 284, Loss: 0.0121\n",
      "Epoch: 9, Index: 285, Loss: 4.2741\n",
      "Epoch: 9, Index: 286, Loss: 1.0131\n",
      "Epoch: 9, Index: 287, Loss: 7.8113\n",
      "Epoch: 9, Index: 288, Loss: 2.2127\n",
      "Epoch: 9, Index: 289, Loss: 1.1826\n",
      "Epoch: 9, Index: 290, Loss: 2.7989\n",
      "Epoch: 9, Index: 291, Loss: 3.6854\n",
      "Epoch: 9, Index: 292, Loss: 1.2799\n",
      "Epoch: 9, Index: 293, Loss: 0.6509\n",
      "Epoch: 9, Index: 294, Loss: 1.1461\n",
      "Epoch: 9, Index: 295, Loss: 0.0732\n",
      "Epoch: 9, Index: 296, Loss: 8.2536\n",
      "Epoch: 9, Index: 297, Loss: 0.3834\n",
      "Epoch: 9, Index: 298, Loss: 1.0098\n",
      "Epoch: 9, Index: 299, Loss: 2.2223\n",
      "Epoch: 9, Index: 300, Loss: 0.2014\n",
      "Epoch: 9, Index: 301, Loss: 2.0997\n",
      "Epoch: 9, Index: 302, Loss: 1.3298\n",
      "Epoch: 9, Index: 303, Loss: 0.1119\n",
      "Epoch: 9, Index: 304, Loss: 2.4619\n",
      "Epoch: 9, Index: 305, Loss: 0.9395\n",
      "Epoch: 9, Index: 306, Loss: 3.8338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71069f19408f4f8893704bbd6ff88d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd16d33093084ff8bde5c7fabbdb68eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Index: 0, Loss: 0.2700\n",
      "Epoch: 10, Index: 1, Loss: 3.7155\n",
      "Epoch: 10, Index: 2, Loss: 0.3566\n",
      "Epoch: 10, Index: 3, Loss: 7.6319\n",
      "Epoch: 10, Index: 4, Loss: 2.6371\n",
      "Epoch: 10, Index: 5, Loss: 16.5236\n",
      "Epoch: 10, Index: 6, Loss: 3.2116\n",
      "Epoch: 10, Index: 7, Loss: 0.2724\n",
      "Epoch: 10, Index: 8, Loss: 0.2444\n",
      "Epoch: 10, Index: 9, Loss: 0.2560\n",
      "Epoch: 10, Index: 10, Loss: 2.0009\n",
      "Epoch: 10, Index: 11, Loss: 0.2403\n",
      "Epoch: 10, Index: 12, Loss: 3.2734\n",
      "Epoch: 10, Index: 13, Loss: 1.7839\n",
      "Epoch: 10, Index: 14, Loss: 0.1503\n",
      "Epoch: 10, Index: 15, Loss: 3.1935\n",
      "Epoch: 10, Index: 16, Loss: 2.7153\n",
      "Epoch: 10, Index: 17, Loss: 3.8039\n",
      "Epoch: 10, Index: 18, Loss: 0.3594\n",
      "Epoch: 10, Index: 19, Loss: 1.2317\n",
      "Epoch: 10, Index: 20, Loss: 1.3043\n",
      "Epoch: 10, Index: 21, Loss: 2.9155\n",
      "Epoch: 10, Index: 22, Loss: 4.2536\n",
      "Epoch: 10, Index: 23, Loss: 0.4853\n",
      "Epoch: 10, Index: 24, Loss: 1.1201\n",
      "Epoch: 10, Index: 25, Loss: 0.5473\n",
      "Epoch: 10, Index: 26, Loss: 0.8124\n",
      "Epoch: 10, Index: 27, Loss: 0.7939\n",
      "Epoch: 10, Index: 28, Loss: 2.1231\n",
      "Epoch: 10, Index: 29, Loss: 7.6063\n",
      "Epoch: 10, Index: 30, Loss: 2.8100\n",
      "Epoch: 10, Index: 31, Loss: 2.5921\n",
      "Epoch: 10, Index: 32, Loss: 1.8560\n",
      "Epoch: 10, Index: 33, Loss: 1.6686\n",
      "Epoch: 10, Index: 34, Loss: 1.0490\n",
      "Epoch: 10, Index: 35, Loss: 1.9928\n",
      "Epoch: 10, Index: 36, Loss: 4.7430\n",
      "Epoch: 10, Index: 37, Loss: 0.8789\n",
      "Epoch: 10, Index: 38, Loss: 3.0748\n",
      "Epoch: 10, Index: 39, Loss: 0.4329\n",
      "Epoch: 10, Index: 40, Loss: 3.6039\n",
      "Epoch: 10, Index: 41, Loss: 9.5382\n",
      "Epoch: 10, Index: 42, Loss: 1.4163\n",
      "Epoch: 10, Index: 43, Loss: 0.8172\n",
      "Epoch: 10, Index: 44, Loss: 2.3022\n",
      "Epoch: 10, Index: 45, Loss: 0.8649\n",
      "Epoch: 10, Index: 46, Loss: 2.1061\n",
      "Epoch: 10, Index: 47, Loss: 2.7018\n",
      "Epoch: 10, Index: 48, Loss: 0.2088\n",
      "Epoch: 10, Index: 49, Loss: 4.1613\n",
      "Epoch: 10, Index: 50, Loss: 2.3257\n",
      "Epoch: 10, Index: 51, Loss: 1.0475\n",
      "Epoch: 10, Index: 52, Loss: 2.4050\n",
      "Epoch: 10, Index: 53, Loss: 3.1731\n",
      "Epoch: 10, Index: 54, Loss: 1.3602\n",
      "Epoch: 10, Index: 55, Loss: 0.9541\n",
      "Epoch: 10, Index: 56, Loss: 0.2448\n",
      "Epoch: 10, Index: 57, Loss: 1.6055\n",
      "Epoch: 10, Index: 58, Loss: 0.0171\n",
      "Epoch: 10, Index: 59, Loss: 1.0937\n",
      "Epoch: 10, Index: 60, Loss: 2.8391\n",
      "Epoch: 10, Index: 61, Loss: 1.2614\n",
      "Epoch: 10, Index: 62, Loss: 1.9630\n",
      "Epoch: 10, Index: 63, Loss: 0.3028\n",
      "Epoch: 10, Index: 64, Loss: 0.3179\n",
      "Epoch: 10, Index: 65, Loss: 3.6111\n",
      "Epoch: 10, Index: 66, Loss: 0.2590\n",
      "Epoch: 10, Index: 67, Loss: 1.2130\n",
      "Epoch: 10, Index: 68, Loss: 9.9093\n",
      "Epoch: 10, Index: 69, Loss: 1.1763\n",
      "Epoch: 10, Index: 70, Loss: 0.8148\n",
      "Epoch: 10, Index: 71, Loss: 3.4084\n",
      "Epoch: 10, Index: 72, Loss: 1.5520\n",
      "Epoch: 10, Index: 73, Loss: 7.2251\n",
      "Epoch: 10, Index: 74, Loss: 0.3046\n",
      "Epoch: 10, Index: 75, Loss: 0.5671\n",
      "Epoch: 10, Index: 76, Loss: 1.1223\n",
      "Epoch: 10, Index: 77, Loss: 0.3843\n",
      "Epoch: 10, Index: 78, Loss: 0.7281\n",
      "Epoch: 10, Index: 79, Loss: 1.1245\n",
      "Epoch: 10, Index: 80, Loss: 1.0440\n",
      "Epoch: 10, Index: 81, Loss: 2.3707\n",
      "Epoch: 10, Index: 82, Loss: 2.0725\n",
      "Epoch: 10, Index: 83, Loss: 0.2116\n",
      "Epoch: 10, Index: 84, Loss: 1.1130\n",
      "Epoch: 10, Index: 85, Loss: 0.8820\n",
      "Epoch: 10, Index: 86, Loss: 0.1304\n",
      "Epoch: 10, Index: 87, Loss: 3.9635\n",
      "Epoch: 10, Index: 88, Loss: 4.5217\n",
      "Epoch: 10, Index: 89, Loss: 1.5704\n",
      "Epoch: 10, Index: 90, Loss: 1.8412\n",
      "Epoch: 10, Index: 91, Loss: 0.3423\n",
      "Epoch: 10, Index: 92, Loss: 1.8814\n",
      "Epoch: 10, Index: 93, Loss: 1.4517\n",
      "Epoch: 10, Index: 94, Loss: 0.5690\n",
      "Epoch: 10, Index: 95, Loss: 6.5996\n",
      "Epoch: 10, Index: 96, Loss: 3.2466\n",
      "Epoch: 10, Index: 97, Loss: 0.0989\n",
      "Epoch: 10, Index: 98, Loss: 0.7332\n",
      "Epoch: 10, Index: 99, Loss: 1.0339\n",
      "Epoch: 10, Index: 100, Loss: 1.6862\n",
      "Epoch: 10, Index: 101, Loss: 6.2858\n",
      "Epoch: 10, Index: 102, Loss: 2.0154\n",
      "Epoch: 10, Index: 103, Loss: 0.3221\n",
      "Epoch: 10, Index: 104, Loss: 0.0665\n",
      "Epoch: 10, Index: 105, Loss: 0.5258\n",
      "Epoch: 10, Index: 106, Loss: 0.4276\n",
      "Epoch: 10, Index: 107, Loss: 0.4616\n",
      "Epoch: 10, Index: 108, Loss: 1.3037\n",
      "Epoch: 10, Index: 109, Loss: 1.4315\n",
      "Epoch: 10, Index: 110, Loss: 2.3483\n",
      "Epoch: 10, Index: 111, Loss: 0.1268\n",
      "Epoch: 10, Index: 112, Loss: 0.1126\n",
      "Epoch: 10, Index: 113, Loss: 0.3307\n",
      "Epoch: 10, Index: 114, Loss: 0.5585\n",
      "Epoch: 10, Index: 115, Loss: 1.3102\n",
      "Epoch: 10, Index: 116, Loss: 3.7506\n",
      "Epoch: 10, Index: 117, Loss: 0.5815\n",
      "Epoch: 10, Index: 118, Loss: 4.3113\n",
      "Epoch: 10, Index: 119, Loss: 2.2381\n",
      "Epoch: 10, Index: 120, Loss: 1.1552\n",
      "Epoch: 10, Index: 121, Loss: 3.7316\n",
      "Epoch: 10, Index: 122, Loss: 2.4943\n",
      "Epoch: 10, Index: 123, Loss: 1.3896\n",
      "Epoch: 10, Index: 124, Loss: 0.8097\n",
      "Epoch: 10, Index: 125, Loss: 4.6844\n",
      "Epoch: 10, Index: 126, Loss: 0.1145\n",
      "Epoch: 10, Index: 127, Loss: 0.0572\n",
      "Epoch: 10, Index: 128, Loss: 6.1375\n",
      "Epoch: 10, Index: 129, Loss: 6.4275\n",
      "Epoch: 10, Index: 130, Loss: 1.6246\n",
      "Epoch: 10, Index: 131, Loss: 0.0561\n",
      "Epoch: 10, Index: 132, Loss: 3.2860\n",
      "Epoch: 10, Index: 133, Loss: 0.8759\n",
      "Epoch: 10, Index: 134, Loss: 0.9662\n",
      "Epoch: 10, Index: 135, Loss: 0.5217\n",
      "Epoch: 10, Index: 136, Loss: 1.2778\n",
      "Epoch: 10, Index: 137, Loss: 2.0891\n",
      "Epoch: 10, Index: 138, Loss: 1.3263\n",
      "Epoch: 10, Index: 139, Loss: 1.9693\n",
      "Epoch: 10, Index: 140, Loss: 2.4704\n",
      "Epoch: 10, Index: 141, Loss: 0.8429\n",
      "Epoch: 10, Index: 142, Loss: 0.3746\n",
      "Epoch: 10, Index: 143, Loss: 0.5649\n",
      "Epoch: 10, Index: 144, Loss: 3.0587\n",
      "Epoch: 10, Index: 145, Loss: 0.1493\n",
      "Epoch: 10, Index: 146, Loss: 2.4372\n",
      "Epoch: 10, Index: 147, Loss: 1.5525\n",
      "Epoch: 10, Index: 148, Loss: 1.6778\n",
      "Epoch: 10, Index: 149, Loss: 3.5756\n",
      "Epoch: 10, Index: 150, Loss: 1.1217\n",
      "Epoch: 10, Index: 151, Loss: 1.0775\n",
      "Epoch: 10, Index: 152, Loss: 0.3584\n",
      "Epoch: 10, Index: 153, Loss: 1.8429\n",
      "Epoch: 10, Index: 154, Loss: 2.5192\n",
      "Epoch: 10, Index: 155, Loss: 2.8253\n",
      "Epoch: 10, Index: 156, Loss: 0.9357\n",
      "Epoch: 10, Index: 157, Loss: 1.1315\n",
      "Epoch: 10, Index: 158, Loss: 0.0640\n",
      "Epoch: 10, Index: 159, Loss: 13.4551\n",
      "Epoch: 10, Index: 160, Loss: 2.2971\n",
      "Epoch: 10, Index: 161, Loss: 8.6141\n",
      "Epoch: 10, Index: 162, Loss: 0.9929\n",
      "Epoch: 10, Index: 163, Loss: 3.0396\n",
      "Epoch: 10, Index: 164, Loss: 2.3029\n",
      "Epoch: 10, Index: 165, Loss: 1.1726\n",
      "Epoch: 10, Index: 166, Loss: 0.7775\n",
      "Epoch: 10, Index: 167, Loss: 0.4155\n",
      "Epoch: 10, Index: 168, Loss: 3.4605\n",
      "Epoch: 10, Index: 169, Loss: 0.4006\n",
      "Epoch: 10, Index: 170, Loss: 2.5475\n",
      "Epoch: 10, Index: 171, Loss: 1.4808\n",
      "Epoch: 10, Index: 172, Loss: 2.9773\n",
      "Epoch: 10, Index: 173, Loss: 2.1234\n",
      "Epoch: 10, Index: 174, Loss: 0.3066\n",
      "Epoch: 10, Index: 175, Loss: 0.8215\n",
      "Epoch: 10, Index: 176, Loss: 3.1844\n",
      "Epoch: 10, Index: 177, Loss: 1.0569\n",
      "Epoch: 10, Index: 178, Loss: 0.6571\n",
      "Epoch: 10, Index: 179, Loss: 1.1854\n",
      "Epoch: 10, Index: 180, Loss: 1.5880\n",
      "Epoch: 10, Index: 181, Loss: 1.2379\n",
      "Epoch: 10, Index: 182, Loss: 2.8705\n",
      "Epoch: 10, Index: 183, Loss: 0.6861\n",
      "Epoch: 10, Index: 184, Loss: 1.9855\n",
      "Epoch: 10, Index: 185, Loss: 0.5173\n",
      "Epoch: 10, Index: 186, Loss: 2.5058\n",
      "Epoch: 10, Index: 187, Loss: 0.8192\n",
      "Epoch: 10, Index: 188, Loss: 0.0167\n",
      "Epoch: 10, Index: 189, Loss: 5.5389\n",
      "Epoch: 10, Index: 190, Loss: 1.4546\n",
      "Epoch: 10, Index: 191, Loss: 1.3038\n",
      "Epoch: 10, Index: 192, Loss: 0.6172\n",
      "Epoch: 10, Index: 193, Loss: 0.5015\n",
      "Epoch: 10, Index: 194, Loss: 0.0164\n",
      "Epoch: 10, Index: 195, Loss: 2.3780\n",
      "Epoch: 10, Index: 196, Loss: 0.1721\n",
      "Epoch: 10, Index: 197, Loss: 8.0300\n",
      "Epoch: 10, Index: 198, Loss: 0.0852\n",
      "Epoch: 10, Index: 199, Loss: 0.2308\n",
      "Epoch: 10, Index: 200, Loss: 2.6550\n",
      "Epoch: 10, Index: 201, Loss: 0.0099\n",
      "Epoch: 10, Index: 202, Loss: 0.4953\n",
      "Epoch: 10, Index: 203, Loss: 0.5633\n",
      "Epoch: 10, Index: 204, Loss: 1.8561\n",
      "Epoch: 10, Index: 205, Loss: 2.7999\n",
      "Epoch: 10, Index: 206, Loss: 1.6392\n",
      "Epoch: 10, Index: 207, Loss: 1.0040\n",
      "Epoch: 10, Index: 208, Loss: 1.7482\n",
      "Epoch: 10, Index: 209, Loss: 4.2408\n",
      "Epoch: 10, Index: 210, Loss: 1.0843\n",
      "Epoch: 10, Index: 211, Loss: 0.7422\n",
      "Epoch: 10, Index: 212, Loss: 3.4241\n",
      "Epoch: 10, Index: 213, Loss: 0.5429\n",
      "Epoch: 10, Index: 214, Loss: 4.3419\n",
      "Epoch: 10, Index: 215, Loss: 0.2296\n",
      "Epoch: 10, Index: 216, Loss: 1.3732\n",
      "Epoch: 10, Index: 217, Loss: 1.2252\n",
      "Epoch: 10, Index: 218, Loss: 1.2941\n",
      "Epoch: 10, Index: 219, Loss: 3.5729\n",
      "Epoch: 10, Index: 220, Loss: 0.4276\n",
      "Epoch: 10, Index: 221, Loss: 0.3745\n",
      "Epoch: 10, Index: 222, Loss: 2.8807\n",
      "Epoch: 10, Index: 223, Loss: 0.6892\n",
      "Epoch: 10, Index: 224, Loss: 1.2745\n",
      "Epoch: 10, Index: 225, Loss: 2.4965\n",
      "Epoch: 10, Index: 226, Loss: 0.3494\n",
      "Epoch: 10, Index: 227, Loss: 0.1836\n",
      "Epoch: 10, Index: 228, Loss: 0.1153\n",
      "Epoch: 10, Index: 229, Loss: 0.4332\n",
      "Epoch: 10, Index: 230, Loss: 1.1318\n",
      "Epoch: 10, Index: 231, Loss: 1.5754\n",
      "Epoch: 10, Index: 232, Loss: 0.1323\n",
      "Epoch: 10, Index: 233, Loss: 2.7510\n",
      "Epoch: 10, Index: 234, Loss: 1.3975\n",
      "Epoch: 10, Index: 235, Loss: 2.6322\n",
      "Epoch: 10, Index: 236, Loss: 2.0605\n",
      "Epoch: 10, Index: 237, Loss: 5.4144\n",
      "Epoch: 10, Index: 238, Loss: 1.7892\n",
      "Epoch: 10, Index: 239, Loss: 1.5594\n",
      "Epoch: 10, Index: 240, Loss: 0.9394\n",
      "Epoch: 10, Index: 241, Loss: 1.1750\n",
      "Epoch: 10, Index: 242, Loss: 1.2404\n",
      "Epoch: 10, Index: 243, Loss: 1.3056\n",
      "Epoch: 10, Index: 244, Loss: 1.1503\n",
      "Epoch: 10, Index: 245, Loss: 6.0255\n",
      "Epoch: 10, Index: 246, Loss: 3.0168\n",
      "Epoch: 10, Index: 247, Loss: 0.2314\n",
      "Epoch: 10, Index: 248, Loss: 5.6737\n",
      "Epoch: 10, Index: 249, Loss: 1.1552\n",
      "Epoch: 10, Index: 250, Loss: 0.4735\n",
      "Epoch: 10, Index: 251, Loss: 1.9283\n",
      "Epoch: 10, Index: 252, Loss: 1.7699\n",
      "Epoch: 10, Index: 253, Loss: 2.4628\n",
      "Epoch: 10, Index: 254, Loss: 0.7388\n",
      "Epoch: 10, Index: 255, Loss: 0.3109\n",
      "Epoch: 10, Index: 256, Loss: 2.3677\n",
      "Epoch: 10, Index: 257, Loss: 0.8216\n",
      "Epoch: 10, Index: 258, Loss: 0.6934\n",
      "Epoch: 10, Index: 259, Loss: 3.4336\n",
      "Epoch: 10, Index: 260, Loss: 0.7531\n",
      "Epoch: 10, Index: 261, Loss: 0.7852\n",
      "Epoch: 10, Index: 262, Loss: 1.4289\n",
      "Epoch: 10, Index: 263, Loss: 4.4391\n",
      "Epoch: 10, Index: 264, Loss: 2.8378\n",
      "Epoch: 10, Index: 265, Loss: 3.4835\n",
      "Epoch: 10, Index: 266, Loss: 1.6109\n",
      "Epoch: 10, Index: 267, Loss: 6.0456\n",
      "Epoch: 10, Index: 268, Loss: 7.8003\n",
      "Epoch: 10, Index: 269, Loss: 2.1627\n",
      "Epoch: 10, Index: 270, Loss: 3.3316\n",
      "Epoch: 10, Index: 271, Loss: 2.0502\n",
      "Epoch: 10, Index: 272, Loss: 0.9396\n",
      "Epoch: 10, Index: 273, Loss: 1.3035\n",
      "Epoch: 10, Index: 274, Loss: 5.1325\n",
      "Epoch: 10, Index: 275, Loss: 3.4246\n",
      "Epoch: 10, Index: 276, Loss: 0.3649\n",
      "Epoch: 10, Index: 277, Loss: 0.2158\n",
      "Epoch: 10, Index: 278, Loss: 1.9516\n",
      "Epoch: 10, Index: 279, Loss: 0.6104\n",
      "Epoch: 10, Index: 280, Loss: 0.3644\n",
      "Epoch: 10, Index: 281, Loss: 1.3433\n",
      "Epoch: 10, Index: 282, Loss: 0.7351\n",
      "Epoch: 10, Index: 283, Loss: 2.8041\n",
      "Epoch: 10, Index: 284, Loss: 1.3241\n",
      "Epoch: 10, Index: 285, Loss: 0.0825\n",
      "Epoch: 10, Index: 286, Loss: 0.4038\n",
      "Epoch: 10, Index: 287, Loss: 1.9301\n",
      "Epoch: 10, Index: 288, Loss: 1.3001\n",
      "Epoch: 10, Index: 289, Loss: 0.8789\n",
      "Epoch: 10, Index: 290, Loss: 2.2475\n",
      "Epoch: 10, Index: 291, Loss: 3.3818\n",
      "Epoch: 10, Index: 292, Loss: 0.2421\n",
      "Epoch: 10, Index: 293, Loss: 0.8083\n",
      "Epoch: 10, Index: 294, Loss: 0.7596\n",
      "Epoch: 10, Index: 295, Loss: 0.0979\n",
      "Epoch: 10, Index: 296, Loss: 0.5836\n",
      "Epoch: 10, Index: 297, Loss: 0.2936\n",
      "Epoch: 10, Index: 298, Loss: 1.4429\n",
      "Epoch: 10, Index: 299, Loss: 2.3492\n",
      "Epoch: 10, Index: 300, Loss: 2.3169\n",
      "Epoch: 10, Index: 301, Loss: 0.5505\n",
      "Epoch: 10, Index: 302, Loss: 2.2195\n",
      "Epoch: 10, Index: 303, Loss: 0.1178\n",
      "Epoch: 10, Index: 304, Loss: 2.7918\n",
      "Epoch: 10, Index: 305, Loss: 0.7905\n",
      "Epoch: 10, Index: 306, Loss: 1.1289\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a35ad8dc3d4a55ac87de004fdba759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b8995f5d99473e891fe736a64f4994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Index: 0, Loss: 1.6976\n",
      "Epoch: 11, Index: 1, Loss: 0.3148\n",
      "Epoch: 11, Index: 2, Loss: 1.9809\n",
      "Epoch: 11, Index: 3, Loss: 1.2043\n",
      "Epoch: 11, Index: 4, Loss: 3.5435\n",
      "Epoch: 11, Index: 5, Loss: 8.4871\n",
      "Epoch: 11, Index: 6, Loss: 0.3380\n",
      "Epoch: 11, Index: 7, Loss: 1.0701\n",
      "Epoch: 11, Index: 8, Loss: 2.8066\n",
      "Epoch: 11, Index: 9, Loss: 1.8173\n",
      "Epoch: 11, Index: 10, Loss: 0.7958\n",
      "Epoch: 11, Index: 11, Loss: 0.3423\n",
      "Epoch: 11, Index: 12, Loss: 0.9439\n",
      "Epoch: 11, Index: 13, Loss: 0.0762\n",
      "Epoch: 11, Index: 14, Loss: 0.4518\n",
      "Epoch: 11, Index: 15, Loss: 1.6553\n",
      "Epoch: 11, Index: 16, Loss: 1.3376\n",
      "Epoch: 11, Index: 17, Loss: 1.7786\n",
      "Epoch: 11, Index: 18, Loss: 0.7237\n",
      "Epoch: 11, Index: 19, Loss: 0.2008\n",
      "Epoch: 11, Index: 20, Loss: 0.3137\n",
      "Epoch: 11, Index: 21, Loss: 0.2318\n",
      "Epoch: 11, Index: 22, Loss: 1.2239\n",
      "Epoch: 11, Index: 23, Loss: 0.4982\n",
      "Epoch: 11, Index: 24, Loss: 3.3868\n",
      "Epoch: 11, Index: 25, Loss: 1.5991\n",
      "Epoch: 11, Index: 26, Loss: 0.4283\n",
      "Epoch: 11, Index: 27, Loss: 0.7744\n",
      "Epoch: 11, Index: 28, Loss: 4.8307\n",
      "Epoch: 11, Index: 29, Loss: 0.5214\n",
      "Epoch: 11, Index: 30, Loss: 1.2605\n",
      "Epoch: 11, Index: 31, Loss: 3.2381\n",
      "Epoch: 11, Index: 32, Loss: 1.4850\n",
      "Epoch: 11, Index: 33, Loss: 0.5950\n",
      "Epoch: 11, Index: 34, Loss: 1.2024\n",
      "Epoch: 11, Index: 35, Loss: 1.4102\n",
      "Epoch: 11, Index: 36, Loss: 3.0572\n",
      "Epoch: 11, Index: 37, Loss: 0.2372\n",
      "Epoch: 11, Index: 38, Loss: 0.5322\n",
      "Epoch: 11, Index: 39, Loss: 2.3931\n",
      "Epoch: 11, Index: 40, Loss: 0.6084\n",
      "Epoch: 11, Index: 41, Loss: 0.0301\n",
      "Epoch: 11, Index: 42, Loss: 1.5455\n",
      "Epoch: 11, Index: 43, Loss: 1.2199\n",
      "Epoch: 11, Index: 44, Loss: 2.2598\n",
      "Epoch: 11, Index: 45, Loss: 1.0523\n",
      "Epoch: 11, Index: 46, Loss: 0.1338\n",
      "Epoch: 11, Index: 47, Loss: 0.7607\n",
      "Epoch: 11, Index: 48, Loss: 2.2525\n",
      "Epoch: 11, Index: 49, Loss: 2.3926\n",
      "Epoch: 11, Index: 50, Loss: 0.5698\n",
      "Epoch: 11, Index: 51, Loss: 0.2069\n",
      "Epoch: 11, Index: 52, Loss: 0.0009\n",
      "Epoch: 11, Index: 53, Loss: 0.6143\n",
      "Epoch: 11, Index: 54, Loss: 2.8105\n",
      "Epoch: 11, Index: 55, Loss: 0.4549\n",
      "Epoch: 11, Index: 56, Loss: 1.7930\n",
      "Epoch: 11, Index: 57, Loss: 1.5714\n",
      "Epoch: 11, Index: 58, Loss: 1.1603\n",
      "Epoch: 11, Index: 59, Loss: 0.3023\n",
      "Epoch: 11, Index: 60, Loss: 0.4343\n",
      "Epoch: 11, Index: 61, Loss: 0.1251\n",
      "Epoch: 11, Index: 62, Loss: 1.4067\n",
      "Epoch: 11, Index: 63, Loss: 4.5460\n",
      "Epoch: 11, Index: 64, Loss: 1.7319\n",
      "Epoch: 11, Index: 65, Loss: 1.4814\n",
      "Epoch: 11, Index: 66, Loss: 0.6018\n",
      "Epoch: 11, Index: 67, Loss: 1.4816\n",
      "Epoch: 11, Index: 68, Loss: 0.8850\n",
      "Epoch: 11, Index: 69, Loss: 0.1935\n",
      "Epoch: 11, Index: 70, Loss: 4.3141\n",
      "Epoch: 11, Index: 71, Loss: 1.2899\n",
      "Epoch: 11, Index: 72, Loss: 0.5531\n",
      "Epoch: 11, Index: 73, Loss: 1.0833\n",
      "Epoch: 11, Index: 74, Loss: 0.0534\n",
      "Epoch: 11, Index: 75, Loss: 0.4215\n",
      "Epoch: 11, Index: 76, Loss: 0.4106\n",
      "Epoch: 11, Index: 77, Loss: 0.8362\n",
      "Epoch: 11, Index: 78, Loss: 1.2131\n",
      "Epoch: 11, Index: 79, Loss: 0.6018\n",
      "Epoch: 11, Index: 80, Loss: 1.8922\n",
      "Epoch: 11, Index: 81, Loss: 1.2312\n",
      "Epoch: 11, Index: 82, Loss: 1.5898\n",
      "Epoch: 11, Index: 83, Loss: 1.2614\n",
      "Epoch: 11, Index: 84, Loss: 2.4202\n",
      "Epoch: 11, Index: 85, Loss: 9.1299\n",
      "Epoch: 11, Index: 86, Loss: 0.5809\n",
      "Epoch: 11, Index: 87, Loss: 0.8281\n",
      "Epoch: 11, Index: 88, Loss: 7.2803\n",
      "Epoch: 11, Index: 89, Loss: 1.6379\n",
      "Epoch: 11, Index: 90, Loss: 1.7368\n",
      "Epoch: 11, Index: 91, Loss: 2.1517\n",
      "Epoch: 11, Index: 92, Loss: 1.7008\n",
      "Epoch: 11, Index: 93, Loss: 2.2497\n",
      "Epoch: 11, Index: 94, Loss: 0.3015\n",
      "Epoch: 11, Index: 95, Loss: 0.4764\n",
      "Epoch: 11, Index: 96, Loss: 15.0342\n",
      "Epoch: 11, Index: 97, Loss: 0.0307\n",
      "Epoch: 11, Index: 98, Loss: 1.0993\n",
      "Epoch: 11, Index: 99, Loss: 2.0198\n",
      "Epoch: 11, Index: 100, Loss: 2.6245\n",
      "Epoch: 11, Index: 101, Loss: 1.5607\n",
      "Epoch: 11, Index: 102, Loss: 1.8489\n",
      "Epoch: 11, Index: 103, Loss: 6.5386\n",
      "Epoch: 11, Index: 104, Loss: 4.2046\n",
      "Epoch: 11, Index: 105, Loss: 1.1184\n",
      "Epoch: 11, Index: 106, Loss: 0.7436\n",
      "Epoch: 11, Index: 107, Loss: 5.5809\n",
      "Epoch: 11, Index: 108, Loss: 1.8215\n",
      "Epoch: 11, Index: 109, Loss: 1.6498\n",
      "Epoch: 11, Index: 110, Loss: 0.0460\n",
      "Epoch: 11, Index: 111, Loss: 1.2291\n",
      "Epoch: 11, Index: 112, Loss: 5.7073\n",
      "Epoch: 11, Index: 113, Loss: 0.4490\n",
      "Epoch: 11, Index: 114, Loss: 0.2659\n",
      "Epoch: 11, Index: 115, Loss: 0.8988\n",
      "Epoch: 11, Index: 116, Loss: 3.7803\n",
      "Epoch: 11, Index: 117, Loss: 4.3943\n",
      "Epoch: 11, Index: 118, Loss: 2.1047\n",
      "Epoch: 11, Index: 119, Loss: 1.5343\n",
      "Epoch: 11, Index: 120, Loss: 7.0232\n",
      "Epoch: 11, Index: 121, Loss: 0.0035\n",
      "Epoch: 11, Index: 122, Loss: 1.2449\n",
      "Epoch: 11, Index: 123, Loss: 0.3110\n",
      "Epoch: 11, Index: 124, Loss: 0.6818\n",
      "Epoch: 11, Index: 125, Loss: 1.7944\n",
      "Epoch: 11, Index: 126, Loss: 5.0375\n",
      "Epoch: 11, Index: 127, Loss: 1.6057\n",
      "Epoch: 11, Index: 128, Loss: 1.0648\n",
      "Epoch: 11, Index: 129, Loss: 0.6286\n",
      "Epoch: 11, Index: 130, Loss: 3.2581\n",
      "Epoch: 11, Index: 131, Loss: 2.2393\n",
      "Epoch: 11, Index: 132, Loss: 11.8854\n",
      "Epoch: 11, Index: 133, Loss: 4.9516\n",
      "Epoch: 11, Index: 134, Loss: 5.5727\n",
      "Epoch: 11, Index: 135, Loss: 5.5459\n",
      "Epoch: 11, Index: 136, Loss: 1.2223\n",
      "Epoch: 11, Index: 137, Loss: 4.2628\n",
      "Epoch: 11, Index: 138, Loss: 1.8854\n",
      "Epoch: 11, Index: 139, Loss: 1.1899\n",
      "Epoch: 11, Index: 140, Loss: 2.1879\n",
      "Epoch: 11, Index: 141, Loss: 0.6916\n",
      "Epoch: 11, Index: 142, Loss: 1.5240\n",
      "Epoch: 11, Index: 143, Loss: 0.0341\n",
      "Epoch: 11, Index: 144, Loss: 0.9982\n",
      "Epoch: 11, Index: 145, Loss: 1.4256\n",
      "Epoch: 11, Index: 146, Loss: 0.0515\n",
      "Epoch: 11, Index: 147, Loss: 3.2511\n",
      "Epoch: 11, Index: 148, Loss: 3.5175\n",
      "Epoch: 11, Index: 149, Loss: 1.8800\n",
      "Epoch: 11, Index: 150, Loss: 2.6516\n",
      "Epoch: 11, Index: 151, Loss: 1.6829\n",
      "Epoch: 11, Index: 152, Loss: 2.6060\n",
      "Epoch: 11, Index: 153, Loss: 0.9907\n",
      "Epoch: 11, Index: 154, Loss: 0.2247\n",
      "Epoch: 11, Index: 155, Loss: 0.7334\n",
      "Epoch: 11, Index: 156, Loss: 1.1414\n",
      "Epoch: 11, Index: 157, Loss: 1.2672\n",
      "Epoch: 11, Index: 158, Loss: 1.5296\n",
      "Epoch: 11, Index: 159, Loss: 0.4846\n",
      "Epoch: 11, Index: 160, Loss: 2.5372\n",
      "Epoch: 11, Index: 161, Loss: 0.1107\n",
      "Epoch: 11, Index: 162, Loss: 1.2784\n",
      "Epoch: 11, Index: 163, Loss: 0.2293\n",
      "Epoch: 11, Index: 164, Loss: 1.6202\n",
      "Epoch: 11, Index: 165, Loss: 5.2766\n",
      "Epoch: 11, Index: 166, Loss: 1.6307\n",
      "Epoch: 11, Index: 167, Loss: 2.0723\n",
      "Epoch: 11, Index: 168, Loss: 1.4287\n",
      "Epoch: 11, Index: 169, Loss: 3.5812\n",
      "Epoch: 11, Index: 170, Loss: 1.8892\n",
      "Epoch: 11, Index: 171, Loss: 6.3458\n",
      "Epoch: 11, Index: 172, Loss: 1.6377\n",
      "Epoch: 11, Index: 173, Loss: 0.5143\n",
      "Epoch: 11, Index: 174, Loss: 0.8155\n",
      "Epoch: 11, Index: 175, Loss: 2.8104\n",
      "Epoch: 11, Index: 176, Loss: 0.2975\n",
      "Epoch: 11, Index: 177, Loss: 1.1890\n",
      "Epoch: 11, Index: 178, Loss: 1.2967\n",
      "Epoch: 11, Index: 179, Loss: 1.4414\n",
      "Epoch: 11, Index: 180, Loss: 1.1109\n",
      "Epoch: 11, Index: 181, Loss: 1.1871\n",
      "Epoch: 11, Index: 182, Loss: 1.4137\n",
      "Epoch: 11, Index: 183, Loss: 3.7409\n",
      "Epoch: 11, Index: 184, Loss: 1.3770\n",
      "Epoch: 11, Index: 185, Loss: 0.5282\n",
      "Epoch: 11, Index: 186, Loss: 0.9324\n",
      "Epoch: 11, Index: 187, Loss: 4.7693\n",
      "Epoch: 11, Index: 188, Loss: 0.7111\n",
      "Epoch: 11, Index: 189, Loss: 0.2329\n",
      "Epoch: 11, Index: 190, Loss: 0.4405\n",
      "Epoch: 11, Index: 191, Loss: 3.5194\n",
      "Epoch: 11, Index: 192, Loss: 3.9718\n",
      "Epoch: 11, Index: 193, Loss: 5.2120\n",
      "Epoch: 11, Index: 194, Loss: 10.6707\n",
      "Epoch: 11, Index: 195, Loss: 6.8820\n",
      "Epoch: 11, Index: 196, Loss: 0.1509\n",
      "Epoch: 11, Index: 197, Loss: 2.0354\n",
      "Epoch: 11, Index: 198, Loss: 4.0776\n",
      "Epoch: 11, Index: 199, Loss: 2.1120\n",
      "Epoch: 11, Index: 200, Loss: 0.1863\n",
      "Epoch: 11, Index: 201, Loss: 1.3094\n",
      "Epoch: 11, Index: 202, Loss: 0.6744\n",
      "Epoch: 11, Index: 203, Loss: 0.0219\n",
      "Epoch: 11, Index: 204, Loss: 0.0598\n",
      "Epoch: 11, Index: 205, Loss: 0.1199\n",
      "Epoch: 11, Index: 206, Loss: 2.7419\n",
      "Epoch: 11, Index: 207, Loss: 0.4663\n",
      "Epoch: 11, Index: 208, Loss: 0.3038\n",
      "Epoch: 11, Index: 209, Loss: 7.0356\n",
      "Epoch: 11, Index: 210, Loss: 3.1598\n",
      "Epoch: 11, Index: 211, Loss: 1.1482\n",
      "Epoch: 11, Index: 212, Loss: 4.5810\n",
      "Epoch: 11, Index: 213, Loss: 0.6967\n",
      "Epoch: 11, Index: 214, Loss: 1.6956\n",
      "Epoch: 11, Index: 215, Loss: 0.6098\n",
      "Epoch: 11, Index: 216, Loss: 1.2846\n",
      "Epoch: 11, Index: 217, Loss: 1.0897\n",
      "Epoch: 11, Index: 218, Loss: 0.5546\n",
      "Epoch: 11, Index: 219, Loss: 1.5139\n",
      "Epoch: 11, Index: 220, Loss: 0.7382\n",
      "Epoch: 11, Index: 221, Loss: 1.0858\n",
      "Epoch: 11, Index: 222, Loss: 0.3083\n",
      "Epoch: 11, Index: 223, Loss: 0.4769\n",
      "Epoch: 11, Index: 224, Loss: 5.7203\n",
      "Epoch: 11, Index: 225, Loss: 0.2698\n",
      "Epoch: 11, Index: 226, Loss: 0.6495\n",
      "Epoch: 11, Index: 227, Loss: 0.7743\n",
      "Epoch: 11, Index: 228, Loss: 2.3510\n",
      "Epoch: 11, Index: 229, Loss: 4.1489\n",
      "Epoch: 11, Index: 230, Loss: 1.4436\n",
      "Epoch: 11, Index: 231, Loss: 2.1835\n",
      "Epoch: 11, Index: 232, Loss: 1.0534\n",
      "Epoch: 11, Index: 233, Loss: 4.6637\n",
      "Epoch: 11, Index: 234, Loss: 0.1996\n",
      "Epoch: 11, Index: 235, Loss: 0.5558\n",
      "Epoch: 11, Index: 236, Loss: 1.5774\n",
      "Epoch: 11, Index: 237, Loss: 0.8364\n",
      "Epoch: 11, Index: 238, Loss: 0.1955\n",
      "Epoch: 11, Index: 239, Loss: 0.8943\n",
      "Epoch: 11, Index: 240, Loss: 0.1284\n",
      "Epoch: 11, Index: 241, Loss: 3.6095\n",
      "Epoch: 11, Index: 242, Loss: 0.1058\n",
      "Epoch: 11, Index: 243, Loss: 0.0704\n",
      "Epoch: 11, Index: 244, Loss: 1.0159\n",
      "Epoch: 11, Index: 245, Loss: 0.4414\n",
      "Epoch: 11, Index: 246, Loss: 1.9096\n",
      "Epoch: 11, Index: 247, Loss: 3.2261\n",
      "Epoch: 11, Index: 248, Loss: 16.9349\n",
      "Epoch: 11, Index: 249, Loss: 0.2300\n",
      "Epoch: 11, Index: 250, Loss: 0.7243\n",
      "Epoch: 11, Index: 251, Loss: 0.3191\n",
      "Epoch: 11, Index: 252, Loss: 2.2463\n",
      "Epoch: 11, Index: 253, Loss: 0.1770\n",
      "Epoch: 11, Index: 254, Loss: 3.2444\n",
      "Epoch: 11, Index: 255, Loss: 4.5870\n",
      "Epoch: 11, Index: 256, Loss: 0.1676\n",
      "Epoch: 11, Index: 257, Loss: 5.0276\n",
      "Epoch: 11, Index: 258, Loss: 0.2299\n",
      "Epoch: 11, Index: 259, Loss: 2.1793\n",
      "Epoch: 11, Index: 260, Loss: 1.9992\n",
      "Epoch: 11, Index: 261, Loss: 3.4660\n",
      "Epoch: 11, Index: 262, Loss: 0.5869\n",
      "Epoch: 11, Index: 263, Loss: 0.7251\n",
      "Epoch: 11, Index: 264, Loss: 0.8074\n",
      "Epoch: 11, Index: 265, Loss: 2.7880\n",
      "Epoch: 11, Index: 266, Loss: 5.0685\n",
      "Epoch: 11, Index: 267, Loss: 0.8534\n",
      "Epoch: 11, Index: 268, Loss: 0.2511\n",
      "Epoch: 11, Index: 269, Loss: 4.2651\n",
      "Epoch: 11, Index: 270, Loss: 0.5059\n",
      "Epoch: 11, Index: 271, Loss: 2.2879\n",
      "Epoch: 11, Index: 272, Loss: 1.1340\n",
      "Epoch: 11, Index: 273, Loss: 0.4409\n",
      "Epoch: 11, Index: 274, Loss: 0.6775\n",
      "Epoch: 11, Index: 275, Loss: 1.1791\n",
      "Epoch: 11, Index: 276, Loss: 0.0929\n",
      "Epoch: 11, Index: 277, Loss: 0.6449\n",
      "Epoch: 11, Index: 278, Loss: 1.2259\n",
      "Epoch: 11, Index: 279, Loss: 0.7353\n",
      "Epoch: 11, Index: 280, Loss: 3.9392\n",
      "Epoch: 11, Index: 281, Loss: 1.0440\n",
      "Epoch: 11, Index: 282, Loss: 8.2808\n",
      "Epoch: 11, Index: 283, Loss: 1.7940\n",
      "Epoch: 11, Index: 284, Loss: 0.9597\n",
      "Epoch: 11, Index: 285, Loss: 6.1356\n",
      "Epoch: 11, Index: 286, Loss: 0.2688\n",
      "Epoch: 11, Index: 287, Loss: 0.2682\n",
      "Epoch: 11, Index: 288, Loss: 0.0538\n",
      "Epoch: 11, Index: 289, Loss: 0.4513\n",
      "Epoch: 11, Index: 290, Loss: 1.3630\n",
      "Epoch: 11, Index: 291, Loss: 0.8464\n",
      "Epoch: 11, Index: 292, Loss: 5.0120\n",
      "Epoch: 11, Index: 293, Loss: 1.2987\n",
      "Epoch: 11, Index: 294, Loss: 0.5990\n",
      "Epoch: 11, Index: 295, Loss: 1.9389\n",
      "Epoch: 11, Index: 296, Loss: 0.6754\n",
      "Epoch: 11, Index: 297, Loss: 1.1118\n",
      "Epoch: 11, Index: 298, Loss: 3.1653\n",
      "Epoch: 11, Index: 299, Loss: 0.1115\n",
      "Epoch: 11, Index: 300, Loss: 0.6190\n",
      "Epoch: 11, Index: 301, Loss: 0.1767\n",
      "Epoch: 11, Index: 302, Loss: 0.1203\n",
      "Epoch: 11, Index: 303, Loss: 2.9395\n",
      "Epoch: 11, Index: 304, Loss: 1.0951\n",
      "Epoch: 11, Index: 305, Loss: 2.2705\n",
      "Epoch: 11, Index: 306, Loss: 5.8419\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "n_total_steps = len(trainloader)\n",
    "avg_train_loss_over_epochs = []\n",
    "avg_val_loss_over_epochs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i, (images, labels) in tqdm(enumerate(trainloader), desc=\"Training Progress\", total=len(trainloader)):\n",
    "        # Move images and labels to device\n",
    "        images = torch.stack(images).float()\n",
    "        images = images.permute(1, 0, 2, 3, 4)  # Change shape to [5, 10, 1, 224, 224]\n",
    "        labels = labels.float()\n",
    "\n",
    "        # Forward pass with autograd\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        tqdm.write(f\"Epoch: {epoch+1}, Index: {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        fabric.backward(loss)\n",
    "        optimizer.step()\n",
    "        # Store the loss\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # Store the average training loss for this epoch\n",
    "    avg_train_loss_over_epochs.append(sum(train_losses) / len(train_losses))\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(valloader, desc=\"Validation Progress\"):\n",
    "            images = torch.stack(images).float()\n",
    "            images = images.permute(1, 0, 2, 3, 4)\n",
    "            labels = labels.float()\n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            print(\"Validation Loss: \", loss.item())\n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    # Store the average validation loss for this epoch\n",
    "    avg_val_loss_over_epochs.append(sum(val_losses) / len(val_losses))\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), avg_train_loss_over_epochs, label='Average Training Loss', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), avg_val_loss_over_epochs, label='Average Validation Loss', marker='o')\n",
    "plt.xticks(range(1, num_epochs + 1))  # Ensure x-axis includes all epoch numbers\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b774addc98457ab556a3cc253bad25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "testloader = fabric.setup_dataloaders(testloader)\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(testloader, desc=\"Testing Progress\"):\n",
    "        images = torch.stack(images).float()\n",
    "        images = images.permute(1, 0, 2, 3, 4)\n",
    "        labels = labels.float()\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        print(\"Test loss\", test_losses)\n",
    "\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "print(f'Average test loss: {avg_test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
