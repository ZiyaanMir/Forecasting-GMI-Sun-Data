{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import torchsummary\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('../DataLoader')\n",
    "\n",
    "from dataloader import SunImageDataset\n",
    "\n",
    "\n",
    "from lightning.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "fabric = Fabric(accelerator='cuda', devices=1, precision=\"bf16-mixed\")\n",
    "fabric.launch()\n",
    "print(fabric.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 224*224\n",
    "hidden_size = 166\n",
    "num_epochs = 20\n",
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "# dropout = 0.6990787087509548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SunImageDataset(csv_file=\"D:\\\\dataset.csv\", offset=0, transform=transforms.ToTensor())\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Without Validation Set\n",
    "trainset, testset = torch.utils.data.Subset(dataset, range(train_size)), torch.utils.data.Subset(dataset, range(train_size, len(dataset)))\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "\n",
    "trainloader = fabric.setup_dataloaders(trainloader)\n",
    "\n",
    "# # With Validation Set\n",
    "# # Split dataset into training and test sets\n",
    "# train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2, shuffle=False)\n",
    "\n",
    "# # Further split training set into training and validation sets\n",
    "# train_indices, val_indices = train_test_split(train_indices, test_size=0.25, shuffle=False)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# trainset = torch.utils.data.Subset(dataset, train_indices)\n",
    "# valset = torch.utils.data.Subset(dataset, val_indices)\n",
    "# testset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "# valloader = torch.utils.data.DataLoader(dataset=valset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# # Get a batch of training data\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "# images = torch.stack(images)\n",
    "# print(images.shape)\n",
    "# print(labels.shape)\n",
    "\n",
    "# print(images)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_FabricModule(\n",
       "  (_forward_module): GmiSwinTransformer(\n",
       "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pretrained_model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers): Sequential(\n",
       "        (0): SwinTransformerStage(\n",
       "          (downsample): Identity()\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.004)\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.013)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.017)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.022)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.026)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.030)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.035)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.039)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.043)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.048)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.052)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.057)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.061)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.065)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.070)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.074)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.078)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.083)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.087)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.096)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Linear(in_features=1024, out_features=166, bias=True)\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Linear(in_features=1660, out_features=166, bias=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=166, out_features=1, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (_original_module): GmiSwinTransformer(\n",
       "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (pretrained_model): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers): Sequential(\n",
       "        (0): SwinTransformerStage(\n",
       "          (downsample): Identity()\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.004)\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.013)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.017)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.022)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.026)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.030)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.035)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.039)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.043)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.048)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.052)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.057)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.061)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.065)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.070)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.074)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.078)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.083)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.087)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.096)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Linear(in_features=1024, out_features=166, bias=True)\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.01)\n",
       "      (1): Linear(in_features=1660, out_features=166, bias=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=166, out_features=1, bias=True)\n",
       "      (5): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class GmiSwinTransformer(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(GmiSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Batch normalization for 3 channels\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # Initialize Swin Transformer\n",
    "        self.pretrained_model = timm.create_model(\n",
    "            'swin_base_patch4_window7_224',\n",
    "            pretrained=True,\n",
    "            num_classes=hidden_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size*10, hidden_size),\n",
    "            nn.Dropout(p=0.5),  # Added dropout probability\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, images) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Batch should be in format:\n",
    "        {\n",
    "            'images': torch.FloatTensor((10, 1, 224, 224))\n",
    "        }\n",
    "        \"\"\"\n",
    "        # # Print input shape for debugging\n",
    "        # # print(\"Input shape:\", images.shape)\n",
    "        # image_features = torch.zeros(images.shape[0],images.shape[1], hidden_size).to(device)\n",
    "        # for i in range(images.shape[0]):\n",
    "        #     image = images[i, :, :, :]\n",
    "        #     # Pretrained swin transformer accepts three channel images\n",
    "        #     three_channel = torch.stack([image,image,image], dim=1).squeeze(2)\n",
    "        #     # print(\"three_channel\", three_channel.size())\n",
    "        #     # Model learns optimal initial normalisation\n",
    "        #     normalized_images = self.bn(three_channel)\n",
    "        #     # Get image features\n",
    "        #     image_feature = self.pretrained_model.forward(normalized_images)\n",
    "        #     image_features[i] = image_feature\n",
    "        # # print(\"image_features before reshaping\", image_features.size())\n",
    "        # image_features = image_features.view(image_features.size(0), -1)\n",
    "        # print(\"image_features after reshaping\", image_features.size())\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        images = images.reshape(-1, 1, 224, 224)\n",
    "        images = torch.cat([images, images, images], dim=1)\n",
    "        normalized_images = self.bn(images)\n",
    "        features = self.pretrained_model(normalized_images)\n",
    "        image_features = features.view(batch_size, -1)\n",
    "        \n",
    "        output = self.fc(image_features)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GmiSwinTransformer(hidden_size=hidden_size).to(device)\n",
    "model = GmiSwinTransformer(hidden_size=hidden_size)\n",
    "\n",
    "# print(torchsummary.summary(model, (10, 1, 224, 224)))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f132c19724436286c0ed54896d2e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model\n",
    "n_total_steps = len(trainloader)\n",
    "avg_loss_over_epochs = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    \n",
    "    for i, (images, labels) in tqdm(enumerate(trainloader), desc=\"Training Progress\", total=len(trainloader)):\n",
    "        # Move images and labels to device\n",
    "        images = torch.stack(images).float()\n",
    "        images = images.permute(1, 0, 2, 3, 4)  # Change shape to [5, 10, 1, 224, 224]\n",
    "        labels = labels.float()\n",
    "\n",
    "        # Forward pass with autograd\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        tqdm.write(f\"Epoch: {epoch+1}, Index: {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        fabric.backward(loss)\n",
    "        optimizer.step()\n",
    "        # Store the loss\n",
    "        train_losses.append(loss.item())\n",
    "    # Store the loss for this epoch\n",
    "    avg_loss_over_epochs.append(sum(train_losses)/len(train_losses))\n",
    "# Plot loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), avg_loss_over_epochs, label='Average Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16422ee6708e4e96af6a9c47c3d47733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing Progress:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss [0.23321127891540527]\n",
      "Test loss [0.23321127891540527, 1.540835976600647]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945, 1.002197265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945, 1.002197265625, 0.3269612789154053]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945, 1.002197265625, 0.3269612789154053, 3.7547106742858887]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945, 1.002197265625, 0.3269612789154053, 3.7547106742858887, 0.19337616860866547]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945, 1.002197265625, 0.3269612789154053, 3.7547106742858887, 0.19337616860866547, 0.6465011835098267]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945, 1.002197265625, 0.3269612789154053, 3.7547106742858887, 0.19337616860866547, 0.6465011835098267, 0.455322265625]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945, 1.002197265625, 0.3269612789154053, 3.7547106742858887, 0.19337616860866547, 0.6465011835098267, 0.455322265625, 0.8992111682891846]\n",
      "Test loss [0.23321127891540527, 1.540835976600647, 0.2957425117492676, 0.6201261281967163, 0.8281573057174683, 2.1394612789154053, 0.25590741634368896, 0.8992111682891846, 2.361572265625, 0.2957425117492676, 0.8281573057174683, 0.2957425117492676, 1.9229071140289307, 0.8679924011230469, 1.002197265625, 2.135782480239868, 0.709032416343689, 2.642822265625, 0.8679924011230469, 1.0241801738739014, 1.540835976600647, 0.19337616860866547, 2.8330016136169434, 0.14430497586727142, 1.3533363342285156, 0.5263761281967163, 0.5889073610305786, 0.8679924011230469, 3.7547106742858887, 0.549072265625, 2.095947265625, 0.549072265625, 0.4951573610305786, 3.783930778503418, 0.455322265625, 0.2957425117492676, 0.19337616860866547, 0.2957425117492676, 0.6201261281967163, 0.455322265625, 0.2644300162792206, 4.682657241821289, 1.5096172094345093, 1.4697821140289307, 0.7402511835098267, 1.2812823057174683, 2.1670010089874268, 1.567211389541626, 2.781085968017578, 1.5046801567077637, 0.455322265625, 2.1394612789154053, 0.8281573057174683, 3.2861266136169434, 2.8728368282318115, 1.0241801738739014, 0.2644300162792206, 0.2644300162792206, 0.04203237593173981, 0.2957425117492676, 3.3798766136169434, 3.2861266136169434, 7.723491668701172, 1.313501238822937, 1.9229071140289307, 1.766626238822937, 1.019336223602295, 0.4951573610305786, 0.38455507159233093, 0.002197265625, 2.361572265625, 0.04203237593173981, 0.19337616860866547, 1.313501238822937, 1.206836462020874, 0.11308623105287552, 9.015657424926758, 2.6541175842285156, 0.04203237593173981, 0.08186748623847961, 0.19337616860866547, 0.9929614663124084, 0.5263761281967163, 11.182459831237793, 0.5263761281967163, 2.624805212020874, 0.08186748623847961, 0.08186748623847961, 18.44725227355957, 3.874835729598999, 0.25590741634368896, 16.488086700439453, 0.8679924011230469, 3.2861266136169434, 11.111406326293945, 1.002197265625, 0.3269612789154053, 3.7547106742858887, 0.19337616860866547, 0.6465011835098267, 0.455322265625, 0.8992111682891846, 0.87890625]\n",
      "Average test loss: 1.8429\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "testloader = fabric.setup_dataloaders(testloader)\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(testloader, desc=\"Testing Progress\"):\n",
    "        images = torch.stack(images).float()\n",
    "        images = images.permute(1, 0, 2, 3, 4)\n",
    "        labels = labels.float()\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        print(\"Test loss\", test_losses)\n",
    "\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "rmse = math.sqrt(avg_test_loss)\n",
    "print(f'Average test loss: {avg_test_loss:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
